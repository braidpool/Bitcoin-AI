So starting, first the chances of LND. So here's the actual first commit in LND. It was October 27, 2015. And this is basically when I was in school still, finishing up school, and this is basically mostly doing my winter break. So this is kind of like the first major sprint that we had in terms of code. We talked about what LND we're using, as far as the licensing, kind of like architecture of the daemon, and then so on. And some fun facts, actually. The original name of LND was actually called Plasma. But before we made it open source, This was like in January 2016. We named it to LND because we used BTCD, so we made it more like BTCDE, LND being like Eric Damon. So yeah, I was kind of sad I lost the name out to other things that came afterwards, but I was first. We had to ask myself. So no wrong from there, right? So then I ended up starting to work on LND full-time in June 2016 when I graduated from school. I was like, yeah, I can do this Bitcoin stuff full-time now. And then at that point, if you look at the contributor history, you see a big spike afterwards, basically. It was like, we're going to break, and then nothing. I know it was TAA, you know, fading, and then boom, again, and then we started off doing things seriously. As far as the release timeline, we had L&D 0.1 initially in January 2017, and that was basically, oh, we have a thing, it mostly works. I think by then you could basically, you could send payments, you could connect to peers, there was no multi-hop, there wasn't really pathfinding, you didn't handle any of the cases and like that. Then we had 0.2, 0.2 had a little more improvements, you know, we could do things like actually send payments across other peers, we had some basic pathfinding. Then there was 0.3, like most recently, before the recent one, now it's kind of a little bit more like fully fledged, right? We could do multi-app payments, we had, you know, sub, but not all of the kind of like on-chain contract handling. We did have, you know, a little bit more, we have like autopilot there for the first time, which is kind of like this automation system within LND to automatically send us channels. And then now we're here because LND ODOT 4. Oh, that says alpha there. That should be beta, not, I guess I can't be basing it, but that's fine. But yes, like this is like the first beta release. And, you know, LND ODOT 4 matters because this is actually the first release that supports mainnet, right? So beforehand, we had only test and send out support. People got really, really excited and then made net anyway. like earlier on, we were kind of like, oh, we're going to make breaking changes, don't do it yet. So we kind of discouraged people from updating because we had planned break changes in the future, so we knew that if you had channels, you have to close them all down, and there were some little mishaps that people were not getting that communicated. Most of the work within 0.3, alpha and 0.4 beta was mostly around security and fault tolerance. So before, there were no backups at all. If it crashed, everything was in memory. But if you got 400 HLCs, you couldn't resume any kind of multi-step contract handling, everything else like that. But all of that's been taken care of and it's now released. And it was the first release that we could kind of feel comfortable with people who were not made in it. Before otherwise, I'd be nervous, like, oh, oh, he's throwing up on the channel. But now it's kind of like, okay. We have a pretty good degree of confidence that people, that if things go down, L&D will actually correct itself. And also, if it ever crashes, it's able to resume where it was and actually continue to advance that state, which is like a pretty big milestone in order to make things happen. All right, so L&D. Within L&D, we use Go primarily. It's kind of like the first class. It's empty pure Go, right? And Go has several advantages, meaning that I think it's a pretty good choice for creating concurrency software in general. And usually the question becomes, why are you using Go? Why not C or C++ or Rust or whatever else? And I think these are some of the reasons why we use it. And so far, I think we've had pretty good developer uptake. People typically find that the codebase is pretty easy to jump into because the language is kind of familiar. If you know C, you know Python, you kind of know Go. It looks the same. Maybe there's some weird keywords. But for the most part, it's pretty straightforward. So one good thing about Go, it has very, very good concurrency support. So LND itself is very, very recurrent, parallel, the architecture itself and the main things that we use within GoToUthens are called channels in goroutine. Goroutine is basically a lightweight thread, it's like a green thread if you load that, for Python and Weedlets and then it has these things called channels. So you have these threads and they can communicate with each other using channels. So I can send messages back and forth pretty easily. If you think about this, this makes it pretty easy to create these concurrent architectures. So maybe you have some producers saying some things to be a producer and a consumer and then you have a pipeline going down and you can do some script pretty easily. It's used very heavily throughout the entire KBase. My other thing is Go has very, very excellent tooling. I think the tooling is maybe what makes it in terms of assistive programming. So with Go it's very, very easy to just do CPU and heat profiling. So I basically on the weekends I profile. I profile, do memory, I do some CPU profiling. Makes it very easy with something called P prop and you can also actually remotely hit a server to get a Go routine done and things like that. So it makes it easy to do things like that. It actually has a racing system detector. So if you've ever done concurrent programming and your racing missions suck, they're super hard to find and they're kind of like the fandom of what you can't always replicate right but go ahead and think of a station tech for so that the program a little punch of the X unless you kind of catch all these issues right so we always run our test with this and you can even run it develop so if the castle basically just stop everything and then don't be that you know the stackers like Oh concurrent right on map or go like read out to write dependency type of thing and the other cool thing is casanova go up empty and I think about what you that everyone's code looks the same right so this matters a bit more like larger projects we don't have to worry about oh do we do a semi-colon then the brace or like a brace with the you know like with the space do we do like a new line basically right to go from T and then boom, everything looks the same. This is good because you can kind of get through with everyone's code. Everyone's code looks the same, so there's no kind of argument in the code review about like, oh, what's the proper code, so I'll go to automatically. The other thing is standard library is like super, super expensive. Standard library has everything, every crypto thing you need, has networking, it has its own S8 encryption in pure go, it has TLS, it has basically everything you ever need in order to do anything, Bitcoin later, process programming. The other cool thing, it basically produced these technically linked binaries by default, and this is nice because I can just have the binary The other cool thing about this, I can cross-compile super, super easily for any platform at all. And usually you never even need to modify your code, basically. If you write it and compile, it's going to run on that other system. At times you have things around maybe like in a 32 or 64 bit, but those really aren't that essential. And so I have a release group that compiles for like MIPS, like PowerPC, like BSD, just everything pretty easily, which is nice. My probably most favorite thing about Go is that it's a very, very simple language, right? So it's like language itself is very, very simple parsing-wise. You don't even need a table to actually parse it. And as a result, you can kind of like focus on the problem at hand, rather than like, oh, am I using the proper, you know, like sealed class trait like monad? No, no, no, it's like, there's nothing, right? Like you just basically write your code, it's very, very simple, and then you just focus on the problem at hand. And the final thing that we really like about Go, this is a really good set of libraries called PTC Suite, right? So this is written in Go as well, and there's basically anything you need to do in Bitcoin, this library has. So things like, you know, signing transactions, parsing things, addresses, you know, peer-to-peer network. You know, so LND is mostly composed of libraries that interact with PTC Suite, right? we're calling out to BTC with itself, right? So I feel like this set of kind of capabilities made Go and also Go in the concept of Bitcoin, kind of like a very good choice for implementing today with LND. All right, so now the architecture of LND. So like LND kind of has a very pretty particular architecture, and we try to like maintain this whenever we're doing things like code review, writing new subsystems, right? So for the most part, like LND is composed of a set of kind of like independent subsystems, right? And these subsystems run concurrently. Like we talked about before, they use Go routines within the codebase itself to run in isolation, they run in parallel, This is pretty good because when you're reading any of these subsystems, you know that only it can mutate its own state. There's no other thing where you have a grace condition to only grab mutex and all of a sudden the state is inconsistent. Instead, in order for me to walk out of my own state, I need to get a message from somebody else. I get a message, I parse the message, I apply it to my state, and then maybe I just reply back as well. So this is really cool because now you basically have a distributed system within the actual process itself. So we, you know, all the time we have to do things kind of like, you need to be able to ensure that you can handle duplicate messages at delivery as well because if something like, you know, restarts and comes back up, you need to be able to do things like, you have to handle that message being received in a duplicate manner. It's a kind of a virtual way people would message queues, and things like that. And the main thing is this main tenant, like a problem with Go, basically don't communicate by sharing memory, instead share memory by communicating. So this means don't have a single shared map that everyone has a log to and everyone can communicate enough. Instead maybe you have that map inside this Go routine, people send it serialized messages so I can modify the state of it, and then you can actually maybe send them the message to actually read the current state of it itself. And that by itself makes concurrent programming very, very easy to read. about right because otherwise you're kind of like okay did i have like the main lock and like you know the other lock on the entrance did i have re-entering locks like you don't have to worry about that basically just send a message and you come back and you can get the message and go on and then do things on and so the other cool thing about this is that like we can even crash recovery specifically between each subsystem by itself in isolation right so each subsystem basically knows like maybe has like a particular log or has kind of like um you know some like um you know the last message we're going to send or things that we're going to get get back so when we start we can as we know we can basically test um you know fault tolerance very very specifically you know So this is really cool because, you know, like me myself, I have like a very particular like login set up. I have things that I know are spamming. I turn those off other things that maybe you know more important like things around like, you know handling states We're basically coming in from the other Other like period things like that But the important part on these that anything that's actually changed specific is all attracted away, right? And then this is good because this is how we're able to support things like so right now on these supports btc Bitcoin D and neutrino is a back-end, right? But you can also like write your own if you if you have a company you basically have like your own API you can basically plug into that back if you want to write and this is First one is Chain Notifier. Chain Notifier basically does things that we do all the time, like let me know when something's been confirmed, let me know when an output's been spent, and then also let me know when a new block comes in. And if you know things about how channels work, you basically use those three things all the time. Next thing is the Signer. The Signer basically handles signing, so it needs to know how to do things like the segwit by segcaching, it needs to know how to do populate, things like that. And that's cool because it's abstract in a way, right now it's all done in the process, but later on maybe in an own dedicated hardware class. Or it could even be in a remote server which has other kind of like action control policies to prevent you know people just like doing kind of like you know where you're signing stuff we have something called the keychain secret keychain and those themselves handle basically driving the keys in a particular manner so basically we can even have this like be even more segregated something that can just like give us addresses and keys and public keys for using with the contract nothing for actually signing them and then finally we have blockchain io which basically you know you can read the blockchain what's the block uh you know give me the transaction or just think on the sponsor and things like that right cool part about this you can basically swap them out very very easily because of the abstraction that we have on your test set up in the integration test we just know that we can kind of assert the equivalent all these different interfaces together and ensure that Bitcoin D works as well as BTC and other things. But as always, there's maybe education and things like that in between them. All right, did this turn out well? It's kind of intense. I have the architecture diagram from maybe two years ago and it wasn't even, it didn't really look close to this, right? Maybe I could have blown it up a little bit, but I hope you guys can squint and maybe look at it later on. So the way it is, anytime there's an arrow, that either means there's a direct dependency or you're just passing a message to another subsystem itself. So at the very bottom we have Lightning P2P network. And above that we have this thing called Brontide. And Brontide is basically this crypto messaging layer that we have on top of, we use it within Lightning. There's something called Noise made by Trevor Caron who works on Signal and WhatsApp and things like that. Very, very good kind of modern messaging protocol with very modern crypto. Has some cool things around the can't checks to ensure that we have certain properties like identity hiding and non-replay and zero entry times, things like that. Then right above that we have LMWire which does all the framing, so encoding and decoding any messages. The cool part about the way this is set up is that If you want to just take our code base, because everything here is its own individual package, and just connect to the network and listen to what's going on, you can do that. Because everything is very modular and abstract in a way, and it actually has its own unit test as well. Right above the wire, we have the peers. This is basically reading and writing messages from different peers. Then we have the server, which itself is kind of like a hook handling the state of all the peers itself. And above that, we have the RPC server. So obviously, the RPC server is such a part of L&D itself. And that's where any time you interact with any application, they're going to the RPC server. So, we can do something called mac-root, which we'll get to a little bit later. So, mac-root is basically this kind of like bare-earth credential. So, typically you have a username and a password, you basically have a username, and you look up what we made before, like a massive list, like an access control list. Instead, we do something called, we actually have a credential system. So, I can give you a credential that says you can only make new addresses. And I give this to the RPC server and it says, oh, you tried to make a channel? No, that's a slout. And then I can make this address, I can take this new address, mac-root, and say, you can only make new pure SegWit addresses. but then you can't make a nest-bit to a sage or anything else like that. So those capabilities that you can basically delegate backroom, but then also kind of attenuate it down. And this is nice because if you're always sending the application on top of LND, you can kind of partition all your boxes and give them only the responsibility that they need. So you can make channels, you can list the free invoices, and you can send payments. And that's pretty good because from a perspective of kind of like, you know, compartmentalization and having kind of like a separate responsibility to manage all the different subsystems itself. Next, left-back boons, we have GRPC, which we use for our P server itself. what most people communicate with on the daily itself. And then if we move over to the right a little bit, we have the Gossiper, right over to the right of the server. So this is basically dealing with exchange information, where all the problems from channel peers, seeing what's going on as far as new channel, things like that. But then you have the router. So the router hooks directly into the Gossiper, because maybe it's getting a new channel now, and it's committed to the state, right into channel DB. And we have the HTLC switch, which is kind of like the forwarding fabric of LND. So this is like the whole payments as package thing, where it basically has a server like channels, which are all linked, So, this is basically handling the capabilities for the in and out, and I'll get into that a little bit later. And then moving up, wrapping it up a little bit too, we have the signer, the keyer, and the writer above that, who hook into the funding manager, right? So, the funding manager handles basically how do we make new channels, right? So, it basically walks the state machine, like, okay, I signed the funding transaction, signed the commission transaction, I'll get some broadcast, and what goes along with that. And that hooks into above the main three interfaces, wallet controller, chain identifier, and blockchain I.O. And as long as we have the use exit nursery, so this comes into place whenever you have So what it does is it babysits these outputs until maturity, once they're mature, you know, CSV or CLTB, relative to the time lock, and it can then actually sweep those back into the wallet. So as a result, because we have that kind of reusable compatibility, we can use it for any contract in the future, and we have the contract court, which is where disputes happen. So if there's any case where for some reason someone broadcasts to prior state, or we need to go to change time on something else, the contract court handles that and basically communicates to the nursery. So it may be the case that, oh, I have an HCLC, it timed out, so now I need to broadcast, and I give that to a nursery, and the nursery wants to until maturity, maybe it's going to be 100 blocks, then pass it over back to the wallet. And then the final thing is the breach arbiter. The breach arbiter has its own section. This is basically where Justice gets dispatched at times. And by Justice I mean because the contract basically has these stipulations where if you ever do this, I get all the money. If that happens, the breach arbiter gets notified by TNertifier, broadcasts a transaction, writes a disk, and then gives it to the nursery maybe to print some time logs. But that's kind of like general architecture of the way things are right now. And this was a lot simpler in the past. These subsystems came up in the past year or so, which we kind of like, you know, we talked a little bit and realized we wanted a little more flexibility in certain areas. I think they're pretty good as of now, and they are isolated, have those sort of tests, which makes things easier to reason about. All right, so let's talk about, I think it's kind of like an application platform. We just, you know, have some, kind of some update recently. So the cool thing about Lightning is basically that it's kind of like this new development platform of Bitcoin, right? Before maybe as a developer, you needed to know about like, oh, you know, how do I assign a UTXO? What's the sequence value? You know, what's, like, how do I actually even do signatures? What's the SIG hash? Things like that. And that can really kind of get, you know, Intense right and that kind of you know, I'll be like you felt me on big when itself because maybe people never documented but also You just kind of see my cover of war against us, right? And they're the thing that was lightning itself now because we have this like, you know much more streamlined API It's like another layer because we have that way we kind of tracked away and all the other details Right when I open a channel like really more know all the steps below that right? There's like a bunch of things going on like he's signing in your ACLCs and everything else with you you open a channel and I think It's much much simpler doing things like metering for services, or maybe I'm paying, I'm walking somewhere outside and I'm hooking someone in a router and I can pay them via VPN servers to actually connect to the router and maybe get WiFi and find my cab or something like that. I can do things like faster-crossing transactions. So we did a demo, kind of a demo, maybe last October, we showed the ability to swap a typical micro-argue instantly. It's a pretty cool use case, because otherwise, I trust an exchange, maybe I do it on-chain, that takes 30 minutes, 20 minutes, instead that could be instant itself. And we've been calling these LAPs, So we wanted it to be kind of like a platform where people make an application go on, where people integrate exchanges and other things. So that's one of the first things we sat down and thought about in terms of design of it. So one of the main things that we use is gRPC. So gRPC, if you guys don't know, is basically something that's actually built internally within Google called Steppy. open source they call it GRPC instead. So like anything that we can Google basically uses something that's very similar to GRPC. It uses protobufs, if you all know or don't know, basically it's like binary solution format, so you can basically have like a definition of a message, and that composites like any language, right? And almost any of those languages, you can use that same, you know, struct or like dictionary or whatever else in that language. And the cool thing about this is it uses HTTP2. So this lets basically like, you know, have one single connection that multiplexes a bunch of other different messages between themselves. And this is cool because now, like, you know, you don't have to basically like create, use like a special SDK, or hit a rest thing, you can basically code in your own language. So you're in Python, you basically have a generator, you're iterating all this stuff, you're sending messages, or maybe in Swift, you're doing this in iOS, and because of that, you can focus and integrate this very deeply into your business logic, rather than having, okay, I'm talking to L&D, and I'm doing my record program, and kind of let them integrate it together. And the other cool thing about it is it has streaming RPCs, so I can basically have one single connection to get notifications, like, okay, let me know every single time a payment is settled, and I can have a callback that maybe hits off some WebSockets, some JS, together, or I can do things like notify when channels are being opened and closed. But generally, we've seen a lot of people build many applications on this, like people build like explorers, different, we've got Yalls actually, like one of the most popular apps out there today is jlc.me, so we've seen a very, very big community around one of the applications we're really super excited about. Yeah, the CEO of Yalls is here actually. Alex, yeah. And the other thing that we have, we have a rest proxy. So maybe you don't want to use your RPC, maybe you don't want to support it, maybe you just kind of like, you know, you like So basically all this is over to the GRPC server and it's done using JSON. It's pretty easy. So here's an example of me querying for the balance of my channels. So using either of these modes, depending on the application, kind of like a way to use either one of these. Once again, we have macroons. We talked about this a little bit before, but like, because you have these bare credentials, so right now we basically have like one app in macroon that basically does reading, right? We have a reader on the macroon, so we can get this out to someone and they can pull up the channels and then we have like an invoice macroon. So the invoice macaroon is cool because now you can have a server that accepts payments on Lightning and can't do anything else, right? So even if that server is compromised, all they can do is make invoices and make addresses, right? And cause invoice inflation or something, which doesn't really affect you that much, right? So we have some other cool features of macaroons that we haven't yet implemented. So we have what we call a bakery blend. So what the bakery blend allows you to do, I can say, here's a macaroon. It can only make channels below 2 BTC on Wednesday, right? And it's only saying that you can make channels below 2 BTC on Wednesday and Friday. You can basically take that down and have very, very fine So we also have this pretty cool developer site made by Max. This is a pretty cool site. You can actually see every single RPC that LND has. And if you look on the top right, we show example code on the command line, on Python, and also on JS. The cool thing about this is it's automatically generated. So any time we update the protos, this will also get updated as well. And that's api.lightname.community. I did the wrong link. I'll fix it. it afterwards but yeah and then we also have this developer sector L&D which is pretty cool itself again made with Max last summer and this is kind of a target I definitely want to like you know build an L&D so we have a pretty good review section that kind of like gets you in the proper like mindset like application wise like you know how do I think about lighting application how do I think about lightning how do I think about you know what's going on in the hood and like walks you through you know making like the topology make channel updates things like that and also has like a directory of all like the cool like labs we've been working on we even have a tutorial called like coin desk which takes you to kind of like how do you make coin desk but I have a table turn right so we have become very hands-on tutorials for developers actually into the development of things like that and the site itself is open source but anything's out of date you can basically make a poor class and then update the site or do you want to add like new examples maybe use total different languages itself you can do that as well all right now we're getting into more a little bit specific stuff like what we're trying as far as safety you know with the own d itself so we have this thing called cypher c first version is called auz so one thing with lightning is that like unlike a regular wallet we basically have like many many secrets For a while, maybe you just have, I'm using BIP44, so I have my counts and I can get a partition of them and that's it. But for all for Lightning, for every single contract that we have, we maybe need five or six different keys. Right now, it would get more complicated in the future. So as a result, we're thinking, how do we make sure this actually works? How can we make it deterministic? How can we get it back linked up properly? Then we look at these existency formats. So everyone knows BIP39, and that's been out there for a while. But the other thing is that there's 39, there's other BIPs like 43 and 49 and 44 that talk about key derivation. Those are like very very simple and not lightning specific. So we're like well, I guess we gotta make our own. So we kind of did that. But we can go through justification of why we did so. Because you can say it's a pretty big departure away from the regular industry, PFR-13-339, while we can make our own T-format. So one column in 339 that had no birthday. So this maybe works for an API, like maybe inside or wherever else to actually do key rescan. But if I'm on my phone, I don't want to start scanning from Genesis. I could be the first big-part adopter. I could be Califini's future self. So we want to avoid and make sure that you don't have to go all the way back into the chain. But I think it has no version, right? Which means that when I have the seed, how do I know exactly how do I drive my new keys, right? I could have Electrum 2.0 and 10 years later I'm using Python 3.7 and it's like, well, that doesn't work with this product version. The other thing is that the way they do the password, it can actually lead to loss of funds, right? Because you don't actually know if this is the correct password. They have a feature where they try to basically let you have hidden wallets, which I think maybe In this case, it might not actually be that useful. But as a result, if I have my C that was power protected, and I put in an invalid password, it doesn't tell me no, that's wrong. It just says, okay, here's your wallet. It could be $5. It could be $20. But now I don't really know what my wallet password was. Another thing, it has a pretty weak key, weak KDF, right? Like, it basically doesn't really do anything in the parameters today, and maybe didn't really do much in the past. And it doesn't really tell you how to draw out the keys, right? So now I need to maybe, like, have, you know, my wallet and the backup, like, together, and then hope in the future that, you know, I can still use Python 2.4 when we're on, like, Python 5 or something like that, right? So instead we created something called Auz. Auz is kind of like the first instantiation of something we call this Cypherseed family itself. So if you look on the top right, you can see what the format of it looks like. So first thing is we have an external version, right? And then we have a ciphertext and it checks over that ciphertext. The ciphertext basically tells you how to parse everything else. So I didn't write it down, but it's basically like one byte right now and we can basically bump it up in the future. So let's do cool things, like let's say we change the KDEF in the future, or we change other different parameters. take an existing seed and offline program and then basically reconvert it back to a new format. Then we have a cybertext, right? So the cybertext is actually an encryption of this payload above, which is the internet version, which basically tells your wallet how to drive the key. So, you know, this could be like, you know, two, five, seven, or whatever else. And it was basically, okay, I was using SegWit, you know, in 2018, and I used Windows KeyHash, and I used Netscape to make space, and that's it. We have the birthday, and the birthday is used something called, we call it Bitcoin Days Genesis, right? So we realized that we didn't really need like a full, you know, eight byte or four byte timestamp, and so we can say, okay, Bitcoin was created in 2009, nothing else happened before then. Let's just count every single day beyond that. The cool thing about this, we can use two bytes and we can go up to 2088 or whatever. So in the future, maybe if we have other use cases and we have a C format and that can work out. Huh? 2180. 2180. Yeah, a few hundred years. There's also the salt that's missing. Oh, I forgot about the salt. The whole thing is salted, so it's sort of like a mini password-protected database for your C. Yeah, yeah. So there's also the password, which I didn't have it there, but we finally have the check-sum. So again, some interesting traits about this is that from the get-go, if you have a password, the seed itself is actually encrypted, right? So I can maybe potentially delete this out and play the text, and no one can even do anything from that because it's actually encrypted. They need the passphrase. We have a passphrase, we run that through a KDEF, and then we also apply a salt, which is encoded with everything else. And then finally, we have a check-sum on the outside. So when you're decrypting, you can first verify that this is the correct set of words. But then even beyond that, because the cybertext uses AAD, we can use something called AEC which is Arbitrary Equal Size Block Server which means that we can encrypt a very small amount of data and control exactly, without the Mac which it has, which is kind of like another thing, we can have 20 bytes turn into 20 bytes, right? Because it can actually adjust the internal mechanisms to basically decide the input itself. And then finally, because it's actually more or less like an AAD, it has a tag inside of it, right? So there's something called Subject Expansion Factor where we can control how many additional bytes to add onto which controls basically the strength of the tag itself. because now once I actually know the correct words, if I put in a password, I know it's wrong. I don't have to use, now at that point, I don't have to be worried about, you know, kind of maybe like finding out that like, I thought it was the right password, I erased my memory, and then my equals were in there. But now this is basically the C format that we use in L and D. And as we've learned pretty well so far, for most people, it's a little bit different than what people are used to because it's longer, maybe people used to use 12 words. We also do recommend that you add the password phrase as well. But more or less, it basically implements all the things that we needed. It has a version, so we basically know, you know, how can we parse the extra part of it. In the future, if we decide we want a bigger checksum, we can basically have a tool to upgrade the version, link it to someone else, has an internal version to tell you how you want to actually drive the keys, and finally has a birthday so I can know with a lifeline how far back to start with. And then let's talk about backups. So the C format is about how do we re-drive all the keys that we had in the past. Now this is basically, once I have all the keys, what can I do with them, right? Or even now, once I'm actually live, actually updating my channel, how can I do a backup? What happens in the case of data recovery? So one thing to know is that Lightning knows an inherently more stable. We don't just have the keys, we also have the current channel state. That basically tells you what state number are we on, what parameter are we using, what's my balance, what's your balance, where HTMCs are active itself. And as a result, you basically need the current state and you just have the keys. Otherwise you can't update. If you have the keys, you don't know the state, you can't do anything. If you have the state and you don't know the keys, you can't do anything. So instead we basically have a two phased approach. The first stage is basically a dynamic channel backup. And you guys maybe heard a little bit about this, but we have these things called watchtowers. So the idea is that any time you make a state upgrade, or maybe you can even batch them all together, you can export this state. There's different versions, but more or less the important part is the watchtower doesn't know which channel is being watched. You can even now encrypt that data so it doesn't know exactly what webbanses were or what state it was in, things like that. I send that state to the private outsourcer, and as a result now I can just know that if I have 10 of these in the world, maybe 100 in the world, only one of these actually act properly. Then we're actually integrating into LND, we're going to actually add them into the routing node itself. You can also run it with watchtowers, which makes it a little easier in terms of discovery to have one of these participants. But also now you can say, if I am a watchtower, then I can also have a running node and integrate them pretty closely together. The other thing we'll add is that you may have to point them at your own instance. So let's say I have my node and I have a computer at home, I'll be able to back up those things to my computer as well in a redundancy thing, or even have it do some quorum, like 3-5 machine, like the confidence you're on board, the availability zones, or something like that. The other thing is that we may ask things to actually batch the updates together, because otherwise it can like it kind of like a prompt there was like the one app one watch our globally because everyone's updates now basically has like a massive time decide channel attack on the entire network right there's almost anything at the time so we're going to add this kind of like batch cyber process within it to basically ensure that you know we're giving away like you know much minor timing of it uh so like this will probably be integrated in that visual reasonable d we're going to kind of like roll it out here first and then likely create the standardized bolt for it in the end because it'd be nice to basically all the nodes you know they're already squashed ourselves for it they have the exact same messaging kind of like framing structure and then now it's very easy for a node just like you know So, you know, spin up, connect some channels, and also ensure that your signature actually does pass the problem. Yeah, alright, did anyone see this? Happened like a day or so ago. So what happens is that someone, they have their note, they saw the song message on the left, right? It says like, channel state number 20 was broadcast, revoked state 20 was broadcast, remote security, remember peers doing something sketchy, exclamation marks, and then basically once that's done, it says waiting for a confirmation, then the justice will be served. And right, you can see the other note that says, justice has been served, we get our money back. The question is exactly what happened in this case. So what happened was that a user had a notification, and for some reason they ran into some issue, or they didn't know how to get past it, so they shut things down, and they restored everything prior backup. They basically did a CP, they copied their channel state, and as a result, when they came back up, they were on state 25, but really they only had state 20. if they actually broadcast their transaction, they violated the contract, right? Which means the breach operator, which is on the other screen, basically, you know, dispatches justice onto the individual, right? So the question is, like, how do you avoid this from happening, right? So if you have your dynamic states, how do you ensure that if I ever have, like, a backup of the static state, it actually works properly, right? So I'm going to explain that, and we have something called static channel backups, right? So what we can do is we can actually overload the watchtower with some static channel information, right? And this information can be basically per state. And I say it's static because you only need this for, like, whenever a channel is created, one of these and when it's closed you can delete it at the end, right? And because it's static combined with the seed format, basically the backup tells you what keys were used, not the exact keys, but like a key path using the key revision protocol using the seed format itself. It also tells you information around kind of like the node and things like that. So basically given your static backup and your seed, you can re-derive all the keys you actually need and then in the case of basically a part of data loss, you can follow this protocol here, right? So first you fetch the backup and you basically make sure it's, you know, maybe have a MAC, make sure it's actually correct, things like that. I use my seed in the backup too. I can connect back to the nodes I had channels with and when they see me in this particular state they'll activate something we call a data lock protection in the protocol itself. So they'll give me some information needed to actually sweep my funds, they close out the channel on chain and then I can sweep mine directly without any delay. So this is kind of like an emergency scenario thing where if this had been fully implemented the prior state would have been prevented because both sides would realize okay hey I'm getting a closed channel lock because they actually lost some data. And we have this plan from Netcomage to release an LND as well because now it's a state where if I have my state and I have one of these backups anywhere, I can get that money back off-chain. So I can use my C for all my on-chain transactions and for my off-chain, depending on if I have a static backup or dynamic backup, I can also hook into this as well. And we plan to basically have some degrees like streaming RPCs for every single time you open a channel. You can basically grab one of these and like sign it off so that it can be dropped off to wherever else to ensure that you have all your state properly set up. This is basically the safe way to do backups on Lightning. If you ever do anything naive, it's kind of just like copying it and hoping that I have the correct version. Unless you're doing some very complicated versioning system, of yourself, this is what you should be doing. And we'll be implementing this in a very soon here within LND. And hopefully again we'll make this the default standard because it would be cool where any node, if they're using the same seed and they have the same backup format, they can connect to any other node and actually re-derive their keys and get all their money back, which is what we want. We want to collect all the Satoshis because in the future, Satoshis may be important for some. The final thing we have in LND is we have now automatic peer-breached shipping. So before with the prior version of LND, you had to connect manually to other peers. like it was a big pin on other individuals because if I'm on IRC, I don't know anyone, I wouldn't be able to actually connect down to anyone else. So instead we basically added this thing called a peer bootstrapper. So one of the important components within the code itself, you can see this interface on the right, we have something called a network peer bootstrapper. And it's basically pretty generic, basically just takes the number of addresses, knows I shouldn't connect to, and then returns addresses I should connect to itself. And then using this, we can compose this for the multi-source bootstrap to basically ensure that we get a number of new peers from a set of distinct bootstrappers We wanted to see bootstrappers because it could be the case that someone's DNS server goes down, all of a sudden the entire network can actually connect to each other. So the current two bootstrap vectors that we have for the codebase, first we have DNS in Bolt 10. It's very similar to the way DNS in Bitcoin works, because there's a good network, we also run a bunch of Bitcoin DNS stuff, and we have one of those for Testnet, Bitcoin mainnet, I was getting one for Litecoin, but I haven't done it yet, but maybe I'll do it after this talk at some point. And then also we have channel peer bootstrapping. So you only need DNS when you actually connect to the peer for the first time. you basically have the setup of authenticated sign announcements. The other thing is you only accept announcements from individuals that can prove they have a channel open, right? This is more of a simple scenario where they just flood you and maybe do an eclipse attack. Basically, you force them to have some skin in the game. You need to have open UTXOs and channels, and otherwise I won't accept your announcement. So as a result, when you come up, you can connect to the DNS resolver. You can get your initial set of peers. But after that, because you have this data, you're fully independent of yourself. You don't require any other server in the future. as well as add additional versions of bootstrapping, because otherwise we basically want as many redundant sources as possible, and for some reason the DNS server is down, you may have issues actually connecting. One thing we've seen in the wild is that, you're sure DNS resolvers, you know, filter out kind of like our larger SRB records, which, you know, because maybe they don't support like, you know, one-vote UDP, or they don't support HTTP resolution, they're unable to actually connect, so maybe we'll, you know, investigate some other redundant sources of how we can do bootstrapping in a decentralized manner. Final thing that we have here, which is pretty cool, which I could like have an entire other talk on, but, you know, in the past, you basically didn't really know what your network was doing, right? You could look at the logs, but you wouldn't know if you actually forwarded transactions or things like that. So in this one, we basically have a time series database with all completed payment circuits, right? The complete payment circuit is basically when I get an ad, and I forward that in the ad agency, and I get back the settled one, then I get some fee itself now, right? So we basically store all those processes on disk. You may want that for several reasons as far as financial record keeping and also different analysis. The cool thing now is I can look at my node, because it's a time series database, and query, oh, between 2 and 3 p.m., there was lunch, on the West Coast, I have like a spike in like activities and like that, I can look at that and maybe try to analyze and see what's going on. Now I can actually see if my node is running properly, right? So if you look on the top right, you see the fee report command. So what it does, it shows you the fees of all the different channels that you have. You can see like I had a fee of like one base little satoshi and I think it was like .0001% per satoshi after that. And it basically has also a breakdown of like the date. So I made seven satoshis that day. This is like on the test composite. And then over the past month, I've had 145 satoshis, which is not that bad. It's so complex right now because fees are very, very low on testnet in particular. And also there's not that much traffic going on right now on testnet because we have one with like most part. And we also have forwarding history command. And what forwarding history does, by default it shows you the last 24 hours of your forwarding. So you can see I had two payments in the natural refraud period. One was 2k satoshis, the other one was 1,700 satoshis. I had one satoshi fee in all of those. This is pretty cool because now what people can do is they can actually do real-time analysis on their channels. So we have something called autopilot in the daemon right now, where right now when we kind of static channel information, static graph information to see where they should connect and actually set up channels to. When the future, we can basically look at the real-time information of all the channels coming in and then decide, do I want to close out channel B because channel A is getting me more revenue but it's almost depleted, so I can close out money over here and spend some money over here. I can do things like ensure that I have a proper rebalancing schedule to ensure that I can accept any available flow at any given time. Maybe it's the case that I'm getting a lot of cancers over here, so I'm going to ramp up my fees to only have things that I can go in. We can do a lot of things in the future looking into this. People can make very cool, pew-pew graphs of every single payment coming in, when we have an extreme RPC, and things like that. And now, it's Conor's time. Thank you, Lolly. Alright, so we're gonna jump into, for the last half, we're gonna do a couple things mostly related to forwarding HTLCs. that's gonna be talked about here is regarding persistence and safety stuff, and then at the end we'll kind of get a little bit more into the actual onion packets and onion route. So, to start here, this is sort of like a very high level diagram of how the core components of our payment forwarding work. In the middle you have the HTLC switch, which is what sits in the middle and sort of manages all of the surrounding links. A link is like a connection between myself and a person who I have a channel with. And so when I'm actually forwarding payments and I send the onion packets they actually go out over these links. So it's the job of the HTLC switch to sort of be sort of like this financial router that is accepting incoming payments and deciding how to forward them out. So for the lifecycle of this HTLC, we'll start on your left with the blue line, follow it all the way through. The red lines sort of indicate where a packet can fail internally, and it'll just be sort of sent back and failed to the person upstream of where it came from. And the green is actually a successful response You see a green line over there, that's when we actually receive a payment locally. So as soon as we receive it, we check like, oh, we have an invoice, cool, settle it back. And over here, we're actually getting a response from a remote peer and then forwarding it back through. The key components here are the circuit map and the forwarding packages. Those are the primary things that were added here. The forwarding packages are mainly responsible for ensuring reliable forwarding of all the packets internally within the switch. So if we basically write everything immediately to disk, and then we come back up, always know how to resume our state. And it'll sort of aggressively make sure that these are pushed through and get to the outward links. The CircuitMap's job is a little different. It sort of sits in the middle. And what its job is to make sure that we never send a packet through the switch more than once within a particular loop cycle. And so it has to basically handle this job of managing, broadcasting messages between M of M different peers. So this is a huge communication bottleneck problem, and so getting a lot of efficient to there So, we'll start here with the circuit map. So, whenever we get a circuit, or sorry, whenever we get an HTLC, it's assigned a circuit key, which is a tuple of the channel and the HTLC ID. HTLC ID is sort of auto increment for each channel, so they'll just like ratchet up and we'll get them in order. And there's sort of like, when you're forwarding a payment, there's sort of an incoming key, so the person who forwarded to me will assign like some HTLC ID tied to this channel, and then I will sort of go through the circuit map and then assign outgoing key and I will assign it sort of an HTLC on my outgoing channel that the remote peer will then handle. So the job of the circummet primarily is to line up those two incoming and outgoing keys. The primary reason is that when the payment comes back across from remote peer, then I have to look up like by the outgoing key where do I actually send this, or which channel do I actually send this back along, like who was the one who originally sent it to me? And that whole process needs to be persistent because if it doesn't, then we might receive a payment and be like, oh, well, I don't know, drop. And that's like the worst case that could possibly happen here because black-holing a payment is basically the worst case. If I send a payment and it just gets lost by the network or your node goes down and restarts and doesn't know how to handle anything or doesn't realize that it already had received this payment and just drops it, then that's just gonna sit there and time out until the CLTV expires. So that's not too great. Now, some of the big challenges here is that some of the links may not be online at the time I'm actually trying to make this payment, right? So the semantics of an ad, which is when you're going out, and blow the diagram this way, are different than when you're going backwards. So when you're going out, it's kind of like a best case, or best effort forwarding, right? If I'm sending a payment and the row appears not on line, I'll just be like, oh, they're not on line, I'll send the bill back. And so that's a little bit easier. But with the response, right, I need to make sure that always gets back. If I committed to forwarding this HLC and in fact did, then when I get a response, I need to make sure it gets back, otherwise that person's gonna be sitting there waiting for forever, really. So that's one of the big challenges. It's also, in between this whole process, the links can flap. So they might come online, go down, and repeat this process. And we've seen this online, I don't know if you guys have tested L&D, you see this, a node will come up, send an error, go down, come up, send an error, go down. I'm sure a lot of you have seen that. So making sure that all of this stays consistent when that process is happening is a big challenge. And really the big things here, like I said, are reliable, and at most, one semantics. And we have to do all this while at the same time for every single HTLC because I can assure you, and it was tested, if you do it once per HTLC, you'll get a throughput of about 10 transactions per second. And that's actually probably the best case. There's about three subsystems that I was working on where they all had to be joined in concert to do this. And just putting one of them with a singular write per subsystem or per HTLC dropped the performance dramatically. So doing all this in a batched manner was hugely crazy. Like I said, and then forwarding packages, so going back to our diagram, so the forwarding packages are internal to every link. As soon as we receive packets from the outgoing world, we sort of write them to disk, and like I said, that serves the sort of primary state in which we'll read again on startup to make sure that we re-forward these internally. Like I said, they're all batched, and they're batched at the level of the channel updates, so when I do a commit sig, receive revoke an act, All HTLCs that are done or handled in those batches are all processed atomically. And the reason is, it simplifies the recovery logic. When you think about it, if you actually receive a batch of HTLCs, but then actually process them individually, let's say I get a batch of 10, and I start processing the first three, and then I go down. When I come back up, how do I necessarily know where I stopped? It could have been, for example, especially with something that needs to be replay protection. and say, oh, those were good the first time, and I've written that to disk, they were good. Then I come back up, and then I check again, and it's like I've replayed myself. So actually by doing everything in a batch, making sure that our persistence logic is batched at the same level as the actual logical atomicity of the channels, is actually a huge safety feature, in my opinion, and it was also kind of necessary from a performance perspective. So basically, when these forwarding packages, we only ever write to disk with these forwarding packages, in the best case. normal operation, all we do is write to disk, everything's buffered in memory throughout the entire switch, and then only if we crash, we actually have to reconstruct our in-memory state. There are a few edge cases here, because like I said, this is just the flow of a payment in memory. This doesn't include all the actual persistent restarts and stuff, so each system has a bunch of internal recovery procedures and stuff like that. One of the cool things about this design, is that because everything's only written to disk when we come up or in the process of when we are done actually with these forwarding packages, we can garbage collect them totally asynchronously just by reading the disk. We read the disk, be like, oh hey, this one's done, remove it, and we kinda do that once a minute, and that doesn't interfere at all with the channels, it can be done on a level of passive level. Basically the win here is that we're able to batch things heavily, and that is a big win for performance. Moving on to multi-chain stuff, in this latest version of O.4 beta, we restructured the data directory entirely. We now segregate, segregated directories, I don't know. We now separate graph and chain data. So LND right now can support Bitcoin and Litecoin, is what it's configured for. We just added Litecoin D support, which builds, I mean, it's almost entirely the same components as the Bitcoin D backend. that it needs might be headers or if you're using Neutrino, it might be compact filters. And then additionally, each one has a wallet. So in the chain data, as you see, we sort of store them by like, Bitcoin, like whatever testnet, mainnet, subnet you're on, and then the actual data entirely. And then the difference between that and graph data is that graph data is sort of shared across all possible chains you could be listening on. So like if, for example, if you guys saw the Swaps demo, all the graph data for both Bitcoin and Lite and Litecoin was all in the graph directory, whereas they would actually have separate testnet BDC and LTC directories and wallets. So this is a nice separation of directories and concerns, and hopefully that gives you sort of an introductory look ahead as to where this will go when we actually incorporate the full multi-chain daemon support. So another cool thing is in preparing for that, So if you guys are familiar with the bit32 key derivation path, it goes perp, hard to purpose, hard to coin type, hard to count, change, like external or child index. What's up? That's 44. Oh, my bad, yeah, yeah. And then, yeah, so a couple changes that we made to support the configuration schemes that Lola was talking about. We employed purpose 1017. BrickSquad, WakaWaka. We use Slip44 for key derivation, so Bitcoin's zero, it's Snet1, Litecoin is a two. And then for all the key derivation, because we have different types of accounts, the standard wallet might just use a default count of zero, but while we've added all this stuff to do the multi-sig keys, replication base points, payment base points, all these things, so we actually swap out the account for this key family sort of notion. And so this is nice because in the multi-chain setting, we can set the different coin types, and we initialize one wallet per chain So each one of those get placed in its own chain directory if you're using LND in that setting. Those wallets can independently be rescan and recovered, so I'm currently working on all the rescan logics so that you'll be able to just say, restart LND, pop in and look ahead at 1,000 or 5,000 or 20, whatever it is, and it'll just sort of drag keys, scan forward, look for them and update your balance as it goes. And finally, the biggest benefit to all of this is that you can use one AEZ Cypher seed and be able to manage funds on all different chains. Finally, getting to some more onion routing packet construction. So in this last version, we sort of optimized the actual construction process. So before we had sort of a quadratic algorithm that when you're processing like a path length of 20 hops, we basically did this O of N squared algorithm to actually derive all the ephemeral private keys and blinded pub keys. Here are the, like you can see the equations right here. Basically you can see that like you go from zero to I my new pack is also exempt from zero to high, but because they're shared across all of them, we can cache them all. And the effects of this are pretty immense. We saw an 8x speedup dropping the time from roughly down to four and a half milliseconds on my machine, 65 less memory, 77% fewer allocations, and shout out to Jimbo, is he here? Well, he read this up and then it was accepted to the Bolt LN spec, so now it's like the reference implementation for how you derive the onion packets. Or at least like these keys. And then we have some benchmarks down here at the bottom and then finally just a comparison of the number of total scalar and base multiplications that you use to compute them. So pretty big savings. And finally, now a part of this whole HTLC switch forwarding process, or HTLC packet forwarding, is the ability to detect replayed Sphinx onion packets. So if I just take a, so the packets over the network, 1,366 byte onion blob, and I receive that off the network and I'm able to process it and get things like the which hop I'm forwarding to, the amount, the timeout, CLTB, stuff like that. And if someone were to actually just intercept those packets and start replaying them to me, it's sort of like a privacy leak, so people can determine, you can see my actions based off, or the actions I take based off of processing the packet, as well as someone might just process it again or whatever, so we want to prevent that as much as possible, and this operation has to survive restart. So if I send you a bunch of stuff, make you crash or DDoS you, come back up, try to send you the same things, you shouldn't be able to accept that, or you should at least detect it. So the way we implement this is we implement a decaying log, which when I receive this onion blob and parse it, I am actually able to generate a shared secret, one of the ones that's, well the equation isn't here, but yeah. And then we hash that and take about the first 20 bytes and store sort of like an on-disk table of all those. Then when these packets come in, I actually just compare against all the ones that I know, and if any are found to be duplicates, we reject them. And then, in that process, we actually record in that batch which ones were actually marked as replays. Because, going back to the example of when we were processing a batch of 10 HTLCs, and we get down to the first three, if we restart and then come up again, we might actually be replaying ourselves and not actually know the difference. So, to prevent against that, we use this, batch identifier, this ID, which is the short chain ID commit height, and we use that as an identifier, we pass that in. If the batch has already been tested for replays, we just return the actual set, the replay indexes of those packets, and we know off the bat that we don't have to do any more processing, that was just the decision before, and we're gonna deterministically replay that. And yeah, like I said, this is primarily our projection against rejecting packets against ourselves after a restart. Just a quick question about the wallet architecture the wallet that Like the address that you send funds to when you start or when they when you sweep the channel are those in the regular bit 44 path Yes for those we use bit 49 which is like a bit 44 base and that's before that is basically SP to SH but we modified to use for the change address, and then we use, I think it's called BIP84, which is basically just pure Nesquik's cache. So you would have to like rescan for those, one-jab-the-C, and everything else. In your key derivation purposes, you listed a multisig purpose, is that just for the channel anchor? Yeah, that's just for the funny confidence song. What? Do you have another use for it or something? Well, you know, multisig and all that. Yeah, we can add like seven, right? The ciphering is distinct from the encoding, so we can swap out recovery words in the future. I have a library where we went through 100,000 words to find the most memorable, most international words, and the only thing that's left in the thing is I need to do Hamming distance. So if somebody is interested, I've got like 80% of what's necessary for a much better word list. Cool. Yeah, so we can do that in the future if we want to modify word lists or any other parameters. Thank you. Oh, and to add on to that, we can also do different languages as well. So you can translate the same sort of raw encoded cybertext into Japanese or whatever you need. Like French. Can you just talk, I mean we're in the early days of Lightning. Can you talk about how you guys see the network growing and it being adopted? I think according to very early days right now we basically have a bunch of enthusiasts who are very excited, maybe aren't really used to running kind of like interfacing services. They're like, oh, someone's like, you know, doing a TCP have open attack, what do I do? I think, you know, once we get back to the initial phase, we're going to work on ourselves with the Lang Labs. It's kind of giving people the educational resources to know how to operate a node, right? So like, you know, what should you be setting as far as like your kernel settings, you know, how do you want to like actually connect to other peers? How do you want to actually manage your connections, you know, as far as accountability? So we're going to do a lot of work to actually get, you know, the education for the node operators to basically know what they're doing, kind of like, you know, be a little bit more aware of the network. We also have planned this UI for node operators to kind of like be able to look at their node and like look at kind of like analysis of what's going on as far as like, you know, payments coming in, how to optimize my channel, things like that. And then as far as like, you know, merchants, and things like that, I think we're in a phase where they can start experimenting with it like now, because I think, you know, one big thing was for them to see kind of like actually like live on the network beyond like a test, right? So now it's just about manning out all the money, we're actually seeing some people already parenting it. I think right now probably like Bitrefill is like the major, you know, kind of like version on the network and people kind of like, oh, realize, oh, I connect to them and I can like, wrap everyone else. So I think we'll also see different mergers come up and kind of like, you know, take advantage of that. One thing we've seen in the past as well, people offering like discounts for channel creations. Like, oh, you know, open a channel to me and I'll, you know, pay the opening fee or something like that, right? Maybe people will be incentivized to have like different lower fees from the get-go. But, so I think it's kind of in a phase where people, you know, can like, you know, jump on early for your early enthusiasts, but now it's kind of like the early days. People kind of like still be setting up the infrastructure. Then beyond that once we actually see things mature a little bit more, I think, you know, more exchanges will come on because there's a pretty cool thing As far as making them less custodial, you know, having kind of like a hybrid, so I put like a channel and I can push my money where they're next to X-Bay trade. And also maybe, you know, do like cross-change arbitrage. I have an account on two exchanges, I can send, you know, my Litecoin over here, sell it, send the Bitcoin, buy more Bitcoin, that's what you want to do, and then do whatever else I want with it. But yeah, that's your question, if we're supposed to make some. I'm wondering about what the major attack vectors are on the Lightning Network. Yeah, attack vectors include Taking down nodes, I guess, which is why you want to have multiple nodes for a particular service to ensure that you actually have availability of your users. Other things you can do which aren't fully-reputted yet are that you don't necessarily need to be advertising the network. You only need to do so if you're actually running a routing method. So me, as a merchant service, I could basically just be on the edge and not even advertise that they're on there. The only people that actually want to route towards me to actually do different payments can do that. Other things include maybe trying to spam a node with very small channels or something like that. You could basically have policies that I need a channel above you know half a Bitcoin right that kind of adds a cost barrier to actually you know spamming with all this thing other things include I guess doing things on the chain you know if you can kind of like you know make the chain like super full or something like that and make a bunch of tags then we have other defenses in that in terms of kind of like do kind of like a scorched earth approach where if you try to cheat me well I'll just send all your money to my mercies right and that's it so there's you know like that's like the first step I do I don't really care I just want to basically have it to be like a strong deterrent for any cheaters to actually go against me but I forgot to add Tor. In the world somewhere instead. Yeah, it also kind of like depends on like what your definition of attack is whether it's like Inconvenience or whether it's actually like full-on like exploits So I mean there's varying levels of like maybe all those but in general I'd say like the ones that are most practical probably should be like inconveniences but rather than just like full of exploits Yeah, I guess there's things that you know have really merged it maybe kind of like some like active privacy attacks You know people kind of like doing things like actively on the network more on the links like to try to like the otherwise users then I'd be having a go about like any sort of privacy net so we'll see where that comes I'm curious how watchtowers are going to work for more intermediate nodes, like people who have nodes on their cell phones that aren't specifically routing 24-7. like some like small social amount per state itself but then also there could be a thing was like a bonus where if you actually act in sort of justice then you get like you know 10 or something like that so but you know ideally uh once we're all like mobile platforms it's going to be all in the background so you know maybe like so ideally just you know does it for uh you know nodes automatically maybe if i'm a power user i can basically scan through our code and make sure it's back to you know my bitcoin d node as well just like say keeping my constant or whatever else yeah yeah ideally you might have like one or two or three just so you can like also cross-reference them and like you can come up and make sure that they're all they're all system like your state when you went down or if you lost something you try to recover them like they're all telling you the same thing and that's what I think a lot of like he's there yeah we're done the industry with it basically because that's a comic a one event thing but then like you know one thing we do within the current could make sure we actually scale the CSV value according to the to the channel size so maybe it's like a $10 house you know one day but it's like you know 20 kids like give me a few weeks you know just make sure things don't happen there yeah but I think what you're getting at also is that once we have like more effective watch hours and stuff like and like, node availability isn't like as much of an attack factor for like stealing your funds, then we can actually reduce those timeouts. And then like, all the other like inconveniences, like oh my funds locked out for two weeks aren't so much of a problem anymore. So. Yeah, so like as some of the timeouts are kind of like on the higher side, that's because like it's new. So we want people to like, you know, be a little more cautious, even though like it can be, you know, that much more. So, try to be, you know, be grateful basically, right? So. Hi, I was curious, it's a small detail, the macaroons. Could you speak a bit more about them and kind of what some use cases are are and you know, are they a feature for node operators or users? Sure, yeah, I mean, so like they're very, very simple. Basically, it's just an HMAC, right? It's an HMAC of like a root key and then maybe some authenticating data along with that, right? So because of this, you can't actually forge macrune ever and you can't go the other direction, right? So you're able to like attenuate it, maybe like, you know, add on like, you know, channel for 1BTC on Monday by kind of like hashing that down and then I can still verify the root chain itself, right? So basically it has this like final digest but then also, you know, kind of like information on how to reconstruct that digest from the root macrune key. I think Google uses them pretty extensively now. If you can check your headers, you maybe have macros that are being used for them. And in terms of node operators, you can have a setup where maybe your macros aren't even on your node at all. Instead, you can basically download them remotely, and you have a special macros for a particular purpose. But then beyond that, you can have monitoring service. Maybe there's some kind of metric gathering service that wants to see what's going on as far as node operators. You can give them that macro to only collect that data itself. But then I think the other cooler aspect of order, basically, you do microservice assignments. where I can have different distinct instances that only do what they need to do and nothing else. So it kind of isolates them. So if they break into this box, they can only list my channels and maybe not actually make payments or things like that. So right now, like I was saying, we have an admin, which is basically all privileges. We have read-only, you can only do read-only. And then we have invoice, which is basically make addresses and list invoices. But overall, we're going to add tooling to basically let you do very custom macaroons. And this will be cool in the future because you can save maybe more on the mobile operating system. And via Intents, I can pass a macaroon to maybe send a payment every day to some canyon carrier like some video game app, right? And then only using the Mac rune or being intent to pass that to LND, LND can execute on there. So there's pretty cool architecture to having multiple different services with the same LND, but having them all have their strip privileges of what they can do. I was wondering what would happen if you sort of Sachi spam channel anchors? Sachi spam? No, during the Sachi somebody was sending all these ones Satoshi output oh you mean that they just like made a bunch of channels or if people were spamming channel anchors with additional UTXOs would they just get integrated into later commitment transactions would they as can it be lost or do you mean like making a bunch of very small channels or third party spams UTXOs to channel anchors oh I see what you're saying I mean so I mean they wouldn't be a part of the original funding output right and outputs with the same address. So like, LND may, could spend from them, but it may not be worth it if they're only one Satoshi. And you can later add, like, kind of like constraints on what channels you'll accept incoming. Just certain degree of incoming channel is free, it's like, oh, you're gonna put up like $100 to me, like, that's fine. Maybe you don't want like a 10 cents output because maybe that's gonna be dust in the future, right? So you can kind of like, it's not implemented yet, but there's an issue of it on the GitHub. As opposed to beginner issues where they want to actually do development LND. You know, basically have more finite policy on like, what channel do I accept, type of thing. Maybe if someone donated a lot of money it might be worth it. But the fee is required to get your one Satoshi back into your wallet balance is going to be more than your one Satoshi. Another question about the watchtowers. Two little questions. Are they necessarily trusted? Is there any way to outsource it trustlessly? And then secondly, is there anything about it that's deterministic? Like you have the deterministic wallet structure. Can you harden the keys that are used for relocation and then give an ex-pub or an ex-pribe, I guess, the watchtower or that kind of thing? every time your channel updates. Are they trustless? You can say they're trustless because you have them do a particular action and if they don't, you have one other one, they can do that action. Could the Watchtower, instead of returning the funds to you, sweep a channel? No, because I give them a signature and if they create another transaction, it's invalid at that point. So you can say, I tell them what the transaction should look like and we use BX69 to ensure the inputs and outputs are ordered signature, basically a balance information, and they can only do that, right? So if we say, okay, you can take 50% on, you know, like justice enforcement, then they can only do that as well. And you need to set that up in advance, the Watchtower's fee, so you would sign that. Exactly. So it's kind of like, we set up a new negotiation, okay, you know, here's 20%, you know, we want to negotiate a single state, and at that point they can only do that action. So what they can do is just like not act, but then in that case, you know, you want to have these be redundant in other places, and like, you know, it takes like one altruist to secure a bunch of things, and people like, you know, Bitcoin, so maybe, you know, they'll do that. And the second question was about... you can decrypt it and act. And otherwise you have to brute force it, you know, AES-286 or whatever. So, good luck with that. Thanks. Great presentation. So, one problem I've had when spinning up Lightning Nodes is figuring out who to connect to and who to open channels with. And especially if you're trying to receive payments, getting somebody who's willing to open a channel with funds on their side. So do you have any potential solutions or do you envision how this problem will be solved in the future and allowing people to figure out who to connect to? Sure, yeah. Yeah, sure. I think one of the things that will be under development a lot this year and in the future will be further work on autopilot. If you can imagine, I think there's a couple different use cases that users might have. You might be specifically a routing node, you might be a person who's trying to pay, you might be a merchant, you might maybe an exchange or whatever. And so you can sort of think of optimizing the select like the attachment profile of autopilot based on like whatever use cases right or what you're trying to optimize for and there might be different fitness models for which you are trying to like Yeah autopilot is something Lollabill, it's basically like an automated channel maker So it'll look at so it can take you like certain inputs like oh, did you did your wallet balance change? Did you catch somebody something like that? And then it'll try to like look and see like oh, what's a good channel to make so like you can use different heuristics to guide that attachment and And the feature, I think, or at the same time, there could be more matchmaking services. People say, hey, I'm trying to meet, maybe we can link up and there's an exchange for making a channel or something. That's another way of going about it. But in general, I think if you have a more informed and more optimized autopilot, then a lot of these problems might go away, because hopefully you just make channels to a better portion of the network that you're trying to target. Yeah, so the end goal is that you put money in a box that just doesn't, it just works, right? We're not quite there yet, but we're making And the current one doesn't test that, and maybe some people would do it by mainnet, tends to minimize the average hop distance, tend towards a scale-free type network. That's what we're running on testnet. We're going to be doing a lot more experimentation on that front, but even in the code base itself, everything's kind of abstracted at an interface level. So you can basically add what's called an attachment heuristic, which has things like, do I have more channels, and who should I connect to? And right now, it basically only uses data in the graph itself. Like I was saying, in the future, we could also start to actually put in signals from each individual channel, like a channel fitness and then from there kind of like call them down in terms of like what you should be doing. And the other thing about, you know, establishing kind of like inbound liquidity. So I think, you know, there'll be kind of like, you know, liquidity master services where it's kind of like, you know, you can basically buy a channel for income, right? And you can say that someone wants to do that because if you're very popular, then maybe, you know, they'll be earning revenue rather towards you. But you can also say maybe if I'm buying a channel, I can give you, you know, like a credit for like, you know, free 20 payments and like that, right? So 10 buys you for also like, you know, having like outsourced revenue to me. So if you're emergent, maybe, you know, you buy something inbound bandwidth because you compute kind of what your total inflows will be on that day, what you need as well as there. So you can have a set of a reliable bandwidth itself. So it's like peering, but the costs are very, very low because you're opening a channel. You usually need some layers to basically signal what your preferences and pricing and things like that and match with people. And then I think to finish up, I think the last thing that would really help that as well is when we enable dual thunder channels. Because right now, yeah, you might be writing, it might be hard to get someone to be like, hey, just put a bunch of money in this channel to me and nothing else. But if you're going to both put up some collateral and basically be like, oh, we're both being like grab keys initially off the bat without having to wait for the channel to normalize or anything, then you're gonna have a lot better time assigning that, I think. So. Yeah, possibly, possibly. It probably won't be limited to the protocol level just for like identity and privacy reasons, but like, definitely possible. If you want to open a channel just call Justin. So actually that's actually how it works. So I didn't really get to where the actual switch replay protection was actually put in that diagram, but it actually happens at the link level at the outer edges. Almost all the logic in the switch was actually pushed out of the interior as much as possible to sort of enable the performance as you were saying. Even when we're actually adding to the circuit map, because of the nature of the circuit map, of the way incoming channels work. When I'm receiving HTLC and I submit a batch to the switch, the only person who will ever submit that range of channel IDs is that link. So as long as it itself isn't replaying them, there's gonna be no contention there. So those are actually all done in parallel, and then the only thing that goes through the actual center of the switch is just all held in memory. I have two practical questions. I was running lightning deep and then I was running LTCD and I was trying to just grab the graph and compute the number of nodes and it didn't match. I run them against the same Bitcoin node and consistently LightningD, the C++ implementation was giving me about, like recently it was about 800 nodes and LTCD was giving me... I'm talking about 480 or something like that. On which network? Mainnet? On mainnet, both. and they just sort of like come as the best effort thing. The other reason that there might be differences is like depending on the little validation that the different implementations are applying, you might actually be storing invalid valid like announcements or whatever on one, while one is actually doing more heavy filtering, which I would guess is the primary reason, but I don't know for sure. and nothing's gonna happen with it, we'll forget that. The implementation may not forget that in the end. They should be around the same size, but also, depending on your knowledge of private channels, you could have a bigger graph and tell them they don't really know the extent of what private channel back when they were, you could have. There could be some other alternative, private network channel, that no one knows about, but it's in use super heavily right now, and it's bridged via the public network as well. The ability to garbage collect those old nodes is actually really beneficial to your routing performance and usability, I mean if you're spending time trying to route through dead nodes, that's just gonna like increase the time and like the number of trials it takes to actually make a payment go through. So we'll prove that. I see. And the second question that could be just my ignorance. Is it possible, so when you open a channel, you commit some funds to the channel, and is it possible to observe those funds being depleted gradually at the level of network or not? Or you just see the event of closing the channel? Network isn't like RPC or like peer-reviewed network. Let's say RPC. Yeah, for RPC you can just pull the... We have that forwarding event thing now which basically will show you every single forwarding event in the channel. So you can use that to see what's happening. There's not yet a streaming RPC but there will be one in the future. And there's also list channels you can just pull to see what the balance or whatever you want to find out. Last question. Okay, so say I think you mentioned this a little bit earlier when you had the dust in the transaction so a transaction that's super small and then you're going through Intermediate area node and say what if he goes in cooperative or he goes silent and then the channel is like HTLC So you would have to have output for that lock, but it's super small and have an output So would it be stuck there? never gets fully completed. So we will record that in the log. We only record things that actually get extended and then come back across. But then also, you know, no dynamic desk limits, so I can say, okay, I only accept ACLC above like two cases of stoches, so I would not accept anything else, right? And then we can also do things around ensuring that our commitment to action is always valid by consensus or by popular policy. So we'll avoid having a desk output there. So it goes to my employees right now, basically. transaction that you have to have like you can have like a maker like okay or anything like that well it depends in certain those will say maybe like I'm a high value note so I don't like other ones will say I want that cuz I want the fees I want the even the frequency of payments okay because like I wasn't sure like lightning that was supposed to handle like very very large transactions versus like maybe smaller faster daily transaction so yeah we can do both and we also have this thing called amp which kind of like what's used for the larger humans like what's like small channels you can still do like a hundred dollar payment through a bunch of ten dollar channels.
