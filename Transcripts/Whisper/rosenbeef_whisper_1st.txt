My name is Lalu Oshidoku and I work with the Lightning guys and you know me as maybe Rosb from the internet on GitHub, Twitter, everything else. I've been Rosb for the past eight years and I'm still Rosb. That's how it works. And I'm gonna give you a deep dive into the Lightning Network. You know I remember like maybe like two years ago when I myself was learning about Bitcoin initially and I went to these you know Bitcoin meetups, the dev meetups at 20th mission and there's also like you know it was a deep deep dive and Tarek was like the MC see they're like super funny guy so I'm like happy today if you like you know give back to the community now with my own talk and you know it's full circle at this point all right it's a quick outline of the talk first I'll kind of like give like a high-level overview of lightning network I won't be kind of like you know going lightning 101 because this kind of like meant to be like a deep dive talk kind of like more into the you know the core and things with the technology then I'll be going over some updates we've made to kind of like our scripts and the revocation protocol within it and then I'll be going over something I call LCP or the lightning commitment protocol which is kind of like exactly how two nodes in the Lightning Network update the commitment transaction. And you basically have HTLCs flying back and forth, and everything's super fast and stuff. And then I'll also talk about routing for a little bit. It's kind of like a hot topic. And I recently collaborated with Bitfury on a paper where we presented a possible solution to routing. And I'll talk about that paper in detail a bit. And then also what we may be doing initially is kind of like a stopgap before we start to do some more advanced stuff. All right. So Lightning from 10,000 feet, bi-directional channels. So the regular payment channel, you kind of have a two of two multi-sig escrow, right? Where Alice and Bob get together, they put one Bitcoin each, or maybe Bob has one, Alice has zero, and they put that into escrow itself. And in order to do this protocol safely, because we have nested transactions with the funding transaction and the equipment transaction, we require some sort of malleability fix. And the one we currently use within our software is SegWit or SegWit to witness. And basically, if you don't have this malleability fix, you can kind of run into some odd areas where maybe Bob malleates the transaction and then Bob does some random thing and basically says, OK, if you want your money, give me a little bit more. And with the malleability fix, we can avoid that. And then another thing is with CLTV and CSV. And CLTV meaning an absolute timeout, and CSV is like a relative time lock itself. Using these two together, we can basically have channels that have an unbounded lifetime, meaning they never need to close. So we open a channel, and we can leave it open for a year or two at the best case if we really want to. Otherwise, maybe we have a CLTV timeout, which means we have a channel open for one or two days, and then we have to close it out, and then that's the extent of the channel. But using this new design, we can basically have channels open indefinitely. So balance updates themselves are basically like a two-party iterated protocol where we try to do all the updates in an atomic manner for the most part, where I push Alice money, and she accepts this new state and then revokes the old state. It's important that Alice revokes the prior state because otherwise Alice can go back to the state where she has the most money. And in order to do that revocation, we basically rip up all the prior states in itself, such that if Alice tries to broadcast this prior state, then I can punish her, essentially, right? And the way I punish Alice is I use the Bitcoin blockchain itself as what I call, I mean, well, there's a paper by Macaulay. It's called an invisible trust of third party. And you can say, oh, yeah, TTPs are bad, right? We want to avoid that in Bitcoin, right? We can view the Bitcoin blockchain as an invisible trust of third party, meaning that it actually only steps in when it's needed, right? If Alice tries to cheat me and broadcast state one I go to the Bitcoin blockchain, this invisible third party, and it acts as the judge. And with this act to connect the judge, the judge basically enforces fairness for us. And then the judge also adds kind of a clause within this fairness protocol, which allows either side some sort of period where they can attest to the current balance. So if I'm Bob and I want to be sneaky and we're at state five and I try to broadcast state two, I can't immediately sweep all the money. I may have to wait some sort of delay period. Maybe it's a day or two. Maybe it's a week. week and this delay period allows Alice to basically come you know to present to the judge the the blockchain itself a better proof of the current state of the channel and because every single time we step forward I revoke a prior state if I ever go backwards then Alice or Bob or you know whoever is my counterparty can then present this proof to the judge that shows that I you know violated the contract and Alex can move forward and actually continue the balance and she gets all the money essentially so it's kind of you know this scenario where both sides always move forward and if you ever try to move backwards, you basically get punished and you're loaning money. And then, so that's how we actually handle the bidirectional payment channel aspect, which allows both sides to send money back and forward. But one of the main things was the concept of the HTLC, the hash time lock contract. And what HTLC says is basically, I'll pay you two Bitcoin, insert whatever the denomination there, if you can open this commitment within one day. And if we're just doing a single hop, then Bob has the commitment, which basically, I mean, he can open the commitment, which means he has pre-image to that commitment, and it gets opened, and then I pay out to Bob, right? So this is basically like a time lock commitment scheme. And, you know, previously, the way people tried to solve this before is they assumed the existence of what they call time lock encryption, where Alice would basically have the pre-image and encrypt it in such a way that Bob, you know, if he's grinding on all four cores for like two days, can actually decrypt that commitment and then can get his money back. But with Bitcoin, we have cool things like time-based opcodes, And with those op codes, we can actually just use that directly in the system. We don't need to rely on some kind of dubious time encryption that may not actually exist at all. And so if we just do a one hop, that's cool. But we can do multiple hops across the network. And when we do multiple hops across the network, we use these HTLCs in a chain manner, where every single hop decrements the time lock. And as you decrement the time lock, that gives each party a window of opportunity in which, once they get the preimage, they can then settle all the way backwards. the HTLCs, meaning that everyone updates their balances. Everyone has one Bitcoin less all the way to Charlie, and at that point, Charlie settles the HTLC by actually revealing his preimage, and that gets routed all the way back to Alice. Everyone pulls the money, and everything's fine. And we can do this with arbitrary path lengths, essentially. Obviously, if you have a longer path length, maybe there's some issues with the time value of money, how long do I want my money locked up, and how many fees do I pay? But using this, you can have end-to-end security with HTLCs, meaning that Bob can't get paid before Charlie gets paid, and so on. And that's how we do these chain commitments. I actually have the network part of the Lightning Network. And just a little bit of a thing about commitment schemes. For Lightning itself, we require secure commitment schemes. And a secure commitment scheme has basically two traits. The first one is called hiding, and the next one is called binding. And what hiding means is that giving two commitments, you can't tell exactly what was committed to you. And if you guys are familiar with randomized encryption, counter mode and AES or like the IND CPA security definition, this is the exact same thing. So basically, you know, me extending the commitment towards Bob, Bob should have no idea exactly what was committed to because if he knew what it was committed to, he could basically just take that money and Alice never gets the money herself, right? And the next one is we want binding, meaning that, you know, once you actually commit to a certain value, you shouldn't be able to open that commitment with two distinct values. And, you know, if you could do that, you could basically have a collision on the hash function that we use. can collide, then you can present to me multiple input values and I can basically pull the money twice. Or you can pull the money twice. And currently in the scheme we use SHA-256 and you can use any other commitment scheme that satisfies these two properties of hiding and binding. And if you assume a random oracle, which means you assume that a hash function gives you a perfectly random value every single time, you can use that to construct the scheme. And it's perfectly hiding and computationally binding. Meaning you can do collisions if you can do one to the 128 worker, so but we assume that's computationally infeasible. All right, and so one thing I mentioned before is the revocation scheme, right? So for every single state you move forward, you need to provide me a revocation of that prior state. Meaning if you ever go back to, you know, state one and we're on state two, I can take all your money. But, you know, as you can see, this kind of grows linearly, right? Where if we're doing like a billion updates, I have to select a billion, you know, values essentially, and that can be limiting because I need, a lot of storage actually to do this. So what we solve this, we solve this with a compact revocation scheme. And the goals here is that the client, I'm sorry, the sender, meaning the person that's actually doing the updates, should only have to store a constant amount of information. That information is essentially like the root of a special tree and the current update counter. And then yet the receiver just only stores log in. So if we're doing like a million updates, it only stores 20, super efficient, right? And the way we do this, we use something I call like an authenticated PRF, function, yet when you give me all the outputs of the PRF, I can be sure that it came from the PRF that you've created with a particular seed. So every single output I get, I'm like, okay, I checked the proof and this is a correct output, and we can keep moving forward. And the way we do this is something we call outgram. And it's basically like a reverse Merkle tree, right? So typically in a Merkle tree, you have the leaves at the bottom, and you hash all the leaves upwards, right? And then finally you have the root, and I can prove to you that any of the leaves are part of this tree in log n space. But instead, we go the other direction. We start with the root. Alice has a certain secret and a hash key derivation function. In the current code, we use the hash key derivation function, or sorry, HMAC key derivation function. And the way you do this, it's a recursive structure. So you have the root at the top. And if you want to go down to the left node, you hash the root and zero. And then if I want to go down to the right node, I hash the root and one. So if we're saying the tree is of height two, then I basically hash it twice. And now I give you that first node. I give you the second node. I give you the third node. You can check the proof. And now you can basically only store the third node. And you can derive the first or the second based on the hashing scheme itself. And one of these are maintained both for the sender and the receiver. So the sender is sending out values. The receiver gets the hash value, checks it's properly in the tree. If it can truncate its storage because it now has a parent node of some leaves, then it can truncate that. And we keep going. So now with this, if I go back and I broadcast state 9000, you can get state 9000 basically in like log in hashes, right? Because you have one of the intermediate nodes and you hash it left and right and then finally you have the proper node and you broadcast that out, you take my money and everything's fine. And then additionally, another update we've made is in the revocation scheme itself. So like initially the revocation scheme kind of used like a chain of BIP32 hierarchy and then it kind of like went to like hash based revocations. I think it was by like Adam, Back and Rusty. But now we're kind of combining those two using revocation keys themselves. So revocation keys, it's kind of like an optimization. We realize that in the script, this allows us to compress the script sig, meaning what we used to redeem the script itself, and also the length of the output script. And then we can also save an extra hash invocation, essentially. So rather than Bob presenting a secret preimage value in order to do revocation, Bob presents a key. And this key is derived in a certain way such that I can give Bob a key, and he can be sure that if I give him a secret value, then he can get the private key to that corresponding public key, right? So instead, we now use revocation keys. So if we're looking at this, if we say C is Bob's commitment key, and P is like my preimage for that particular state, for Bob to create this next state, I give him this revocation key. What I do is I add his point, you know, his public key to the channel, and then I add, I create a point with the preimage, and then I add that together with Bob's point, right? what Bob gets is Bob gets the verification key. And he uses that and everything's fine. But when I go to the next state, what I do is I give him this p value, right? And once he has p, he can actually derive the private key which corresponds to the verification key. And this kind of like uses a trick in elliptic curves, basically the addition operations commutative. So we can see initially like, you know, c plus g times p goes down to g times little c, which is Bob's private key. And then because you can then, you know, redistribute the math there, you get g times c plus p, and which is some R value. So now when I give Bob the preimage, all Bob does is he takes his regular private key for his commitment key, take the preimage, adds it together, and then he gets the verification key. And as you can see, it simplifies down to the exact same solution. And with this, we get a little more compact scripts, and everything's slightly more efficient. And if you're familiar with pay-to-contract hash, this is what they use where basically you can say that a merchant wants to buy something, they have an initial hash, and use this to drive a payment address for the merchant. And you can say in court, oh, the merchant, about our contract because they can derive the private key if they didn't have the contract itself. It's also used in like vanity address generation where you can give a vanity address generator like a single key and they can just add points together. Because point addition is much faster than like elliptic curve scalar multiplication. And you can also do like a trick called double scalar multiplication to like speed that up where you basically calculate G times C plus G times P in a single step rather than to do it individually. That's like another optimization you can exploit. All right, so the next thing we're going over now is called the Lightning Commitment Protocol. And so what this is, essentially, the Lightning Commitment Protocol is the link layer protocol between two nodes itself. And this is the protocol that the nodes use to actually update the commitment transaction in an efficient manner. So we have HTLCs flinging across. We're sending HTLCs, and they get locked into the commitment transaction. And eventually, Bob settles the commitment transaction. And right off the bat, we want to make this extremely fast, right? protocol, you know, optimize and such that you optimize throughput, then the network is a whole and aggregate. If all the links are optimized, the network itself will be optimized as a whole in terms of the total throughput of the network. And, you know, we have a few goals that we set out when we're designing this protocol. One thing is we want to batch updates, right? If I have, like, you know, 10 HTLCs I want to send, I don't want to wait, you know, for, like, 10 different updates, right? I want to say I'm putting all 10 of these HTLCs on from a transaction at once. You act those changes and we move forward, right? Another thing is I want to be able to pipeline new changes. A scenario where we have basically very high throughput bidirectionally, I wouldn't be able to queue up new changes before you even get back to me. Because otherwise, I have to stop and wait. And if I'm waiting, I could be forwarding HDLCs. I could be making more money. We could be helping out the network better. And another thing is we want to desynchronize updates, meaning that Alice and Bob being connected to each other, we don't want to only have Alice do an update, only have Bob do an update. Because if you imagine a network that's well connected and there's updates going in both directions, to basically create updates independent of one another. And if you can have the desynchronization, then for the most part, you can eliminate blocking. And if you eliminate blocking and you're desynchronized and you can allow batch and pipeline updates, you basically have very, very high bidirectional throughput. And LCP, in a nutshell, is basically kind of like a thought model I'm using right now. I think of Lightning as basically like shadowchains, meaning they're kind of like blockchains, right? Where with Lightning, there's like asymmetric state, meaning I have a community transaction and you have a community transaction, right? then you can imagine I can add updates to the state, and that's basically like the mempool, right? So I'm adding, okay, add this HTLC, add this one to this one, and that's in the mempool. And then when I actually want to commit to the, you know, extend the chain one to move to a new state, I reference the mempool. I say, okay, you know, get those five HTLCs I added, and this is the new state itself, right? So this allows me to basically keep adding new changes in a desynchronized manner. And we have something else we call like the revocation window. And what the revocation window is, it's kind of like a sliding window in TCP, right? Where I can keep adding new updates until I exhaust your window. And this lets me basically, in a desynchronized manner, add new updates before you even reply to me. And if I get to the end of the window, I wait and you act those changes and then I can continue forward. And there's like a timer there and maybe I can also batch these updates in once. And the revocation window also kind of acts as flow control. Where if I'm doing updates fast and you can actually keep up, stop for a second, I wait, I let you add those new things, and we can move forward. In the current protocol, the replication window is fixed. It's maybe like 4 or 16. But you can imagine maybe you can do more advanced things where, OK, I'm moving faster, so you can let me extend the replication window by two, and maybe we can shrink it, and so on. But we keep it simple for now. All right, so I have a quick demo. Not a demo, sorry, a diagram to kind of show the way the protocol works itself. So at the top of the screen, we have Bob's commitment chain, reasonably at state 0. And at the bottom, we have Alice's chain. And in the middle, we have this shared log. And the log is, you can treat it as a mempool. And the log is, for the most part, it's a pen only. And it can be compacted once we know that those entries are no longer needed. And in this example, we have a replication window of size 2, meaning Bob can create two new updates without Alice acting those updates, essentially. So initially, Alice is adding some HTLCs. She has another one and another one. in the log, right? And they're uncommitted yet. So this means that both sides basically have this in their log, but they haven't yet updated the new state to reflect the HTLCs itself. So now what Alice does is Alice creates a new chain in the commitment, right? So Alice creates a new update. And when Alice sends over the update, she sends over a signature, right? But then she also sends over a log index, which indexes into Alice's log and says, you know, this commitment signature includes every single HTLC or every single update, you know, below this log index, right? And this lets Bob say, OK, I have your index. Bob then constructs the proper view. And a view is essentially like a commitment transaction with the proper HTLCs. And the balance is reflected as so. So because we have add A1, A2, A0, Alice then gets her balance decremented by that amount for each of the HTLCs. And we have three pending HTLCs on Bob's commitment transaction, right? So at this point, Bob has the new update, right? Because Alice signed the update to Bob, if he wants to, and everything's okay. But what Bob does now is Bob revokes the prior state, boom, and that basically, he says, okay, I like state one, I don't need state zero. When Bob revokes the prior state, Bob sends over the pre-image for that prior state to Alice. And you'll also notice that when Alice sends over that new commitment update, Alice consumed one item from Bob's verification window. So boom, verification window documented by one, Bob has this new, state, Bob revokes the prior state, and when Bob revokes the prior state, he also adds on to the end of revocation window. So now Alice can now do two more updates without Bob responding in a desynchronized manner. So now Alice has Bob's revocation hash, and now at this point, Bob sends over to Alice, okay, I like that update, here's your version, and then again, Alice revokes it, and then Bob gets this new revocation hash, right? So what we did here is Bob basically pipelined or batched three updates. He added three HDL these at once in a single commitment transaction, rather than basically doing three individual updates. So with this, you can basically batch updates and get very, very high throughput up to a certain amount. One thing you have to be careful about in terms of batching is you need to ensure that the transaction can actually get onto the blockchain. Because you can create something that has maybe like 10,000 outputs, right? And that's going to basically violate the cost or the weight limit in terms of the maximum size of the transaction. But if you stay below that, you can basically keep batching these updates and then do it a single one. Alice has these three HTLCs, and Alice is going to send these HTLCs to Bob. So now Bob's turn, right? Let's say Bob, after some period of time, Bob gets the pre-images. Once Bob gets the pre-images, Bob can settle all three HTLCs in a single transaction, right? So what Bob does is, you know, oh, I thought that was animated, but it's not. Bob sends three settle messages and adds them to the log, and that basically settles, you know, A0, A1, and A2. And then Bob also adds two HTLCs of his own, you know, going in the other direction. So with a single update, you know, there's a brand new set of HTLCs in the transaction. We didn't need to wait and do six or three updates. Bob says, OK, settling all those and then adding my own HTLCs. And then at that point, Bob can now create a new update for Alice. And Alice's update references everything that Alice has done, which are her HTLCs. And then it also references Bob's HTLCs, meaning Bob settles and then the new HTLCs that he adds himself. And then at this point, Alice now says, that state update revokes your prior state and Bob now gets a new revocation hash from Alice. So at this point if Alice tries to broadcast A0 or A1, Bob has the preimage and then Bob can punish Alice. So at this point Alice then replies back to Bob, gives him a green transaction which references again into this shared log and then Bob's like okay I like that too and then that's done itself. So you know this illustrates the concept of LCP where basically both within a certain verification window. And for the most part, things are non-blocking. If at some point Bob is moving too fast for Alice, and Bob keeps extending new HDLCs and new commitment updates, Bob stops for a second. There's a time of waitout period. And then eventually Alice acts on the old changes, and Bob can continue and move forward. All right, now to LND. So LND is the Lightning Network Daemon. The name might change, but this is what we're calling it for now. The language of choice is Go. Go, it's a system language created by Google in about like 08, 09 or so. Initially, it wasn't very popular, but now it's getting more and more ground. It was created by the guys who initially worked on Plan 9 and C, and they kind of had a few principles in mind. One thing is Go is very, very simple. The syntax is extremely simple. It doesn't have a lot of line noise like REST or C++ or anything like that. There's also kind of like a de facto formatting in the language. So it's very easy to work in a large code base because everyone else's code looks the same. The style is enforced by this particular tool. Go also has first-class concurrency support. It has these lightweight threads and also primitives to do message passing between the threads. And that comes in handy in several places in LND because we want this to be as concurrent as possible. We want to basically optimize the daemon itself such that it can handle several HTLCs at once. And it also has a pretty expensive standard library, meaning a lot of things like TLS or some certain cryptographic operations are all in the standard library. And you also get a single standardly linked binary, which is very easy to cross-compile between various operating systems. particular Bitcoin library called BTC Suite. These are the guys who make BTCD. You guys heard of BTCD? It's another alternative node implementation, which I contribute to myself. And I think it's a great library. It has very good documentation, very good testing. I actually think it's a pretty good resource for newcomers to come and learn to Bitcoin. You can basically just jump right into the code. The code is pretty well documented. It's pretty well commented. And for the most part, they're very helpful. You can jump on IRC. We'll help you out. And we use a series of libraries from these guys, from this GitHub repo itself. That includes things like handling scripts, creating transactions, validating transactions, doing things with BIP32, the HTTPS keychain, some Bloom filter stuff for your SPV nodes, and so on. And then the daemon also has an RPC server. And initially, we have two versions. You see the only one is implemented right now. But in the final version, you'll be able to pick between two RPC servers. And the first one is HTTP REST. As you guys know, it's HTTP-based. You do GET and POST and various other verbs. And you can basically use that in terms of a JavaScript or be like a regular command line, and use JSON. Or we can also have an option for a gRPC. And what gRPC is based on protobuf, right? If you guys know protobuf, it's a message serialization layer created by Google. And it basically lets you have this kind of declarative language where you declare it in a file. This is my message. These are the fields. And you can compile that to any language, right? Once you compile that to a language, you basically get free serialization. And the serialization is optimized to a degree. It handles things like variable-like integers and tries to keep things compact for the most part. But gRPC is an extension of that. We're in the same configuration file to file, you can then define an RPC service, right? And the RPC service says, okay, this is foo. Foo takes the message bar and then returns pass. And then you compile that. And that generates an entire server view and then also client binding for that server. And the cool thing about gRPC, it's based on HTTP2, which is kind of like the next generation of HTTP, which actually adds things like binary authorization, flow control, multiplexing, and so on. And the cool fact is that HTTP2 was actually created, or for the most part, the main co-author was Mike Belshi. of BitGo, you know, fun fact for you guys. And another cool thing about gRPC, it supports bidirectional streaming RPCs, meaning that we can have like a stream between a client and server, and the server and client can communicate bidirectionally without having to create a new TCP connection and so forth for every single request itself. And then this is the architecture of the data. So the two things in italics, the wallet controller and the chain notifier, are actually interfaces. And the wallet controller is basically like a bare-bones interface for kind of like a basic Bitcoin wallet, right? This wallet doesn't understand Lightning or anything yet, but it can basically just, you know, give us outputs, give us keys, and then do the basic things for that, right? And then around that, we have the Ellen wallet. And the Ellen wallet is the version of the wallet which encapsulates the wallet controller and interface, and that can actually drive the daemon because it knows what channels are, how to do funding, and so forth. And then we have something called the funding reservations within the wallet. And what this does, it allows the wallet to basically handle concurrent funding reservations, right? Because you can imagine there's a race condition where, oh, this guy wants this one BTC output, right? So that needs to be locked up for the duration of that funding. Because otherwise, you have a double spend. If you have two funds transacted that represent the same output, and the funding reservation helps to make sure that doesn't happen. And then we have the chain identifier. So with Lightning, it's very important, depending on your contract and depending on the time locks, that you watch the blockchain, right? So the chain identifier kind of abstracts that away. And this is also an And this is responsible for things like letting me know when a new block comes in, letting me know when something's been spent, let me know when I have four confirmations in my funding transaction. And this is also abstracted away. So you can implement this with things like an API, Bitcoin D, BTCD, and so forth. And then also with the wallet controller itself, we have some default implementations of the daemon, which include a BTC wallet, which is a wallet created by the same guy as the BTCD. And we're also working on some SPV support. So you can drop that right in in any other wallet, and it should just work perfectly. And then, you know, continuing there, we have the funding manager. And the funding manager kind of bridges the wallet's reservation protocol and the PDP protocol itself. So, you know, these were designed to be relatively decoupled, meaning we can take the same wallet and use it in some other application independent of what we design for our protocol. And then we have the BoltDB. BoltDB is a pure Go embedded database. It's a pretty simple key value store. This is what we use currently to store things like, you know, the state of the channel, what my current identification is. things like the routing table and so on. And next we have the network router. The network router communicates directly with the server, and this is kind of, you know, this incorporates what it learned from the network in terms of the current graph. So the network router knows about all the channels currently open on the network. It knows about, you know, what my current, like, neighborhood size is, and then, you know, that handles, like, the layer three routing. So once a user wants to send a payment, it goes to the network router and then goes to the HTLC switch, which is connected to all the peers. And the switch is just concerned with helping multi-hours forwarding. So you know it treats all the peers and open channels as interfaces. When a payment comes in it knows who to send it to next and then forwards it on the correct interface. And then finally we have the RPC server which lets the user you know control and drive all those aspects. And I guess I'm going to do a demo now. So the server I have right now I have two VPSs right now. One is in New York the other one is in San Francisco and actually with this demo now we have a bit of real latency so we can actually see some real-world scenarios here. On the top right screen here I have a BTCD node running in SimNet mode and you know with SimNet basically I can create blocks instantaneously and I don't have to wait for you know a block to come every 20 minutes or so like it would be in testnet. So here's the node right here do a get info we're gonna have to do it fine. Alright so I'm gonna start up the two LND nodes here's the first one So both nodes are up, they're currently connected to BTCD. So right now to control LND we have a CLI called LNCLI. This is similar to Bitcoin CLI or BTC CTL and basically this lets us do various RPC commands. So first one we have get info and that's like the lighting ID and the identity address. we adjust a hash of the node public key, as we're currently using right now to identify nodes within the network. So we have both nodes up right now. And one thing I'll do real quick, I'll connect the nodes. So like that, both nodes are connected now. And right now, we can do list peers. And we see that we have a node. It's a lighten ID. And there's nothing else really to report because we don't have a channel open with it yet. So to get around that, we can now open a channel. Take off that block parameter actually, had that on there before. But now we say, okay, we're going to open up a channel to period one. We have 100 million satoshis or one Bitcoin and we want to wait exactly for one confirmation before we consider the channel open. So there we go. The channel is open now and both sides are now currently waiting for the channel to finish, right? So one thing we can do here, so basically they went through an initial funding workflow where basically this node right here, the demo one node, basically said, hey, I want to open a channel with you, and then they went through the workflow. Currently in our daemon we only have single funding channels open, and this is just basically just for an interest in simplicity of the implementation and the network itself, and also because if you want to do a dual funding channel where basically maybe we both put in 5 Bitcoin each then you might require you know possibly a little more trust because you know at that point you're working with some stranger and they have your money tied up and if they go away then well you know you need to wait a week or so so you can get inconvenienced. So one thing here is on the left hand screen we have some logs. I'm running in verbose mode just so you guys can see you know all the logs that are actually there right now and initially you see we have the funding request you send that over it gets a response And this is basically just giving you parameters to open the channel, such as various keys we need, parameters like how long do we need to wait for the CSV delay, and it goes through both sides. And then finally, the originating node broadcasts the transaction. So now at this point, both nodes are waiting for a single confirmation, and we can give them that confirmation real quick by having BTCD generate a single block. So boom the blocks have been generated now and now both sides are ready to rock essentially So if we come over to this guy the node who was connected to and we do list peers Then we see you know, we have a channel with the other guy That's one Bitcoin and they have all the money and then if we go over to this guy Again we have you know one Bitcoin channel with the other person and local balance is one Bitcoin so I have all the money right and you know, so we see some log messages over here on the left hand side and And what they're doing here is they're filling up the initial revocation window. And both sides basically just fill this up by sending revocations. But these revocations have basically a nil preimage, meaning they don't actually do anything. And these are just meant to populate the initial revocation window. So one thing I want to show here, I want to show just kind of sending some payments and some of the APIs between the RPC server and the client itself. So I have this little small Go program over here. It first creates a client, and then creating that client basically is just connecting over local host to the daemon itself. And once we have this client, we basically have a stub of the gRPC server itself. So using the stub, we can send payments around as we want to, and we work with native objects in whichever language we're working in. So basically, one of the things gRPC has is it has bidirectional streams, and we're going to be utilizing that here. So what the client does, it creates a stream initially. and the stream basically just opens a new session between itself and the RPC server, and with the stream, it can then send and receive in a non-blocking manner across the stream, and the server can do the same also. So we're just going to show basically here a burst of HTLCs going across, or some micro-payments. So we want to send 2,000 Satoshis, right? But we're going to send 2,000 Satoshis one Satoshi at a time, meaning we're going to complete 2,000 total payments. And the way the loop works here is basically just, Keeps going it's like a while essentially keeps you going until all the Satoshi's are set and then for each send attempt it launches a new go routine and these are basically like a lightweight threads and go and you can want like a thousand of them and you know, there's really not much overhead and they're very small stack and the runtime scheduler handle handles them rather efficiently, so, you know after each of the Pims have been completed. We're gonna basically Push down on the semaphore and then print out the number and then finally at the end I'll be printing out the elapsed time and then kind of like a rough And then so if we come over here to the server, this is basically the way the server code is set up and the path through it. So initially on the right here, we have the RPC server. And this is the method where it's handling the send payment command, right? And with that, it basically reads in from the client. And once it reads in from the client, it launches a new go routine. And that go routine sends the new payment request over to the HTLC switch, right? And then finally, responding to the payment request, back to the client once that's been completed. And then over here on the left, this is the switch itself. It gets the packet and then checks if it has the proper interface or not. And if it has the proper interface, then it finally goes through and attempts to send a payment out if it has sufficient capacity. So that's how the demo is going to be. And we have right here a pre-command binary. So I can hit enter and the demo will run. And as you see, we're done here. And we're scrolling a little bit on the left, but that's just just these log messages because they take more time to actually flush through the buffer, but it's actually done at this point. And you see it took about 1.8 seconds. We sent 2,000 individual updates, and that ended up taking, and we did that in 1.8 seconds, so we have about 1,000 TPS. That means with micropayments, we can just keep doing this, and note that this is only on one channel in a single direction. So you can assume if we do this bidirectionally, we can increase the throughput by twofold, and per channel, and per node and so on, this can really just scale horizontally. And it's only dependent on latency and then also the hardware of the node itself. Currently, within the code, it's pretty much IO bound, just because we're using an inefficient manager to record the states. But that could be improved instead to be like an append-only log and can make things much more efficient. So if you do list peers here, then we see that the remote balance has two and we target 2,000 and we have now 2,000 less than one Bitcoin and we took 13 updates, right? And this guy's already down over here because he has less log messages as the receiver. But as you can see, finally, it extends the local chain and we see that this is the final commitment transaction here. And this commitment transaction has 2,000 Satoshis to us. You know, we have this delay output showing that it's a witness crypt hash. And then, you know, the other side gets the rest and they can, you know, spend their money immediately because this is our version. version of the commitment transaction. And OK, it's still settling over there. But yeah, so that was a quick demo. And at this point, some remaining steps to do with the commitment protocol, LCP, is I'm going to be looking into doing some formal verification in the form of TLA plus or plus cal, which is a modeling framework created by Leslie Lamport used to check the correctness of concurrent protocols. Because we've just created a concurrent protocol, we'd like to have some assurance as to exactly the qualities, make sure that we have libeliness, meaning we don't result in deadlock throughout, and that we have safety, that we'll always end up at the same state, and things of that nature. And if we wanted to send another quick payment, we could do one. OK, I'm sending it 100 satoshis. And that's finished now, right? So now finally, we'd like to close out the channel, right? So when we close out the channel, as we get the channel point, We get the channel point and then now we're going to call it the cat alright, so the closed channel funding TX ID equals Not letting me Copy-paste All right there it is and then I'll put index is equal to zero So just like that now this this side the initiator sends the close channel request The other guy then accepts it and broadcasts the channel. So as you can see here on the left-hand side, the channel has been broadcast. This is the witness spending from the multisig. And we have our two keys, and then we also have the redeem script itself. And both sides get their money. This guy gets his 2,100 Satoshis, and the other guy gets the remainder, and we pay basically a small fee. So now finally, in order to close everything else out, we need to generate another block. So blocks have been generated, both sides have closed the channel, and then finally everything is good. So now if we go back on this guy or either one of them, we do list peers, we see we still have a peer, but there aren't any more channels remaining, and both sides have now set up their balances on the blockchain. So just to recap this demo, we basically brought up two nodes, we opened a channel between them, and we sent 2,000 Satoshis across as individual one Satoshi payment in the micropayment scenario. It took about a second or so, and we achieved around 1,300 transaction per second, assuming each ACLC is in the atomic transaction. And there's about 70 milliseconds of latency between them. But this is completely unoptimized at all. And this is just showing a demo of what we've worked on so far. So that's the end of the demo. Back to the presentation. Thank you. And you guys can pull down the repo. And that works now on testnet or semnet. But obviously, I did testnet or segnet. here, just basically control block creation myself. All right, so now some things about routing. So one issue we run into with Lightning is basically path authentication. If I'm in a network and any node can basically, there's no curation, no one tells me this is the graph, any node can feed me basically an invalid path. And you can imagine me as a node, I get isolated, and then someone feeds me this parallel network that doesn't actually exist. And I say, oh, I'm going to route all my payments through this. I route all my payments, and they become stuck, and I just have to wait the entire time because there's an attacker. To prevent this, we basically authenticate all the path advertisements. So meaning, when you tell me there's a path between Bob and Charlie, you also gave me a proof of that path. And the proof basically consists of two different things. The first part of the proof is SPV proof of the funding transaction, meaning you treat me as a like client, and you give me a proof showing that at some time, this output was created in the blockchain. You show me sufficient work, and I say, OK, this channel was there at some point. But then now I want to know that you actually control the two, you actually have a connection in the network between these two peers and those two peers actually know the private keys of the funding transaction which is the 202 multisig. So to do this we use an aggregate signature of the four pseudonyms, right? So if you can imagine if A1 and A2 are the two identities on the network, let's say they have public keys for identities, and B1 and B2 are the channel public keys, meaning these are the public keys within the blockchain itself in the 202 multisig, both sides, they add those two points together, they're public keys, and they get C1 and C2. They take C1 and C2 and add that together itself. C2 is the group channel public key. In order for us to link all four identities, they generate a signature over some details of the transaction hash, and that signature is a group signature using Ishii Schnorr. We do one single signature which authenticates all the parties. You can do two signatures, but that would allow multiple peers in the network to sign to basically attest to a single channel. And that would give you basically like a non-canonical view of the network. But we want to say, you know, every single peer is connected only, you know, is peer-wise connected both in the network and within the blockhead itself. So, you know, if you send this to me, I'm like, okay, you know, I'll add this because you did some work, meaning you did work to, you know, pay the transaction fees, right, to create the channel, and you also did work in order to actually, you know, lock up some amount of funds in the channel itself. And then Flare. So, you know, I collaborated with Bitfury on a paper we called Flare. It was kind of like an initial approach to some things that we would like to see in terms of routing within the network. So Flare kind of borrows heavily from existing literature in what they call like mannets, or like mobile ad hoc networks. And these are mesh networks. Because the scenario in Lightning is very similar to a mesh network, meaning there's no central provider. No one gives route IP addresses. It should be self-configuring. Nodes may come and leave at any time because they're going offline or not. So we thought that we could basically learn a lot from the literature in router protocols in Mad Hats. And it's a hybrid routing protocol, meaning it combines two phases, right? Typically, you have just proactive routing, or you have reactive routing. And with proactive routing, typically, you basically have a link state or a distance vector, meaning you collect all the information proactively. And then once you actually want to send, you have all the information, right? That comes at a cost. That comes at a storage cost. But then also, that comes at a bandwidth cost to basically handle all the updates from everybody else. So then reactive routing basically says, OK, I won't keep the entire state. to send a payment, I may have to consult the network, which basically adds latency into my connection establishment. And then if the network knows my path, they send it back to me. And then I can actually route and then send packets around. And this is reactive because at the point it actually wants to send, then does it actually go into the network. So Flutter is a hybrid routing protocol which combines these two approaches. So first, there is a reactive state. And in the reactive state, as a node, you have an initial neighborhood radius. And this is maybe like five hops or four hops And within this neighborhood radius, you basically handle all the updates, essentially. You handle all the updates of people opening and closing channels, and you handle people opening and leaving. And because this is only a subset of the entire network, you have savings in both bandwidth and storage. Because rather than worrying about 100 million peers, I only worry about maybe this five in this distance, which is maybe like 100 or so. And then we have this thing called beacons. And beacons borrow from Catamelia, where they add this X-Store distance, meaning you might be a possible beacon catameliator. candidate for myself if our address distance is close. And that address distance is basically, I have my address A and your address B, and we XOR those. And once we XOR those, if you're a possible beacon, then I will add you to my table, and I'll get your routes from you. And these are parameters, the neighborhood size and also the number of beacons. And what happens initially is you connect to the network, you have your initial neighborhood size, and then from there you do the beacon search. So you consult all your current neighbors, like who is close to me? such that I can get a better view of the network, essentially. And because of the way the addresses are generated via hash function, and then also via XOR, this basically allows me to get some random feeler connection down to the network. So I basically have a very, very well-illuminated view of the neighborhood. And then also, in addition, I have some feeler connections out into the network, which are farther away and randomly distributed. And this basically resembles a fog of war, where I initially have a very good view of my local. And then beyond that, it's a little more froggy. And then we have a reactive aspect of it. So reactive comes out when I actually want to send a payment itself, right? And because this is Lightning and we imagine that maybe fee updates are very, very fast, we could flood all those updates and all the fee schedules, but that may consume a large amount of bandwidth and they may change very rapidly. So instead, we know our candidate paths and we basically establish a hornet-onion circuit through this candidate path. And then as we're establishing the circuit with each node within the path, we collect additional payment information and this payment information is in the form of fees. So, you know, initially I have this route, I have this path discovery and then I have, you know, an insurer gets every single one of the candidate routes. I pick the one, maybe I pick two for redundancy, one that has the least amount of fees and then I can actually use that route to send payments and exchange possible additional hashes between me and the other person. But in the case that, you know, let's say my beacons were insufficient, meaning with my local neighborhood and my beacons, What I can do now is I know your address and I can use the beacon to basically do like a DFS search using this address distance and then eventually get to you. So this is similar to basically like an iterated DHT lookup rather than a recursive one. And using this, I will be able to find a path with high probability. There's a drawback, meaning that we don't get optimal routing distance because we're using this high probabilistic structure and we may basically do some unnecessary hops on the way. What this allows us to do is allows us to basically only have a very, very small amount of client state, yet still be able to route with high probability. Initially, we won't be implementing all of Flare, or maybe in it full at all, because some of the optimizations are unnecessary in the initial stage. Maybe we have like 1,000 nodes, and that's good for us. Flare is if you have hundreds of millions, and you don't want to store all the information. But if you only have 1,000 nodes, and every node has maybe 10 channels or so, that's not much state. And everyone can just keep that state initially. use channel proofs again, in order to authenticate all the paths itself. And because we have the global state, we have all the information, we can achieve the optimal path length and find a node in optimal time. And yeah, so by the way, I work with Lightning Labs. And we're also hiring, to add to our team, we're hiring engineers, front end engineers, systems engineers, protocol engineers. And you can find our code, the daemon, which I showed, lightning-network-slash-lnd, and some of the underwriting code I mentioned, which is horn-n-node. in Sphinx to add privacy to the network at Lighting Onion. And yeah, thank you.
