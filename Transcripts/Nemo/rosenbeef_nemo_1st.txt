Hey, my name is Lalu Oshidoku, and I work with the Lightning guys. And you know me as maybe URSB on the internet, on GitHub, Twitter, everything else. I've been RussBee for the past eight years, and I'm still RussBeef. That's how it works. And I'm going to give you a deep dive into the Lightning Network. I remember maybe two years ago when myself was learning about Bitcoin initially, and I went to these Bitcoin meetups, the dev meetups at 20th Mission, and there's also a deep dive, and Tarek was like the MC. And there's also, like, you know, it was a deep dive, and Tarek was like the MC there, like, super friendly guy. So I'm happy to be able to give back to the community now with my own talk. And, you know, it's full circle at this point. All right, just a quick outline on the talk. First, I'll kind of give a high-level overview of the Lightning Network. I won't be kind of like, you know, going Lightning 101 because this kind of meant to be a deep dive talk, kind of like more into the core endpoints of the technology. Then I'll be going over some updates we've made to kind of like our scripts and the revocation protocol within it. And then I'll be going over something I call. Protocol within it. And then I'll be going over something I call LCP or the Lightning Commitment Protocol, which is kind of like exactly how two nodes in the Lightning Network update the commitment transaction. And you basically have HTLCs flying back and forth, and everything's super fast and stuff. And then I'll also talk about routing for a little bit. It's kind of like a hot topic. And I recently collaborated with BitFury on a paper where we presented a possible solution to routing. And I'll talk about that paper in detail a bit. And then also kind of what we may be doing initially as kind of like a stopgap before we start to like. What we may be doing initially is kind of like a stopgap before we start to use some more advanced stuff. All right, so lightning from 10,000 feet, bi-directional channels. So the regular payment channel, you kind of have a two-of-two multi-seq escrow, right? Where Alice and Bob get together, they put one Bitcoin each, or maybe Bob has one, Alice has zero, and they put that into escrow itself. And in order to do this protocol safely, because we have nested transactions with the funding transaction and the equipment transaction, we require some sort of malleability fix. And the one we currently use within our software is Malleability fix, and the one we currently use within our software is SegWit or SeggratedWitness. And basically, if you don't have this malleability fix, you can kind of run into some odd areas where maybe Bob mallets the transaction and Alice's funds are stuck, and then Bob kind of does some random thing and basically says, okay, if you want your money, give me a little bit more. And with a malleability fix, we can avoid that. And then another thing is with CLTV and CSV. And CLTV meaning an absolute timeout, and CSV is like a relative time time lock itself. And CSV is like a relative time time lock itself. Using these two together, we can basically have channels that have an unbounded lifetime, meaning they never need to close, right? So we open a channel and we can leave it open for a year or two at the best case if we really want to. Otherwise, maybe we have a CL TV timeout, which means we have a channel open for one or two days and then we have to close it out. And then that's the extent of the channel. But using this new design, we can basically have channels open indefinitely. And so balance updates themselves are basically like a two-party iterated protocol. Alice updates themselves are basically like a two-party iterated protocol where we try to do all the updates in an atomic manner for the most part, where I push Alice money and she accepts this new state and then revokes the old state. It's important that Alice revokes the prior state because otherwise Alice can go back to the state where she has the most money. And in order to do that revocation, we basically rip up all the prior states in itself such that if Alice tries to broadcast this prior state, then I can punish her essentially. And the way I punish Alice is I use the Bitcoin blockchain. And the way I punish Alice is I use the Bitcoin blockchain itself as what I call, I mean, well, this is a paper by Macaulay. It's called an invisible trusted third party. And you can say, oh, yeah, TTPs are bad, right? We want to avoid that in Bitcoin, right? But you can view the Bitcoin blockchain as an invisible trusted third party, meaning that it actually only steps in when it's needed. If Alice tries to cheat me and broadcast state one and worry state two, I go to the Bitcoin blockchain, this invisible third party, and it acts as the judge, right? And with this acting act the judge, the judge basically enforces. With this ActConect the judge, the judge basically enforces fairness for us, and then the judge also adds kind of a clause within this fairness protocol, which allows either side some sort of period where they can attest to the current balance. So, if I'm Bob and I want to be sneaky and we're at state five and I try to broadcast state two, I can't immediately sweep all the money. I may have to wait some sort of delay period. Maybe it's like a day or two, maybe it's a week. And this delay period allows Alice to basically come to present to the judge the blockchain. come to present to the judge, the blockchain itself, a better proof of the current state of the channel. And because every single time we step forward, I revoke a prior state, if I ever go backwards, then Alice or Bob or whoever is my counterparty can then present this proof to the judge that shows that I violated the contract, and Alec can move forward and actually continue the balance. And she gets all the money essentially. So it's kind of this scenario where both sides always move forward. And if you ever try to move backwards, you basically get punished and you lose all your money. Try to move backwards, you basically get punished and you lose all your money. And then, you know, so that's how we actually handle the bidirectional payment channel aspect, which allows both sides to send money back and forward. But one of the main things was the concept of the HTLC, the hash time lock contract. And what an HTLC says is basically, I'll pay you two Bitcoin, insert whatever the denomination there, if you can open this commitment within one day. And if we're just doing a single hop, then Bob has. If we're just doing a single hop, then Bob has the commitment, which basically, I mean, he can open the commitment, which means he has pre-image to that commitment, and it gets opened, and then I pay out to Bob, right? So this is basically like a time-lock commitment scheme. And previously, the way people tried to solve this before is they assumed the existence of what they call time-lock encryption, where Alice would basically have the pre-image and encrypt it in such a way that Bob, if he's grinding on all four cores for two days, can actually decrypt that commitment and then can get his money back. And then can get his money back. But with Bitcoin, we have cool things like time-based opcodes. And with those opcodes, we can actually just use that directly in the system. We don't need to rely on some kind of dubious time lock encryption that may not actually exist at all. And so if we just do a one-hop, that's cool. But we can do multiple hops across the network. And when we do multiple hops across the network, we use these HTLCs in a chain manner where every single hop decrements the time lock. And as you decrement the time lock, that gives each party a window. And as you decrement the time lock, that gives each party a window of opportunity in which once they get the pre-image, they can then settle all the way backwards. So initially we clear the HTLCs, meaning that everyone updates the balances. Everyone has one Bitcoin less all the way to Charlie. And at that point, Charlie settles the HTLC by actually revealing his pre-image. And that gets routed all the way back to Alice. Everyone pulls the money and everything's fine. And we can do this with arbitrary path lengths, essentially. Obviously, if you have a longer path length, maybe there's some issues with the time value of money. Or path length, maybe there's some issues with the time value of money. How long do I want my money locked up? And how many fees do I pay? But using this, you can have end-to-end security with HTLCs, meaning that Bob can't get paid before Charlie gets paid, and so on. And that's how we do these change commitments. I actually have the network part of the Lightning Network. And just a little bit of commitment schemes. For Lightning itself, we require secure commitment schemes, right? And a secure commitment scheme has basically Secure commitment schemes, right? And a secure commitment scheme has basically two traits. The first one is called hiding, and the next one is called binding. And what hiding means is that, you know, giving two commitments, you can't tell exactly what was committed to. And if you guys are familiar with like randomized encryption, like you know, counter mode and AES or like the IND CPA security definition, this is the exact same thing. So basically, you know, me extending the commitment towards Bob, Bob should have no idea exactly what was committed to. Because if he knew what it was committed to, he could basically just take that money and Alice never gets the money itself. Committed to, he could basically just take that money, and Alice never gets the money. It's herself, right? And the next one is we want binding, meaning that once you actually commit to a certain value, you shouldn't be able to open that commitment with two distinct values. And if you could do that, you could basically have a collision on the hash function that we use. And if you can collide, then you can present to me multiple input values, and basically be, I can pull the money twice, or you can pull the money twice. And currently in the scheme, we use SHA2P6, and you can use. And currently in the scheme, we use SHA2PT6, and you can use any other commitment scheme that satisfies these two properties of hiding and binding. And if you assume like a random oracle, which means you assume that a hash function gives you a perfectly random value every single time, you can use that to construct the scheme. And it's perfectly hiding and computationally binding. Meaning, you can do collisions if you can do the 128 work or so. But we assume that's computationally feasible. All right. And so, one thing I mentioned before is. Right, and so one thing I mentioned before is the revocation scheme, right? So for every single state you move forward, you need to provide me a revocation of that prior state. Meaning, if you ever go back to state one and we're on state two, I can take all your money. But as you can see, this kind of grows linearly, right? Where if we're doing like a billion updates, I have a billion values essentially. And that can be limiting because I need a lot of storage, actually, to do this. So the way we solve this, we solve this with a compact revocation scheme. We solve this with a compact revocation scheme. And the goals here is that the client, I'm sorry, the sender, meaning the person that's actually doing the updates, should only have to store a constant amount of information. That information is essentially like the root of a special tree and the current update counter. And then yet the receiver just only stores log n. So if we're doing like a million updates, it only stores 20. Super efficient, right? And the way we do this, we use something I call an authenticated PRF, meaning it's a pseudo-random function. Yet, when you give me all the outputs of the PRF, function, yet when you give me all the outputs of the PRF, I can be sure that it came from the PRF that you've created with a particular seed. So every single, you know, every single output I get, I'm like, okay, I check the proof and this is a correct output, and we can keep moving forward. And the way we do this is something we call Elkrum. And it's basically like a reverse Merkle tree, right? So typically in a Merkle tree, you have the leaves at the bottom, and you hash all the leaves upwards, right? And then finally, you have the root, and I can prove to you that any of the leaves are part of this tree in log n space. That any of the leaves are part of this tree in log n space, right? But instead, we go the other direction. We start with the root. Alice has a certain secret and a hash, a key derivation function. In the current code, we use the hash key derivation function, or sorry, HMAC key derivation function. And the way you do this, it's a recursive structure, right? So you have the root at the top, and if you want to go down to the left node, you hash the root and zero, right? And then if I want to go down to the right node, I hash the root and one. So if we're saying the tree is of height two, then I basically hash it twice. Tree is of height 2, then I basically hash it twice, and now I give you that first node. And then as we continue, I give you the second node, I give you the third node, you can check the proof, and now you can basically only store the third node, and you can derive the first or the second, you know, based on this commitment scheme. I mean, based on the hashing scheme itself. And when these are maintained both for the sender and the receiver, so the sender is sending out values, the receiver gets the hash value, checks it's properly in the tree, if it can truncate its storage, because it now has a parent node of some leaves, and it can. Truncated storage because it now has a parent node of some leaves and it can truncate that, and we keep going. So, now with this, if I go back and I broadcast state 9,000, you can get to state 9,000 basically in login hashes, right? Because you have one of the intermediate nodes and you hash it left and right, and then finally you have the proper node, and you broadcast that out, you take my money, and everything's fine. And then, additionally, another update we've made is in the revocation scheme itself. So, initially, the revocation scheme kind of used a chain BIP32 hierarchy. the revocation scheme kind of used like a chain BIP32 hierarchy, and then it kind of like went to hash-based revocations. I think it was proposed by Adam, Back, and Rusty. But now we're kind of combining those two and we're using revocation keys themselves. So revocation keys, it's kind of like an optimization. We realize that in the script, this allows us to compress the script sig, meaning what we use to redeem the script itself, and also the length of the output script. And then we can also save an extra hash invocation, essentially. And then we can also save an extra hash invocation, essentially, right? So rather than Bob presenting a secret pre-image value in order to do revocation, Bob presents a key, right? And this key is derived in a certain way such that I can give Bob a key, and he can be sure that if I give him a secret value, then he can get the private key to that corresponding public key, right? So instead, we now use revocation keys. So if we're looking at this, if we say C is Bob's commitment key, and P is like my pre-image for that particular state, for Bob to create Pre-image for that particular state. For Bob to create this next state, I give him this revocation key. What I do is I add his point, you know, his public key of the channel, and then I add, I create a point with the pre-image, and then I add that together with Bob's point, right? And what Bob gets is Bob gets the revocation key, and he uses that, and everything's fine. But when I go to the next state, what I do is I give him this p-value, right? And once he has p, he can actually derive the private key which corresponds to the revocation key. The private key which corresponds to the revocational key. And this kind of like uses a trick in elliptic curves. Basically, the addition operation is commutative. So we can see initially, like, you know, C plus G times P goes down to G times little C, which is Bob's private key. And then, because you can then undistribute the math there, you get G times C plus P, and which is some R value. So now, when I give Bob the pre-image, all Bob does it, you know, he takes his regular private key for his commitment key, take the pre-image, adds it together, and then he gets the revocation key. Take the pre-image, add it together, and then he gets the revocation key. And as you can see, you know, it simplifies down to the exact same solution. And with this, we get a little more compact scripts, and everything's slightly more efficient. And if you're familiar with pay-to-contract hash, this is what they use, where basically you can say that a merchant wants to buy something, they have an initial hash, and use this to drive a payment address for the merchant. And you can say in court, oh, the merchant knew about our contract because they couldn't derive the private key if they didn't have the contract itself. Have the contract itself. And it's also used in vanity address generation, where you can give a vanity address generator a single key, and they can just add points together. Because point addition is much faster than elliptic curve scalar multiplication. And you can also do a trick called double scalar multiplication to speed that up, where you basically calculate G times C plus G times P in a single step, rather than to do it individually. That's another optimization you can exploit there. All right, so the next thing we're going. All right, so the next thing we're going over now is called the Lightning Commitment Protocol. And so, like, what this is essentially, the Lightning Commitment Protocol is the link layer protocol between two nodes itself. And this is the protocol that the nodes use to actually update the commitment transaction in an efficient manner. So, you know, we have HTLCs flinging across. We're sending HTLCs and they get locked into the commitment transaction. And then eventually, Bob settles the commitment transaction. And right at the bat, we want to make this extremely fast, right? Because if you make the link layer, Right at the bat, we want to make this extremely fast, right? Because if you make the link layer protocol optimized and such that you optimize throughput, then the network as a whole in aggregate, if all the links are optimized, the network itself will be optimized as a whole in terms of the total throughput of the network. And we have a few goals that we set out when we're designing this protocol. One thing is we want to batch updates, right? If I have 10 HTLCs I want to send, I don't want to wait for 10 different updates, right? I want to say I'm putting all 10 of these HTLCs onto a transaction at once. Of these HTLCs aren't from a transaction at once, you act those changes and we move forward, right? Another thing is, I want to be able to pipeline new changes. A scenario where we have basically very high throughput bi-directionally, I want to be able to queue up new changes before you even get back to me. Because otherwise, I have to stop and wait. And if I'm waiting, I could be forward HTLCs, I could be making more money, we could be helping out the network better. And another thing is, we want to desynchronize updates, meaning that Alice and Bob being connected to each other, we don't want to only have Alice do an update. We don't want to only have Alice do an update, only have Bob do an update, right? If you imagine a network that's well connected and there's updates going in both directions, they need to be able to basically create updates independent of one another. And if you can add the desynchronization, then for the most part, you can eliminate blocking. And if you eliminate blocking and you're desynchronized and you can allow bash and pipeline updates, you basically have very, very high bi-directional throughput. And LCP in a nutshell is basically kind of like a thought model I'm using right now. I think of Lightning as basically. A thought model I'm using right now, I think of Lightning as basically shadow chains, meaning they're kind of like blockchains, right? Where with Lightning, you know, there's like asymmetric state, meaning I have a commitment transaction and you have a Kumo transaction, right? And then you can imagine I can add updates to the state, and that's basically like the mempool, right? So I'm adding, okay, add this HTLC, add this one to this one, and that's in the mempool. And then when I actually want to commit to the extended chain one to move to a new state, I reference the mempool. I say, okay, A new state, I reference the mempool. I say, okay, get those five HTLCs I added, and this is the new state itself. So this allows me to basically keep adding new changes in a desynchronized manner. And we have something else we call the revocation window. And what the revocation window is, it's kind of like a sliding window in TCP, where I can keep adding new updates until I exhaust your window. And this lets me basically, in a desynchronized manner, add new updates before you even reply to me. And if I get to the end of the window, Before you even reply to me. And if I get to the end of the window, I wait and you act those changes, and then I can continue forward. And there's like a timer there, and maybe I can also batch these updates in once. And the revocation window also kind of acts as flow control, where if I'm doing updates faster than you can actually keep up, I stop for a second, I wait, I let you act those new things, and we can move forward. In the current protocol, the revocation window is fixed. It's maybe like 4 or 16. But you can imagine maybe you can do more advanced things where, okay, I'm moving faster. Maybe we can do more advanced things where, okay, I'm moving faster, so you can let me extend the revocation window by two, and maybe we can shrink it, and so on. But we keep it simple for now. All right, so I have a quick demo. I mean, not a demo, sorry, a diagram to kind of show the way the protocol works itself, right? So at the top of the screen, we have Bob's commitment chain, where he's only at state zero, and at the bottom, we have Alice's chain. And in the middle, we have this shared log, right? And the log is, you can treat it as a bimple. And the log is, you can treat it as a bimple. And the log is, for the most part, it's a pend only. And it can be compacted once we know that those entries are no longer needed. And in this example, we have a revocation window of size two, meaning Bob can create two new updates without Alec act without Alice acting those updates, essentially, right? So initially, Alice is adding some HTLCs. She has another one, and another one. So now we have three HTLCs in the log, right? And they're uncommitted yet. So this means that both sides basically have this in their log. This means that both sides basically have this in their log, but they haven't yet updated the new state to reflect the HTLCs itself. So now what Alice does is Alice creates a new chain in the commitment, right? So Alice creates a new update. And when Alice sends over the update, she sends over a signature, right? But then she also sends over a log index, which indexes into Alice's log and says, you know, this commitment signature includes every single HTLC or every single update, you know, below this log index, right? And what this lets Below this log index, right? And what this lets Bob say, okay, I have your index. Bob then constructs the proper view, and a view is essentially like a commitment transaction with the proper HTLCs, and the balance is reflected as so. So because we have add A1, A2, A0, Alice then gets her balance decremented by that amount for each of the HTLCs, and we have three pending HTLCs on this on Bob's commitment transaction, right? So at this point, Bob has the new update, right? Because Alice signed the update to Bob, Bob can broadcast. Update, right? Because Alice signed the update to Bob, Bob can broadcast if he wants to. And everything's okay. But what Bob does now is Bob revokes the prior state, boom, and that basically, he says, okay, I like state one, I don't need state zero. When Bob revokes the prior state, Bob sends over the pre-image for that prior state to Alice. And you'll also notice that when Alice sends over that new commitment update, Alice consumed one of Bob's revocation, one item from Bob's revocation window. So boom, you know. Item from Bob's revocation window. So boom, you know, a revocation window decremented by one. Bob has this new state. Bob revokes the prior state. And when Bob revokes the prior state, he also adds onto the end of the revocation window. So now Alice can now do two more updates without Bob responding in a desynchronized manner. So now Alice has Bob's revocation hash. And now at this point, Bob sends over to Alice: okay, I like that update. Here's your version. And then again, Alice revokes it, and then Bob gets this new revocation. Alice revokes it, and then Bob gets this new replication hash, right? So, what we did here is Bob basically pipelined or batched three updates. He added three HTLCs at once in a single commitment transaction rather than basically doing three individual updates. So, with this, you can basically batch up this and get very, very high throughput up to a certain amount. One thing you have to be careful about in terms of batching is you need to ensure that the transaction can actually get onto the blockchain. And because you can create something that has maybe like 10,000 outputs, right? And that's going to basically violate the 10,000 outputs, right? And that's gonna basically violate the cost or the weight limit in terms of the maximum size, say, the official transaction. But if you stay below that, you can basically keep bashing these updates and then do it a single one. So now, you know, Alice has these three HTLCs, and Alice is gonna send these HTLCs to Bob. So now Bob's turn, right? Let's say Bob, after some period of time, Bob gets the pre-images. Once Bob gets the pre-images, Bob can settle all three HTLCs in a single transaction, right? So what Bob does is, oh, I thought I was animated, but it's not. Oh, I thought I was animated, but it's not. Bob sends three settle messages and adds them to the log, and that basically settles A0, A1, and A2. And then Bob also adds two HTLCs of his own, going in the other direction. So with a single update, there's a brand new set of HTLCs in the Kubernetes transaction. We didn't need to wait and do six or three updates. Bob says, okay, settling all of those and then adding my own HTLCs. And then at that point, Bob can now create a new update for Alice. Bob can now create a new update for Alice. And Alice's update references everything that Alice has done, which are her HTLCs, and then also references Bob's HTLCs, meaning Bob settles and then the new HTLCs that he adds himself. And then at this point, Alice now says, I like that state update, revokes your prior state, and Bob now gets a new revocation hash from Alice. So at this point, if Alice tries to broadcast A0 or A1, Bob has the pre-image, and then Bob can pump it. 0A1, Bob has the pre-image, and then Bob can finish Alice. So at this point, Alice then replies back to Bob, gives him a commute transaction, which references again into this shared log, and then Bob's like, okay, I like that too, and then that's done itself. So this illustrates the concept of LCP, where basically both sides are completely synchronized. You can batch and pipeline updates within a certain verification window, and for the most part, things are non-blocking. If at some point Bob is moving too fast for Alice, and Bob keeps extending new HDLCs, Bob keeps extending new HDLCs and new commitment updates. Bob stops for a second. There's a timer write-out period. And then eventually, Alice acts one of the old changes, and Bob can continue and move forward. All right, now to LD. So, LND is the Lightning Network daemon. The name might change, but this is what we're calling it for now. The language of choice is Go. If you guys don't know Go, it's a systems language created by Google in about 08, 09 or so. Initially, it wasn't very popular, but now it's getting. Initially, it wasn't very popular, but now it's getting more and more around. It was created by the guys who initially worked on Plan 9 and C, and they kind of had a few principles in mind. One thing is Go is very, very simple. The syntax is extremely simple. It doesn't have a lot of line noise like Rust or C or anything like that. There's also kind of like a de facto formatting in the language. So it's very easy to work in a large code base, because everyone else's code looks the same. The style is enforced by this particular tool. Go also has first-class concurrency support. It has these lightweight threads and also primitives to do message passing. It has these lightweight threads and also primitives to do message passing between the threads. And that comes in handy in several places in LND because we want this to be as concurrent as possible. We want to basically optimize the daemon itself such that it can handle separate HTLCs at once. And it also has a pretty expensive standard library, meaning a lot of things like TLS or some certain cryptographic operations are all in the standard library. And you also get a single statically linked binary, which is very easy to cross-compile between various operating systems. And we use a particular Various operating systems. And we use a particular Bitcoin library called BTC Suite. These are the guys who make BTCD. Have you guys heard of BTCD? It's another alternative node implementation, which I contribute to myself. And I think this is a great library. It has very good documentation, very good testing. I actually think it's a pretty good resource for newcomers to come and learn into Bitcoin. You can basically just jump right into the code. The code is pretty well documented, it's pretty well commented, and for the most part, they're very helpful. You can jump on IRC, we'll help you out. And we use a series of libraries from these guys. And we use a series of libraries from these guys, I mean, from this GitHub repo itself. And that includes things like handling scripts, creating transactions, validating transactions, doing things with BIP32, the HD keychain, some Bloom filter stuff for your SPV nodes, and so on. And then the daemon also has an RPC server. And initially, we have two versions. You see, only one is implemented right now, but in the final version, you'll be able to pick between two RPC servers. And the first one is HTTP REST. As you guys know, it's One is HTTP REST. As you guys know, it's HTTP-based. You do get and post and various other verbs, and you can basically use that in terms of a JavaScript library or be like a regular command line and it's JSON. Or we can also have an option for a gRPC. And what gRPC is based on Perdobuff, right? If you guys know Perdobuff, it's a mess serialization layer created by Google, and it basically lets you have this kind of declarative language where you declare in a file, this is my message, these are the fields, and you can compile that to any language. Once you compile that to a language, you basically get free serialization. And the serialization is Language, you basically get free serialization. And the serialization is optimized to a degree. It handles things like variable-length integers and tries to keep things compact for the most part. But gRPC is an extension of that. Where in the same configuration file, this declarative file, you can then define an RPC service, right? And the RPC service says, okay, this is foo, foo takes the message bar, handle returns bask. And then you compile that. And that generates like an entire server view, and then also client binding for that server. And the cool thing about gRPC, it's based on HTTP2, which is kind of like the next generation HTTP2. Which is kind of like the next generation of HTTP2, I mean, the next generation of HTTP, which actually adds things like binary visualization, flow control, multiplexing, and so on. And the cool fact is that HTTP2 was actually created, or for the most part, the main co-author was Mike Belshi of BitGo, fun fact for you guys. And another cool thing about gRPC, it supports bi-directional streaming RPCs, meaning that we can have a stream between a client and server, and the server and client can communicate bi-directionally without having to create a new TCP connection and so forth. Without having to create a new TCP connection and so forth for every single request itself. And then this is the architecture of the data. So the two things in italics, the wallet controller and the chain notifier, are actually interfaces. And the wallet controller is basically like a bare bones interface for kind of like a basic Bitcoin wallet, right? This wallet doesn't understand Lightning or anything yet, but it can basically just give us altimets, give us keys, and then do the basic things for that. And then around that, we have the LN wallet. For that, right? And then around that, we have the LN wallet. And the LN wallet is the version of the wallet which encapsulates the wallet controller and interface. And that can actually drive the daemon because it knows what channels are, there's how to do funding, and so forth. And then we have something called the funding reservations within the wallet. And what this does is it allows the wallet to basically handle concurrent funding reservations, right? Because you can imagine there's like a race condition where, like, oh, this guy wants this one BTC output, right? So then that needs to be locked up for the duration of that funding. Because otherwise, That needs to be locked up for the duration of that funding. Because otherwise, you have a double spend. If you have two funding transactions that represent the same output, and the funding reservation helps to make sure that doesn't happen. And then we have the chain notifier. So with Lightning, it's very important, depending on your contract, depending on the time locks, that you watch the blockchain. So the chain notifier kind of abstracts that away. And this is also an interface. And this is responsible for things like letting me know when a new block comes in, letting me know when something's been spent. When a new block comes in, letting me know when something's been spent, let me know when I have four confirmations of my funding transaction. And this is also abstracted away. So you can implement this with things like an API, Bitcoin D, BTCD, and so forth. And then also with the wallet controller itself, we have some default implementations of the daemon, which include BTC wallet, which is a wallet kind of created by the same guys as BTCD. And we're also working on some SVP support. So you can drop that right in any other wallet, and it should just work perfectly. And then continuing there, we have the funding manager. And then continuing there, we have the funding manager. And the funding manager kind of bridges the wallet's reservation protocol and the PDP protocol itself. So these were designed to be relatively decoupled, meaning we can take the same wallet and use it in some other application independent of what we design for our protocol. And then we have the Bolt DP. BoltDB is a pure Go embedded database. It's a pretty simple key-value store. This is what we use currently to store things like the state of the channel, what my current identification is. You know, the state of the channel, what my current identification is, things like the routing table, and so on. And next, we have the network router. The network router communicates directly with the server, and this is kind of, you know, this incorporates what it learns from the network in terms of the current graph. So the network router knows about all the channels currently open on the network. It knows about what my current neighborhood size is, and then that handles the layer 3 routing. So once a user wants to send a payment, it goes to the network router and then goes to the HTLC switch, which is connected to all the Network router and then goes to the HTLC switch, which is connected to all the peers. And the switch is just concerned with helping multi-op forwarding. So it treats all the peers and open channels as interfaces. When a payment comes in, it knows who to send it to next and then forwards it on the correct interface. And then finally, we have the RPC server, which lets the user control and drive all those aspects. And I guess I'm going to do a demo now. So the server I have right now, I have two VPSs right now. One is in New York, the other one is in New York. One is in New York, the other one is in San Francisco. And actually, with the seminar, we have a bit of real latency, so we can actually see some real-world scenarios here. On the top right screen here, I have a BTCD node running in Semnet mode. And with SimNet, basically, I can create blocks instantaneously, and I don't have to wait for a block to come every 20 minutes or so, like it would be in testnet. Here's the node right here. Do we get info and everything fine? Alright, so I'm going to start up the two LND nodes. Here's the first one comes up. And then also, here's the second one here. So both nodes are up. They're currently connected to BTCD. So right now, to control LND, we have a CLI called BTC, I mean called LNCLI. This is similar to Bitcoin CLI. I mean, called LNCLI. This is similar to BiconCLI or BTC CTL, and basically, you know, this lets us do various RPC commands. So, first one, we have get info, and that's like the lighting ID and the identity address. The ID is basically just a shot to this hash of the node public key, and this we're currently using right now to identify nodes within the network. So, we have both nodes up right now, and you know, one thing I'll do real quick: I'll connect the nodes. So, like that, both nodes are connected now. So, like that, both nodes are connected now. And right now, we can do list peers. And we see that we have a node, it's a lighten ID, and there's nothing else really to report because we don't have a channel open with it yet. So, to get around that, we can now open a channel. I'm going to take off that block parameter, actually, had that on there before. But now we say, okay, we're going to open up a channel with period one. We have 100 million satoshis or one bitcoin and we want to wait exactly for one confirmation before we consider the channel open. So there we go. The channel is open now. And both sides are now currently waiting for the channel to finish. So one thing we can do here, so basically they went through an initial So basically, they went through an initial funding workflow where basically this node right here, the demo one node, basically said, hey, I want to open a channel with you. And then they went through the workflow. Currently in our Daemon, we only have single funder channels open. And this is just basically just for initial simplicity of the implementation and the network itself. And also because if you want to do a dual funder channel where basically maybe we both put in a five Bitcoin each, then you might require possibly a little more trust because at that You know, possibly like a little more trust because at that point you're working with some stranger and they have your money tied up. And if they go away, then well, you know, you need to wait a week or so. So you can get inconvenienced. So one thing here is on the left-hand screen, we have some logs. I'm running in verbose mode, just so you guys can see all the logs that are actually there right now. And initially, you see we have the fronting request. You send that over, it gets a response, and this is basically just giving you parameters to open the channel. And this is basically just giving you parameters to open the channel, such as various keys we need, parameters like how long do we need to wait for the CSV delay, and you know, it goes through both sides. And then finally, the originating node broadcasts the transaction, right? So now at this point, both nodes are waiting for a single confirmation, and we can give them that confirmation real quick by having BTCD generate a single block. So, boom, the block's been generated now, and now both sides are ready to rock essentially. So, if we come over to this guy, the node who was connected to, and we do list peers, then we see we have a channel open with the other guy, it's one Bitcoin, and they have all the money. And then, if we go over to this guy, again, we have one Bitcoin channel with the other person, and the local balance is one Bitcoin, so I have all the money, right? And, you know, so. Bitcoin, so I have all the money, right? And you know, so we see some log messages over here on the left-hand side, and what they're doing here is they're filling up that initial revocation window, right? And both sides basically just fill this up by sending revocations, but these revocations have basically a nil pre-image, meaning they don't actually do anything, and these are just meant to populate the initial revocation window. So, you know, one thing I want to show here, I want to show, you know, just kind of sending some payments and some of the APIs between the RPC server and the client itself. between the RPC server and the client itself. So I have this little small Go program over here, right? And what this program does, it first creates a client, and then creating that client basically is just connecting over localhost to the daemon itself. And once we have this client, we basically had a stub of the gRPC server itself. So using a stub, we can send payments around as we want to, and we work with native objects in whichever language we're working in, right? We work with native objects in whichever language we're working in, right? So basically, one of the things gRPC has is it has bi-directional streams, and we're going to be utilizing that here. So what the client does, it creates a stream initially, and a stream is basically just opens a new session between itself and the RPC server. And with the stream, it can then send and receive a non-blocking manner across the stream, and the server can do the same also. So we're just going to show basically here like a burst of HTLCs going across or some micropayments. Of HTLCs going across, or you know, some micropayments. So, we want to send 2,000 Satoshis, right? But we're going to send the 2,000, we're going to send 2,000 Satoshis one Satoshi at a time, meaning we're going to complete 2,000 total payments. And the way the loop works here is basically just keeps going, it's like a while loop essentially, keeps it going until all the Satoshis are sent, and then for each send attempt, it launches a new Go routine. And these are basically like lightweight threads and go, and you can launch like a thousand of them, and there's really not much overhead, and they have a very small stack. Not much overhead, and they have a very small stack, and the runtime scheduler handles them rather efficiently. So, you know, after each of the payments have been completed, we're going to basically push down on the semaphore and then print out the number. And then finally, at the end, I'll be printing out the elapsed time and then kind of like a rough TPS metrics, right? And then, so, if we come over here to the server, this is basically the way the server code is set up and the path through it. So, initially, on the right here, we have the RPC server. So, initially, on the right here, we have the RPC server. And this is the method where it's handling the send payment command, right? And with that, it basically reads in from the client. And once it reads in from the client, it launches a new Go routine. And that Go routine sends the new payment request over to the HTLC switch, right? And then finally responds back to the client once that's been completed. And then over here on the left, this is the switch itself. It gets the packet and then checks if it has the proper interface or not. Then checks if it has the proper interface or not. And if it has the proper interface, then it finally goes through and attempts to send a payment out if it has sufficient capacity. So that's how the demo is going to be. And we have right here a pre-comboled binary. So I can just hit enter and the demo will run. And as you see, we're done here. And we're scrolling a little bit on the left, but that's just these log messages because they take more time to actually flush through the buffer. They take more time to actually flush through the buffer, but it's actually done at this point. And you see, it took about 1.8 seconds. We sent 2,000 individual updates, and that ended up taking, and we did that in 1.8 seconds, so we have about 1,000 TPS. So that means with micropayments, we can just keep doing this. And note that this is only on one channel in a single direction. So you can assume if we do this bi-directionally, we can increase the throughput by twofold. And per channel and per node and so on, this can really just scale. Per node, and so on, this can really just scale out horizontally. And it's only dependent on latency and then also the hardware of the node itself. Currently, within the code, it's pretty much I/O-bound, just because we're using an inefficient manager to record the states, but that can be improved instead to be like an append-only log and can make things much more efficient. So, if we do list peers here, then we see that the remote balance has two. See that the remote balance has 200, and we target 2,000, and we have now 2,000 less than one Bitcoin, and we took 13 updates. And this guy's already done over here because he has less log messages as the receiver. But as you can see, finally, it extends the local chain, and we see that this is the final commitment transaction here. And this commitment transaction has 2,000 Satoshis to us. We have this delay output showing that it's a witness script hash, and then the other side gets the rest, and they The other side gets the rest, and they can spend their money immediately because this is our version of the commitment transaction. And okay, it's still settling over there. But yeah, so that was a quick demo. And at this point, some remaining steps to do with the commitment protocol is: I'm going to be looking into doing some formal verification in the form of TLA or PlusCal, which is a modeling framework created by Leslie Lamport, used to check the correct. Leslie Lamport used to check the correctness of concurrent protocols. Because we've just created a concurrent protocol, we'd like to have some assurance as to exactly the qualities, make sure that we have liveliness, meaning we don't result in deadlock throughout, and that we have safety, that we'll always end up at the same state, and things of that nature. And if we wanted to send another quick payment, we could do one. Okay, I'm sending it 100 Satoshis. Shutting it, you know, 100 Satoshis. And that's finished now, right? So now finally, we'd like to close out the channel, right? So the way we close out the channel is we get the channel point. We get the channel point, and then now we're going to close out the channel, right? So we do close channel funding TXID equals, is it not letting me copy paste? All right. Copy paste. Alright, there it is. And then I'll put index is equal to zero. So just like that. Now this side, the initiator, sends the closed channel request. The other guy then accepts it and broadcasts the channel. So as you can see here on the left-hand side, the channel has been broadcast. This is the witness spending from the multisig. And we have our two keys. And then we also have the redeemed script itself. And then we also have the redeemed script itself. And both sides get their money. This guy gets his 2100 Satoshis, and the other guy gets the remainder, and we pay basically a small fee. So now, finally, in order to close everything else out, we need to generate another block. So blocks have been generated, both sides have closed the channel, and then finally, everything is good, right? So now if we go back on this guy or either one of them, we do list peers, we see we still have a peer, but Peers, we see we still have a peer, but there aren't any more channels remaining, and both sides have now set up their balances on the blockchain. So, just to recap this demo, we basically brought up two nodes, we opened a channel between them, and we sent 2,000 Satoshis across as individual one Satoshi payments in the micropayment scenario. It took about a second or so, and we achieved around 1,000 transactions per second, assuming HCLC is an atomic transaction, and there's about like 70 milliseconds of latency between them, but this is completely. There's about like 70 milliseconds of latency between them, but this is completely unoptimized at all. And this is kind of just showing a demo of what we've worked on so far. Yeah, so that's the end of the demo. Back to the presentation. Yay! Thank you. Yeah, and you guys can pull down the repo, and that works now on testnet or SimNet. But obviously, I'm a testnet or SegNet, but I did SebNet here just to basically control block creation myself. All right, so now some things about routing, right? All right, so now some things about routing, right? So one issue we run into with Lightning is basically path authentication, right? If I'm in a network and any node can basically, you know, there's no curation, no one tells me this is the graph, any node can feed me basically an invalid path, right? And you can imagine me as a node, I get isolated, and then someone feeds me this parallel network that doesn't actually exist, and I say, oh, I'm going to route all my payments through this. I route all my payments, and they become stuck, and I just have to wait the entire time because there's an attacker. So to prevent this, we basically all authenticate. Because there's an attacker. So, to prevent this, we basically authenticate all the path advertisements, right? So, meaning when you tell me there's a path between Bob and Charlie, you also gave me a proof of that path, right? And the proof basically consists of two different things. The first part of the proof is an SPV proof of the funding transaction. Meaning, you treat me as a like client, and you give me a proof showing that at some time this output, this output was created in the blockchain. You show me sufficient work, and I say, okay, this channel was there at some point. But then now I want to know that you actually control the two. Now, I want to know that you actually control the two, you actually have a connection in the network between these two peers, and those two peers actually know the private keys of the funding transaction, which is the 2 of 2 multisig. So, to do this, we use an aggregate signature of the four pseudonyms, right? So, if you can imagine if A1 and A2 are the two identities on the network, let's say they have public keys for identities, and B1 and B2 are the channel public keys, meaning these are the public keys within the blockchain itself in the 2 of 2 multisig, both sides they add those two points. And the 2 of 2 multisig. Both sides, they add those two points together, their public keys, and they get C1 and C2. And then they take C1 and C2, and then they add that together itself. And C2 is basically kind of like the group channel public key. And in order for us to link all four identities, what they do is they generate a signature over some details of the transaction hashed. And that signature is a group signature using Ishi Snor, meaning rather than doing four individual signatures, we do one single signature which authenticates all the parties. Signature which authenticates all the parties. And you could do two signatures, but then that would basically allow two, that would allow multiple peers on the network to basically attest to a single channel. And that would give you basically a non-canonical view of the network. But we want to say, every single peer is connected only, is peer-wise connected both in the network and within the blockchain itself. So if you send this to me, I'm like, okay, I'll add this because you did some work, meaning you did work to pay the transaction fees to create the channel, and you also did work in order. Fees to create the channel, and he also did work in order to actually lock up some amount of funds in the channel itself. And then Flare. So I collaborated with BitFury on a paper, we call it Flare. It was kind of like an initial approach to some things that we would like to see in terms of routing within the network. So Flare kind of borrows heavily from existing literature in what they call Manettes or mobile ad hoc networks. And these are mesh networks. Because the scenario in Lightning is very similar to a mesh network. mesh networks because the scenario in Lightning is very similar to a mesh network meaning you know there's no central provider no one gives routing IP addresses it should be self-configuring nodes may come and leave at any time because they're going offline or not so we thought that we could basically learn a lot from you know the literature in routing protocols in MANAS and it's a hybrid routing protocol meaning it combines two phases right typically you have just proactive routing and then you or you have reactive routing and with proactive routing typically you basically you have like a link state or distance vector meaning you collect all the information proactively distance vector, meaning you collect all the information proactively, and then once you actually want to send, you have all the information, right? That comes at a cost, that comes at a storage cost, but then also that comes at a bandwidth cost to basically handle all the updates from everybody else. So then reactive routing basically says, okay, I won't keep the entire state, and when I actually want to send a payment, I may have to consult the network, which basically adds latency into my connection establishment. And then if the network knows my path, they send it back to me, and then I can actually route and then send packets around. And then I can actually route and then send packets around. And this is reactive because at the point it actually wants to send, then it's actually going to the network. So Flare is a hybrid routing protocol which combines these two approaches. So first there's a reactive state. And in the reactive state, you have, as a node, you have an initial kind of neighborhood radius, right? And this is like maybe five hops or four ops at max. And within this neighborhood radius, you basically handle all the updates, essentially. You handle all the obvious people opening and closing channels, and you handle people opening and leaving. People opening and closing channels, and you handle people opening and leaving. And because this is only like a subset of the entire network, you have savings in both bandwidth and storage. Because rather than worrying about 100 million peers, I only worry about maybe this 5 and this distance, which is maybe like 100 or so. And then we have these things called beacons, right? And beacons kind of borrow from Cadame, where they add this X store distance, meaning you might be a possible beacon candidate for myself if our address distance is close. And that address distance is basically: I have my address A, and you're And that address system is basically: I have my address A and your address B, and we extore those. And once we extore those, if you're a possible beacon, then I will add you to my table, and I'll get your other routes from, I'll get your routes from you. And each, these are parameters, the neighborhood size, and also the number of beacons. And what happens initially is you connect to the network, you have your initial neighborhood size, and then from there you do the beacon search, right? So you can salt all your current neighbors, like who is close to me, such that I can get a better view of the network, essentially. such that I can get a better view of the network essentially. And because of the way the addresses are generated via hash function and then also the XOR, this basically allows me to get some random feeler connections out into the network. So I basically have a very, very well-illuminated view of the neighborhood. And then also in addition, I have some feeler connections out into the network which are farther away and randomly distributed. And this basically resembles a fog of war, where I initially have a very good view of my local, and then beyond that, it's a little more froggy. Local, and then beyond that, it's a little more fruggy. And then we have a reactive aspect of it. So, reactive comes out when I actually want to send a payment itself, right? And because this is Lightning, and we imagine that maybe fee updates are very, very fast, we can flood all those updates and all the fee schedules, but that may consume a large amount of bandwidth, and they may change very rapidly. So, instead, we know our candidate paths, and we basically establish a Hornet Onion circuit through this candidate path. Hornet onion circuit through this candidate path. And then as we're establishing the circuit with each node within the path, we collect additional payment information. And this payment information is in the form of fees. So initially I have this route, I have this path discovery, and then I have onion circuits every single one of the candidate routes. I pick the one, maybe I pick two for redundancy, one that has the least amount of fees, and then I can actually use that route to send payments and exchange possible additional hashes between me and the other person. But in the case that, let's say, And the other person. But in the case that, you know, let's say my beacons were insufficient, meaning with my local neighborhood and my beacons, I wasn't able to find a proper path. What I can do now is I know your address, and I can use the beacon to basically do like a DFS search using this address distance and then eventually get to you. So this is similar to basically like an iterated DHT lookup rather than a recursive one. And using this, I will be able to find a path of high probability. We have to find a path of high probability. There's a drawback, meaning that we don't get optimal routing distance because we're using this probabilistic structure and we may basically do some unnecessary hops on the way. But what this allows us to do is allows us to basically only have a very, very small amount of client state, yet still be able to route with high probability. So, initially, we won't be implementing all of Flurr or maybe in it full at all because some of the optimizations are unnecessary in the initial stage, right? Maybe we have like a thousand. Are necessary in the initial stage, right? Maybe we have like a thousand nodes and that's good for us, right? Flare is kind of if you have like you know hundreds of millions and you don't want to store all the information. But if you only have a thousand nodes and every node has maybe like ten channels or so, that's not much state, right? And everyone can just kind of keep that state initially. And we use channel proofs, again, you know, in order to authenticate all the past itself. And because we have the global state and we have all the information, we can achieve the optimal path length and find a node in optimal time. And yeah, so by the way, I work with Lightning Labs, and we're also hiring, you know, to add to our team. We're hiring, you know, engineers, front engineers, systems engineers, protocol engineers. And you can find our code, the daemon, which I showed, Lightning Network slash LND, and some of the underwriting code I mentioned, which is Hornet in Sphinx, to add privacy to the network at Lightning Onion. And yeah, thank you.
