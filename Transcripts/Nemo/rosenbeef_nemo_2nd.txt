Alright, so starting first, the genesis of LND. So here's the actual first commit in LND. It was October 27th, 2015. And this is basically when I was in school still, finishing up school, and this is basically mostly doing my winter break. So this is kind of like the first major sprint that we had in terms of like code. We talked about what LMD were using, as far as the licensing, kind of like architecture of the daemon, and then so on. And some fun facts actually, the original name of LND was actually called Plasma. But before we made it open source, Before we made it openings, open source, this was like in January 2016, we named it to LND because we used BTCD, so we made it more kind of like BTCDE, LND being like Emeritum. So yeah, I was kind of sad I lost Maymount to other things that came afterwards, but I was first. We had to cost me, so go wrong from there, right? So then I ended up starting to work on LND full-time in June 2016 when I graduated from school. And I was like, yeah, I can do this Bitcoin stuff full-time now. And then at that point, if you look at the kind of like contributor history of GitHub, you see a big spike afterwards. Contributor history on GitHub, you see like a big spot afterwards, basically. It was like you know, render break, and then nothing, you know, school, finals, TAing, you know, fading, and then boom, again. And then we started off doing things seriously. As far as the release timeline, we had LMD 0.1 initially in January 2017, and that was basically, oh, we have a thing, it mostly works. I think by then you could basically, you could send payments, you can connect to peers, there was no multi-hop, there wasn't really pathfinding, didn't handle any case or anything like that. Then we had 0.2, 0.2 had a little more improvements, you know, we could do things. ODA, ODA2 had a little more improvements. You know, we could do things like actually send payments across other periods. We had some basic path binding. Then there's O.3, like most recently, before this recent one, now it's kind of like a little bit more fully fledged, right? We could do multi-half payments. We had some, but not all of the kind of on-chain contract handling. We didn't have a little bit more, we had autopilot there for the first time, which kind of like its automation system within LND to automatically establish channels. And then now we're here because LND ODA4. Oh, that stays alpha there. That should be beta. I guess I copy and pasted it, but that's fine. But yes, this is like the first beta release. It's like copy and pasted it, but that's fine. But yes, this is like the first beta release. And LND 104 matters because this is actually the first release that supports mainnet, right? So beforehand, we had only test and sign-up support. People got really, really excited and did mainnet anyway. Like earlier on, we're kind of like, oh, we're going to make breaking changes, don't do it yet. So then we kind of discouraged people from updating it because we had planned to make changes in the future. So we knew that if you had channels, you have to close them all down. And there were some little mishaps that people were not getting that communicated. We also worked within ODA3, Alpha. We also work within ODA, Alpha and R.4, Beta was mostly kind of around security and fault tolerance. So before you couldn't, there were no backups at all. The thing, if it crashed, everything was in memory. What if we got 40 HTLCs? You couldn't resume any kind of multi-step contract handling or anything else like that. But all of that's been taken care of and it's now released. And the first release that we can kind of feel comfortable with people working on maintenance, right? Before, otherwise, I would be nervous, oh, you don't open a channel, but now it's kind of okay. We have a pretty good degree of confidence that people, if things go down, LD will actually correct itself. People that things go down, LD will actually correct itself, and also if it ever crashes, then it's able to resume where it was and actually continue to that state, which is like a pretty big milestone in order to make things happen. All right, so LDs. Within LD, we use Go primarily, it's kind of like the first class, it's effective pure Go, right? And Go has several advantages, meaning that I think it's a pretty good kind of choice for creating current software in general. And usually the question becomes like, you know, why are you using Go? Why not C or C or R Us or whatever else? And I think these are some of the reasons why we use it. And so far, I think we've had. I think these are some of the reasons why we use it. And so far I think we've had pretty good developer uptake. People typically find that the code base is pretty easy to jump into because the language is kind of familiar. If you know C, you know Python, you kind of know Go, it looks the same. Maybe there's some weird keywords. But for the most part, it's pretty straightforward, right? So one good thing about Go, it has very, very good concurrency support. So LND itself is very, very concurrent, parallel, by nature with the architecture itself. And the main things that we use within Go to use this are called channels and go routine, right? So GoRoutine is basically kind of like a lightweight thread. It's like a green thread if you're familiar with that from Python green. kind of like a lightweight thread. It's like a green thread if you load that from like Python to Greenlets. And then it has these things called channels, right? So you have these, you know, you have these threads and they can communicate with each other using channels. So I can like send messages back and forth pretty easily. And if you can think about this, this makes it pretty easy to kind of like create these kind of like concurrent architectures, right? So maybe if you have like just a producer saying something to an intermediate producer and a consumer, and then you have a pipeline going down and you can kind of do subscription pretty easily. It's used very heavily throughout the entire code-based stuff. Another thing is Go has very, very excellent tooling, right? Like it's like, I think the tooling is maybe what makes it. I think the tooling is maybe what makes it in terms of assistance programming language, right? So for Go, it's very, very easy to just do like CPU and heat profiling. So I basically on the weekends I profile, right? I profile, you know, doing memory, you know, I do some kind of like CPU profiling. It makes it very easy with something called PPROP, and you can also actually remotely hit the server to get a Go routine done and things like that. So it makes it easy to kind of do things like that. It actually has a race addition detector, right? So if you've ever done concurrent programming, you know, race conditions suck. They're super hard to find. And they're kind of like the fandom bug, which you can't always replicate. But Go has a thing called racing detector, which slows down programming. Replicate, right? But Go has a thing called race condition text, which slows down a program maybe like 150x, unless you kind of catch all these issues, right? So we always run our tests with this, and you can even run it and develop it locally. So if it catches one, it'll basically just stop everything and then dump the stack because like, oh, concurrent write-on map, or oh, like, you know, read after write dependency type of thing. And the other cool thing, it basically has something called GoFMT. And the thing about GoFMT is that everyone's code looks the same, right? So this matters a little bit more in kind of like larger projects. We don't have to worry about like, oh, do we do a semicolon then a brace or like a brace with a space? Do we do like a new line? Basically, you write your code. Like with the space, do we do like with a new line? Basically, you write your code, you do go from T, and then boom, everything looks the same. This is good because you can kind of get through with everyone's code. Everyone's code looks the same, so there's no kind of arguing with the code review about, like, oh, what's the proper code style, whatever else it all goes in automatically. The other thing is, the standard library is like super, super expensive. The standard library has every crypto thing you need, has networking, and it has its own SSH information in Pure Go, it has a TLS, has basically everything you ever need in order to do anything, you know, Bitcoin later, across this programming. Another cool thing, it basically produces these technically like. Programming. Another cool thing, it basically produces these technically linked binaries by default. And this is nice because I can just have the binary and take it anywhere. The other cool thing about this is I can cross-compile super, super easy for any platform at all. And usually, you never even need to modify your code, basically. If you write it and compile, it's going to run on that other system. At times, you have things around maybe like 32 for 64-bit, but those really aren't that exponential. And so, like, I have a release group that compiles for MIPS, like PowerPC, like BSD, just everything pretty easily, which is nice. My probably think most favorite thing about it. Which is nice. My probably most favorite thing about Go is that it's a very, very simple language, right? So it's like language itself is very, very simple parsing-wise. We don't even need a table to actually parse it. And as a result, you can kind of focus on the problem at hand rather than, oh, am I using the proper sealed class trait like Monad? No, no, no, there's none of that, right? Like, you just basically write your code, it's very, very simple, and you focus on the problem at hand. And the final thing that we really like about Go, this is a really good set of libraries, called VTC Suite. So this is written in Go as well, and they're basically anything you need to do with Bitcoin, this library has. Basically, anything you need to do in Bitcoin, this library has. So, things like assigning transactions, parsing things, addresses, peer-to-peer network. So, LND is mostly composed of libraries that interact with BTC Suite. Anytime we need to actually interact with a chain, we're calling that to BTC Suite itself. So, I feel like this set of kind of capabilities made Go and also Go in the content of Bitcoin, not like a very good choice for implementing today with LND. All right, so now the architecture of LND. So, LND kind of has a pretty particular architecture. We try to maintain this whenever we're doing things like Kirby. Architecture, and we try to maintain this whenever we're doing anything like curvy view or any new subsystems, right? So, for the most part, LNE is composed of a set of kind of like independent subsystems, right? And these subsystems run concurrently. Like we talked about before, they use Go routines within the code base itself to run in isolation, they run in parallel, but then they use channel to mutate within each other, right? So, this is pretty good because when you're reading any one of these subsystems, you know that only it can mutate its own state, right? There's no other thing where you have recognition, so when you grab mutex and all of a sudden, like, you know, the state's inconsistent. Instead, in order for me to modify my own state, I need to get a message from my yellow system. I get a message, I parse a message, I pipe to my state. Get a message response, yes. I get a message, I parse a message, I apply it to my state, and then maybe I send a reply back as well. So, this is really cool because now you basically have like a distributed system within the actual process itself, right? So, we, you know, times we have to do things kind of like, you need to be able to ensure that you can handle kind of like duplicate message delivery as well, because if something like, you know, restructuring comes back up, you need to be able to handle that message, you know, being received in a duplicate manner. It's kind of similar to why people implement message queues and regular distributors and things like that, right? And the main thing is kind of like this main tenet, like a problem with Go. Basically, don't communicate by sharing memory, et cetera. Problem with Go, basically don't communicate by sharing memory, instead share memory by communicating, right? So this means you don't have like a single shared map that everyone has a log to and everyone communicating of, right? Instead, maybe you have that map inside this Google and Go routine. People send it, you know, serialized message to actually modify the state of it, and then you can actually maybe send the message to actually read the current state of it itself. And that by itself makes concurrent programming very, very easy to reason about, right? Because otherwise, you're kind of like, okay, did I have the main lock and the other lock on the entry? Did I have re-entry locks? You don't have to worry about that. You basically just send a message and you come back and you can get the message. You don't have to worry about that. Basically, you just send a message and you come back, and you can get the message and go on and then do things on its own. The other cool thing about this is that, like, we can think about crash recovery specifically between each subsystem by itself in isolation, right? So, each each subsystem basically knows, like, maybe has a particular log, or has kind of like, you know, some last message we're going to send or things that we're going to get back. So, on restart, we can, as a result, we can basically test fault tolerance, very, very specifically in a particular subsystem and use a unit test, right? Rather than just kind of like hoping, you know, hey, hopefully, it all works, right? But no, instead, we can kind of really focus. Hey, hopefully, it all works, right? But no, instead, we can kind of like really focus and know what messages we need to be sending inside and out and out of the subsystem to ensure it's actually properly false-arrowned. But the cool thing is, like, each subsystem actually has its own logger, right? And this is really, really good for debugging because you're like, oh, like, what happened to my channel? Oh, let me look at the subsystem that just does channel updates, right? Or what messages came in? Okay, now I can crank up just the peer-to-peer messaging log and things like that. So, this is really cool because me myself, I have a very particular logging setup. I have things that I know are spammy, I turn those off. I have other things that may be a little more important, like things. I turn those off. I have other things that may be a little more important, like things around handling states or messages that are coming in from the other peers, things like that. Another important part of LND is that anything that's actually chain-specific is all abstracted away, right? And this is good because this is how we're able to support things like, so right now LND supports VTCB, Bitcoin D, and Neutrino as a back end, right? But you can also write your own. If you're a company and you basically have your own API, you can basically plug it to that backend if you want to, right? And this is what they go, yeah. But yeah, it's pretty easy because these are kind of like the main interface. But it's pretty easy because these are kind of like the main interface that we have. First one is chain notifier. Chain notifier basically does things that we do all the time, like you know, let me know when something's been confirmed, let me know when an output's been spent, and then also let me know when a new block comes in, right? And if you know things about like how channels work, you basically use those three things all the time, right? Next thing is the signer. The signer basically handles signing, so it needs to know how to do things like the SegWit by SIG hashing, it needs to know how to do properly witness and things like that. And that's cool because it's abstracted away, right now it's all done in the process, but later on, maybe in an own dedicated. Right now, I did in the cost of it. Later on, it may be in an own dedicated hardware device, right? Or it could even be in a remote server which has other kind of access control policies to prevent people just doing kind of like resigning stuff. Rather than call the keychain, secret keychain, those themselves handle basically driving the keys in a particular manner. So basically, we can even have this be even more segregated. Something that can just give us addresses and keys and public keys for using with the contract, another thing for actually signing them. And then finally, we have blockchain IO, which basically, you know, you can read the blockchain, what's the block, you know, give me the transaction, is the thing unspent, and things like that, right? Cool part about this, you can basically swap them out. Expensive and things like that, right? Cool part about this, you can basically swap them out very, very easily. Because of the abstraction that we have, our unit test set up and the integration test, we just know that we can kind of insert the equivalent of all these different interfaces together and ensure that Bitcoin D works as well as PTCD and other things. But there's always, you know, there's maybe edge cases and things like that in between them. Alright, did this turn out well? That's kind of a common sense. I have the RSH diagram from maybe like two years ago, and it wasn't like even, it didn't really look close to this, right? Maybe I could have blown it up a little bit, but hope you guys can like split. Maybe I could have blown it up a little bit, but I hope you guys can like squint and maybe look at it later on. So, the way it is, anytime there's an arrow, that either means there's kind of like a direct dependency or just like passing a message to another subsystem itself, right? So, very bottom, we have Lightning PDP network, right? And then above that, we have this thing called Brontide. And Brontide is basically this crypto messaging layer that we have on top of us within Lightning. There's basically something called Noise made by Trevor Kerran, who works on Signal and WhatsApp and things like that. Very, very good kind of modern messaging protocol with very modern crypto, has some cool things around. Protocol with you know very modern crypto has some cool things around like handshakes to ensure that like you know we have certain properties like identity hiding and like non-replay and like zero entrance times things like that. Then you're right above that we have LMIR which does all the framing, so like encoding and decoding any messages. The cool part about the way this is set up is that if you want to just like take our code base because everything here is like its own individual package and just connect to the network and listen to you know what's going on, you can do that, right? Because everything is very very you know modular and abstracted away and actually has some nice sort of unit tests as well. Right above the LMIR we have the peers that this is basically you know reading Right above the LMI, we have the peers. This is basically reading write messages from different peers. Then we have the server, which itself is kind of like a hook handling the state of all the peers itself. And above that, we have the RPC server. So RPC server is pretty central part of LMD itself. And that's where any time you're interacting with any application, they're going to the RPC server, right? The RPC server uses something called Mac Roots while in case that we'll get to a little bit later. So anytime, so MacRoots are basically this kind of like barrier credential, right? So typically you have maybe have a username and a password. You basically have the username, and you look up kind of like, you know, what can they perform on this like massive list? You look up kind of like, you know, what can they perform on this massive list, come access to the control list. Instead, we do something called, we actually have credential systems, right? So I can give you a credential essays, you can only make new addresses, right? And I give this to the RPC server and say, oh, you tried to make a channel, no, that's disallowed, right? But then I can make this address, and I can take this new address back room and say, you can only make new pure, you know, segwit addresses, right? And then you can't make NetSB2SH or anything else like that. So those capabilities that you can basically delegate back room, but then also kind of attenuate it down. And this is nice because if you're ever sending the application on top of L and D, It down. And this is nice because if you're ever sending an application on top of LD, you can kind of like partition all your boxes and give them only the responsibility that they need, right? So you can make channels, you can list for invoices, and you can send payments. And that's pretty good because, from a perspective of kind of like, you know, compartmentalization and having a separate responsibility amongst all the different sources itself. Next, on the back rooms, we have gRPC, which we use for our key server itself, and we have a REST proxy, which is basically what most people communicate with on the daily itself. And then if we move over to the right a little bit, we have the gossip. And then, if we move over to the right a little bit, we have the gossiper right over to the right of the server. So, this is basically dealing with kind of like exchanging information for the different channel peers, kind of like seeing what's going on as far as new channel, things like that. But then you have the router. So, the router hooks directly into the gossiper, because maybe it's getting a new channel out and it's committed to the state, right into channel DD. And we have the HTLC switch, which is kind of like the fabric of LND, right? So, this is like the whole payment as package thing, where it basically has a server channels, which are all links, and it's basically handling the capabilities forwarding them in and out. And then moving up, you know, right from up a little bit too, we have the signer, the key right above that, who hooked into the funding manager, right? So the funding manager handles basically how do we make new channels, right? So it basically walks the state machine of like, okay, I signed, you know, the funding transaction, the human transaction, okay, some broadcasts, what goes along with that, and that hooks into above the main three interfaces, wall controller, channel, refly, and blood, and blockchain.io. And as long as that we have UTX in their screen. So this comes into place whenever you have kind of like a time-locked option. So, this comes into place whenever you have kind of like a time-locked output, right? So, what it does is basically babysits these outputs until maturity. Once they're mature, you know, CSV or CLTB, relative absolute time lock, and it can then actually sweep those back into the wallet, right? So, as a result, because we have that kind of like usable computability we can use for any contract in the future, then we have the contract court. Basically, contract court is where disputes happen, right? So, if there'd be any case where for some reason someone broadcasts a prior state, or we need to go to change time-lock something else, contract court handles that, then basically communicate to the nursery. You know, so it may be the case that, oh, I have an HCLC, it timed out, so now I need to broadcast. I have an HTLC, it timed out, so now I need to broadcast. And I give that to UTXO nursery. The nursery walks over it until maturity. Maybe it's going to be 100 blocks, then pads the back over back to the wallet. And then the final thing is the breach arbiter. The breech arbiter is kind of its own section. This is basically where justice gets dispatched at times. And by justice, I mean, because the contract basically has kind of like these stipulations where if you ever do this, I get all the money. If that happens, the breach arbiter gets notified by a chain notifier, broadcasts the transaction, writes to disk, and then it gives it to the nursery, maybe to A broadcast transaction writes a disk and then gives it to the nursery, maybe to put some time logs. But that's kind of like general architecture of the way things are right now. And this was a lot simpler in the past. Maybe a few of these subsystems came up in the past year or so once we kind of retracted a little bit and realized that we wanted a little more flexibility in certain areas. I think they're pretty good as of now. And they're isolated, have their own set of tests, which makes things easier to reason about. Alright, so let's talk about Lightning is kind of like navigation platform, which is have some kind of motivate recently. So, cool thing about Lightning is basically that's kind of like this new development platform, Bitcoin, right? Before, maybe as a developer, you need to know. New development platform on Bitcoin, right? Before, maybe as a developer, you need to know about how do I sign up to UTXO? What's the sequence value? How do I actually even do signatures? What's the sick hash? Things like that. And that can really kind of get intense. And that kind of, I think, often people actually developing on Bitcoin itself because maybe things weren't that well documented, but also it just kind of seemed like kind of overwhelming itself. And the other thing is that with Lightning itself, now because we have this much more streamlined API, it's actually like another layer. And because we have that other layer, we kind of abstract away from all the other. Because we have that other way, we kind of distract away from all the lower level details, right? When I open a channel, you only want to know all the stuff below that, right? There's like a bunch of things going on, like keys, signing, HTLCs, and everything else. But if you, you open a channel, it's much much simpler in your PI. So this lists are things kind of like doing things like metering for services. Maybe I'm paying, I'm walking somewhere outside and hooking someone's router and I can pay them via VPN servers to actually connect to the router. It may be via Wi-Fi and finding my cab or something like that. I can do things like faster costing transactions. I can do things like faster cross-chain transactions. So we did a demo, a kind of a demo of this maybe last October where we showed the ability to swap a TB on my partner instantly. It's a pretty cool use case, and otherwise I trust an exchange, maybe I do it on chain, that takes 30 minutes, 20 minutes. And instead, that can be instant itself. And so we've been calling these laps. It's kind of like a play on dApps. It's kind of like a keck thing, like a play-on words. But it seems to be catching on now, so maybe people will get into that themselves. So LD itself, if you go back to that in the diagram, we can architect this to make development a little bit easier, right? We kind of architect this to make development a little bit easier, right? So, we wanted it to be kind of like a platform people make applications on, where people integrate exchanges and merges and other things. So, we kind of like, that's one of the first things we kind of sat down and thought about in terms of design of it itself. So, one of the main things that we use is gRPC. So, gRPC, if you guys don't know, is basically something actually developed internally within Google called Steppe, but then they open source things called geRPC instead. So, anything within Google basically uses something that's very similar to geography. It uses protobufs, if you all don't know. Basically, it's like binary solution formats. Know, don't know. Basically, it's like binary simulation format. So, you can basically have a definition of a message, and that can pause onto any language, right? And once any of those languages, you can use that same struct or dictionary or whatever else in that language. And the cool thing about this over uses HTTP2. So, this lets us basically have one single connection that multiplexes a bunch of other different messages between themselves. And this is cool because now, like, you know, you don't have to basically use a special SDK or hit like a REST thing. You can basically code in your own language, right? So, you're in Python, like, you know, you basically have a generator, you're iterating all this. So, you're in Python, you basically have a generator, you're integrating all this stuff, you're sending messages, or maybe in Swift, you're doing this in iOS. And because of that, you can kind of focus and integrate this very deeply into your business logic, rather than having, okay, I'm talking to LD and I'm doing my record program, they kind of look more integrated together. And the other cool thing about it basically has streaming RPCs. So I can basically have one single connection to get notifications, like, okay, let me know every single time a payment is settled. And I can have a callback that maybe hits off some WebSockets on JSON, kind of like send things together. Or I can do things like notifying one. Together, where I can do things like notifying when the channels are being open or closed. But generally, we've seen a lot of people building many applications on this. People build Explorers, different we've got YALS, actually, like one of the most popular apps earlier in the day, htlc.me. So we've seen like a very, very big community around one of the applications we're using to work. Yeah, the CEO of YALS in here, actually. So Alex, yeah. And the other thing that we have, we have a REST proxy. So maybe you don't want to use your RPC, maybe it's more supportive. Maybe you just kind of like, you know, you like typing. Maybe you like, maybe you don't want to support it, or maybe you just kind of like, you know, you like typing like raw UCP query in the command line using Telnet, you can use this instead, right? So basically, all this does proxies over to the gRPC server, and it's done using JSON. And it's pretty easy. Like, here's a good example of me, you know, querying for the balance of my channels, right? So using either of these modes, depending on the application, you know, kind of like way before the application-wise, you can use either one of these. Once again, we have Mac Runes. I guess I talked about this a little bit before, but because you have these barrow credentials. So right now, we basically have one ad in Mac Rune that basically does read, right? We have a read-only Mac Rune. Basically, it does read and write. We have a read-only Mac Room, so we can give this out to someone they can pull up the channels, and then we have an invoice back room. So, the invoice MacRune is cool because now we can have a server that accepts payments on Lightning and can't do anything else, right? So, even if that server is compromised, all they can do is make invoices and make addresses, right? And cause invoice inflation or something, which doesn't really affect you that much, right? But so, we have some other cool features in Macaroon that we have yet to implement, right? So, we have what we call a bakery planned. So, what the bank read will allow us to do, I can say, here's Mac Room, it can only make channels below 2 BTC on. It can only make channels below 2 BTC on Wednesday, right? And this only take that. You don't make a channel below 2 BTC on Wednesday, then fight it. You can basically take that down and have very, very fine-grade authentication for your applications, and that's going to be coming soon in the future. Alright, so we also have this pretty cool developer site made by Max. Intern was last summer. Maybe he's not here. Maybe he's here, but I guess so. Oh, there he is. Cool. So, this is a pretty cool site. You can actually see every single RPC that LMD has. And if you look on the top right, we show example code on the command line. Example code on, like, you know, on the command line, on Python, and also on JS. The cool thing about this is automatically generated, so anytime we update the protos, this will also get updated as well. And that's api.lightning.community. I did the wrong link. I'll fix it afterwards. But yeah. And then we also have this developer set for LND, which is pretty cool itself. Again, made with Max last summer. And this is kind of a targeted dev that wanted to build on LND. So we have a pretty good overview section that kind of gets you in the proper mindset application-wise. How do I think about Lightning applications? How do I think about Lightning? How do I think about what's going on? think about lightning applications, how do I think about lightning, how do I think about what's going on under the hood, kind of like walks you through making the topology, making channel updates, things like that. And also has a directory of all the cool labs we've been working on. We even have a tutorial called Lightning Coin Desk, which basically takes you to how do you make CoinDesk but how payable to it. So we have very hands-on tutorials for developers actually into the development and things like that. And the site itself is open source. Anything's out of date, you can basically make a pull request and then update the site. Or if you want to add new examples or maybe new tutorials in different languages itself, you can do that as well. Examples or maybe use total different languages itself. You can do that as well. Alright, now we're getting into more a little bit specific stuff like what we're working on as far as safety with the LND itself. So we have this thing called Cypher-C. First version is called AEZ. So one thing with Lightning is that, unlike a regular wallet, we basically have many, many secret keys we need to drive. For a wallet, maybe you just have like, you know, I'm using BIP44, so I have my accounts and I can partition them, and that's it. But for all for Lightning, for every single contract that we have, we maybe need five or six different keys. Right now, we get more complicated. Like five or six different keys, like right now, it would get more complicated in the future, right? So, as a result, we're kind of thinking like how do we make sure this actually works, how can we make it deterministic, how can we connect it back ends up properly. Then we look at these existency formats, right? So, everyone knows you know, BIP39, and that's like was been out there for a while. But the other thing is that, like, you know, there's 39, there's other BIPs like you know, 43 and like 49 and 44 that talk about kind of like key derivation, but those are like very, very simple and not lightning specific. So, we're like, well, I guess we've got to make our own. So, we kind of did that. But we can go through kind of like justification of why we did so, because it's kind of. Go through kind of like justification of why we did so. Because you can say it's like pretty big departure away from the regular industry, PFING Blue39. Why would we make our own teeth on that? So, one of the colonial Blue39 is that it had no birthday. So, this maybe works through an API, maybe inside or whatever else to actually do a key rescan. But if I'm on my phone, I don't want to start scanning from Genesis, right? I could be the first Bitcoin adopter, right? I could be like HalPeny's future stuff like that, right? So, we want to make sure that you don't have to go all the way back into the chain. The other thing it has no version. All the way back into the chain. The other thing it has no version, right? Which means that when I have the seed, how do I know exactly how do I drive my new keys? I could have Electro 2.0, and 10 years ago, I'm using Python 3.7, and it's like, well, that doesn't work with this prior version. The other thing is that the way they do the password, it can actually lead to a loss of funds, right? Because you don't actually know if this is the correct password. They have a feature there, they try to basically let you have hidden wallets, which I think maybe depending on the use case may not be actually be that useful. But as a result, if I have my C, use case might not be actually be that useful, but as a result, if I have my seat and it was power protected and I put in an invalid password, it doesn't tell me no that's wrong, it just says, okay, here's your wallet. It could be $5, it could be $20, but now I don't really know what my password was. And everything has a pretty weak KDF, right? Like it basically doesn't really do anything in the parameters and maybe didn't really do much in the past. And it doesn't really tell you how to derive the keys, right? So now I need to maybe have my wallet and the backup together and then hope in the future that I can still use Python 2.4, 1.1 Python 5 or something like that, right? Python 3.4 when we're on like Python 5 or something like that, right? So instead, we created something called AEZ. AEZ is kind of like the first instantiation, something we call like this cyberseed family itself, right? So if you look on the right, on the top right, you can basically kind of like see what the format of it looks like. So first thing is we have an external version, right? And then we have a cybertext and it checks them over that cybertext. The external version basically tells you kind of like how to parse everything else. So I didn't write that, it's basically like one byte right now, and we can basically bump that up in the future. It's let's do cool things. Let's say we change Bump that up in the future. Let's let you do cool things like let's say we change the KDF in the future or we change other different parameters. We can take an existing seed in an offline program and then basically reconvert it back to a new format. Then we have the cybertext, right? So the cybertext is actually an encryption of this payload above, which is the intranet version, which basically tells your wallet how to drive the key. So this can be like 25 seconds or whatever else. It knows basically, okay, I was using SegWit 2018 and I use Windows key hash and I use networks based and that's it. We have the birthday. And the birthday is used some And that's it. We have the birthday, and the birthday is used something called what we call Bitcoin Day's Genesis, right? So we realized that we didn't really need a full, you know, 8-byte or 4-byte times happen. We can say, okay, you know, Bitcoin was created in 2009, nothing else matters before then. Let's just count every single day beyond that. The cool thing about this, we can use two bytes and we can go up to like 2088 or whatever. So in the future, maybe if we have other use cases and we have a new C format, then that can work out itself. 2020. Huh? 21. We have like, yeah, maybe a few hundred years, which is which is our name. There's also the salt. There's also the salt that's missing. Oh, I forgot about the salt. Yeah, yeah. So, yeah, so there's also the password resident that happened there when we finally have a check stuff. So, again, some interesting traits about this is that, like, from the get-go, if you have a password, the seed itself is actually encrypted, right? So, I can maybe protect leave this out in plain text, and no one can even do anything from that because it's actually encrypted. They need the passphrase. We're going to talk about it. So, we have a passphrase, we've run that through a KDF. The passphrase. What I didn't talk about is we have a passphrase, we've run that through a KDF, and then we also apply assault, which is encoded with everything else. And then finally, we have the checks on the outside. So when you're decrypting, you can first, you know, verify that this is the correct set of words. But then, even beyond that, because the cybertext uses this AEAD, within it, we use something called AEZ, which is arbitrary equal size block cyber, which means that we can encrypt a very small amount of data and control exactly without the Mac, which it has, which is kind of like another thing, we can have 20 bytes turn to 20 bytes, right? Because it can actually fix, we can adjust the Because it can actually fix, it can adjust the internal mechanism to basically decide the input itself. And then finally, because it's actually more or less like an AAD, it has a tag inside of it, right? So there's something called a subject expansion factor where we can control how many additional bytes to add onto it, which controls basically the strength of the tag itself. And this is cool because now once I actually know the correct words, if I put it in a password, I know it's wrong. I don't have to, so now at that point, I don't have to be worried about maybe finding out that I thought it was the right password, I erased my memory, and then I'm like, I wasn't really there. Was the right password I erase my memory and then my eyeballs weren't there. But now this is basically the C format that we're using within LD. And it's been working pretty well so far for most people. It's a little bit different than what people were used to because it's longer. Maybe we used to use 12 words. We also do recommend that you add the password phrase as well. But more or less, the HD implements are all the things that we needed. It has a version, so we basically know how can we parse the external part of it. In the future, if we decide we want a bigger checksum and want to use the encrypted system, we can basically have a tool to upgrade the version and link it to someone else. It has internal version to tell you how to. To give it someone else, has internet room to tell you how you want how you actually derive the keys, and finally has a birthday so I can know with the life client how far back to start looking. And then let's talk about backups time. So we talked about, you know, so the C format is about how do we rederive all the keys that we have in the past. Now, this is basically: once I have all the keys, what can I do with them, right? Or even now, once I'm actually live and actually updating my channel, how can I do back what happened in the case of data recovery, right? So, you know, one thing to know is that Lightning nodes are inherently more stable, right? We don't just have the keys, we also have the current channel state, right? For, right? We don't just have the keys, we also have the current channel state, right? That basically tells you what state number are we on, you know, what parameters are we using, what's my balance, what's your balance, you know, where HTMLCs are active itself. And as a result, you basically need the current state and your set of keys, otherwise, you can't update, right? If you have your keys, you don't know the state, you can't really do anything. If you have the state and you don't know the keys, then you can't do any updates at all. So instead, we basically have a two-phase approach. The first stage is basically a dynamic channel backup. And you guys maybe heard a little bit about this, but we have these things called like watchtowers, right? So the idea is that anytime you make a state update, or maybe you can even back a little bit more. So, the idea is that anytime you make a state upgrade, or maybe you can even bash several of them together, you can export this state. It's actually, you know, there's different versions, but more or less, like, the important part is the watchtower doesn't know which channels is being watched, right? You can even now encrypt that data so it doesn't know exactly what my balances were, or the mistake of it, things like that. I send that state to the private outsourcer, right? And as a result, now I can just know that, like, okay, if I have 10 of these in the world, maybe 100 in the world, only one of them needs to actually act properly, right? And when we're actually going to integrate it into LND, we're going to actually add them into the routing node itself. So, basically, if you're running, you know, them into the routing node itself. So basically, if you're running a routing node, you can also run it into watchtower. It's actually a little easier in terms of discovery to have one of these participants. But also now you can say, okay, if I'm a watchtower, then I can also have a routing node integrate them pretty closely together. The other thing we'll add is that you may be able to point them at your own instance, right? So let's say I have my node and I have a computer at home. I'll be able to back up those states to my computer as well in kind of like a redundancy thing. Or even have it do something like core on a 3D machine, like the Toronto or the ability zones or something like that. The other thing is that we may. Zones or something like that. The other thing is that we may add something to actually batch the upface together. Because otherwise, if you can imagine a prompt standard where it's like one mushroom globally, and it gets everyone's updated, it now basically has a massive time to side channel attack on the entire network, right? It doesn't send anything at any given time. So, we're going to add this kind of batch timering process within it to basically ensure that we're giving away minor timing of it. So, this will probably be integrated into measurable reasons in LND. We're going to kind of roll it out there first and then create the standardized bolt for it in the end. Because it'd be nice if we basically all the nodes. Volt for it in the end. Because it'd be nice if we basically all the nodes, you know, they're all writing this watchtower software. They have the exact same messaging, kind of like framing structure, and then now it's very easy for a node just like, you know, you know, spin up, connect some channels, and also ensure that your stage actually dispatched properly. Yeah. All right, did anyone see this? Happened like a day or so ago. But so what happened is that like someone, like, you know, they have their node, they're like, you know, turn log, they saw this log message on the left, right? They say it's like, channel state number 20 was broadcast. On the left, right, that says channel state number 20 was broadcast, you know, revoke state 20 was broadcast, remote security, the remote appears doing something sketchy, exclamation marks, and then basically once that's done, it says, you know, waiting for a confirmation, then justice will be served, right? And on the right, you can see the other node that says, you know, justice served, we get our menu back. The question is, like, you know, exactly what happened in this case. So, what happened was that a user had a node information, right? And for some reason, they ran into some issue or they didn't know how to get past it. So they shut things down and they restore. They didn't know how to get past it. So they shut things down and they restored with a prior backup, right? They basically did like a CP, they copied their channel state. And as a result, when they came back up, they were on state 25, but really they only had to do state 20. So as a result, if they actually broadcast their transaction, they violated the contract, right? Which means the breach arbiter, which was on the other screen, basically dispatched justice onto the individual, right? So the question is: how do you avoid this from happening? So if you have your dynamic states, how do you ensure that if I ever have a backup of this kind of States, how do you ensure that if I have a backup of the static state, that actually works properly, right? So I'm going to explain that, and we have something called static channel backups, right? So, what we can do is we can actually overload the watchtower with some static channel information, right? And this information can be basically per state. And I say it's static because you only need this for when a channel is created, you create one of these, and when it's closed, you can delete it at the end, right? And because it's static, combined with the seed format, basically the backup tells you what keys were used, not the exact keys, but like a key path using the key information in that protocol using the seed format itself. path using the key derivation protocol using the C format itself and also tells you information around you know kind of like the node and things like that. So basically given your static backup and your C, you can redrive all the keys you actually need and then in the case of basically a part of data loss you can follow this protocol here, right? So first you fetch the backup and you basically make sure it's you know maybe I have a Mac, make sure it's actually correct and things like that. I use my C in the backup to redrive all the keys that I actually had. I can connect back to the nodes that I had channels with and when they see me in this particular state they'll activate something we call like data loss protection in the protocol itself, right? They do something we call like data lock protection in the protocol itself, right? So they'll give me some information needed to actually sweep my funds. They close out the channel on-chain, and then I can sweep mine directly without any delay. So this is kind of like an emergency scenario thing, where if this had been fully implemented, the prior state would have been prevented because both time we realized, okay, hey, I'm going to close the channel lock because they actually lost some data. And we have this plan for NetMajor release of LND as well, because now it's a state where if I have my C and I have one of these backups anywhere, I can get that money back off-chain, right? So I can use my C for all my on-chain transactions. Off-chain, right? So I can use my C for all my on-chain transactions and then for my off-chain, depending on if I have the static backup or dynamic backup, I can also open to this as well. And we plan to basically have some degrees like streaming RPCs. So every single time you open a channel, you basically grab one of these and sing it off your Dropbox or whatever else to ensure that you have all your state properly set up. So this is basically the safe way to backup on Lightning. If you ever do anything naive, it's kind of just like copying it and hoping that I have the correct version. Unless you're doing some very complicated versioning system yourself, this is what you should be doing. And we'll be influencing. Yourself, this is what you should be doing. And we'll be implementing this very soon here within LD. And hopefully, again, we'll make this double volt standard because it'd be cool where any node, if they're using the same seed and they have the same backup format, they can connect to any other node and actually rederive their keys and get all their money back, which is what we want. We want to collect all the satoshis because in the future, well, satoshis may be important for us. The final thing we have within LND is we have now automatic peerbroot shapping. So, before with the private version of LND, you had to connect manually to other peers. So before with the private version of LNU, you have to connect manually to other peers. And this kind of like, you know, was a big pin on other individuals, because if I don't, I'm on IRC, I don't know anyone, I wouldn't be able to actually connect down to anyone else. So instead, we basically added this thing called I'm at peer bootstrapper, right? So one important component of this, like within the code itself, you can't see these interfaces on the right, we have something we call it repeater bootstrapper, right? And it's basically pretty generic, basically just takes number of addresses, knows I shouldn't connect to, and then returns addresses that I should connect to itself. And then using this, we've composed this for the multi-source bootstrap to basically ensure that we get Multi-source bootstrap to basically ensure that we get a number of new peers from a set of distinct bootstrappers, right? We want to see bootstrappers because it could be the case that someone's DNS server goes down, all of a sudden the entire network can actually connect to each other, right? So the current two bootstrap connections that we have for the codebase, first we have DNS and Volt 10. It's very similar to the way DNS Bitcoin works. It was made by Christian Decker, who also runs a bunch of Bitcoin DNS stuff. And we have one of those for testnet, Bitcoin mainnet. Now I was getting one for Litecoin, but I have an idea. Maybe I'll do it after this talk at some point. For Litecoin, but I haven't done it yet, but maybe I'll do it after this talk at some point. And then also, we have channel peer restartment, right? So you only need DNS when you actually connect to the peer for the first time. After you connect, you basically have the set of kind of authenticated sign announcements. The other thing is, you only accept an announcement from individuals that can prove they have a channel open, right? This avoids kind of a civil war scenario where they just like you know flood you and maybe do like an eclipse attack. Basically, you force them to have some skin in the game. You need to have open UTX symbols and channels, and otherwise I won't accept your announcement. So, as well, when you come up, you can connect to the DNS resolver, you can get your You can connect to the DNS resolver. You can get your initial set up peers. But after that, because you have this data, you're fully independent on yourself. You don't require any other server in the future. But one thing we're probably going to do in the future as well is add additional versions of Google trapping, because otherwise we basically want as many redundant sources as possible. And for some reason, the DNS server is down, you may have issues actually connecting. One thing we've seen in the wild is that certain DNS resolvers fills around kind of like our larger SRB records, which because maybe they don't support Wumbo UDP or they don't support UTC. Maybe they don't support one bone UDP or they don't support TCP resolution, they're not able to actually connect. Maybe we'll investigate some other redundant sources of how we can do bootstrapping in a decentralized manner. Final thing that we have here, which is pretty cool, which I could have an entire other talk on. But in the past, you basically didn't really know what your unit was doing. You can look at the logs, but you wouldn't know if you actually forwarded transactions or things like that. So, in this one, we basically have a time series database called completed payment circuits, right? The complete payment circuit is basically when I get an ad, and I forward out in the ad HFC, and I get back the same. I get an ad, and I forward down the ad HFC, and I get back to settle, and then I get some fee itself now. So, we basically store all this persistently on disk. You may want that for several reasons, as far as financial record keeping and also different analysis. The cool thing now is I can look at my node because it's like a time-school database, and query, like, oh, between 2 and 3 p.m., you know, there was like lunch on the west coast, and I have a spike in activities like that. I can look at that and maybe try to analyze and see what's going on. But now, I can actually see if my node is running properly, right? So, if you look on the top right, you see the fee report command. So, what it does, it shows you the fees. Top right, you see the fee report command. So, what it does, it shows you the fees of all the different channels that you have. You can see, like, I had a fee of like one baseline satoshi, and I think it was like 0.01% per satoshi after that. And it basically has also a breakdown of like the day. So, I made seven satoshis that day. This is like on the on the testnet faucet. And then, over the past month, I've had 145 satoshis, which is not that bad. And you know, it's not complex right now because fees are very, very low on testnet in particular. And also, you know, there's not like that much traffic going on right now on testnet because people want to move around for the most part. Going on right now in test time because we have like movie command for the most part. Then we also have the forwarding history command. And what forwarding history does, by default, shows you the last 24 hours of forwarding, right? So you can see, you know, I had two payments in that 24-hour period. One was 2K Satoshis, the other one was 1700 Satoshis, and I had one Satoshi fee in all those, right? This is pretty cool because now what people can do is they can actually do roads and analysis on their channels, right? So we have to go autopilot in the daemon right now, where right now only looks at kind of like static channel information, static graph information, see where they should connect and actually separate channels to. But in the future, we can basically look Connect and actually separate channels to. But in the future, we can basically look at the real-time information of all these channels coming in and decide: do I want to close out channel B because channel A is getting me more revenue, but it's almost depleted. So I can close out money over here and spend some over here. I can do things like ensure that I have a proper rebalancing schedule to ensure that I can accept any available flow any given time. Maybe it's the case that I'm getting a lot of cancers over here, so I want to ramp up my fees to only have things that I like to go in. So we can do a lot of things in the future hooking into this. People can make very cool. Do a lot of things in the future, we'll hook them into this. People can make very cool kind of like QP graphs of every single payment coming in, or we have a streamer RPC and things like that. And now it's kind of our standard. Thank you, Bali. Alright, so we're going to jump into for the last half, we're going to do a couple things mostly related to forwarding HTLCs. Most of the old work that's going Most of the work that is going to be talked about here is regarding persistence and safety stuff. And then at the end, we'll kind of get a little bit more into the actual onion packets and onion routes. So to start here, this is sort of like a very high-level diagram of how the core components of our payment forwarding work. In the middle, you have the HTLC switch, which sits in the middle and sort of manages all of the surrounding links. A link is like a connection between myself and a person who I have a channel with. And the person who I have a channel with. And so when I'm actually forwarding payments and I send the onion packets, they actually go out over these links. So it's the job of the HTLC switch to sort of be sort of like this financial router that is accepting incoming payments and deciding how to forward them out. So for the life cycle, this HTLC will start on your left with the blue line, follow it all the way through. The red lines sort of indicate where a packet can fail internally, and it'll just get sort of sent back and failed to the And it'll just sort of sent back and failed to the person upstream of where it came from. And the green is actually like a successful response or settle. You see a green line over there, that's when we actually receive a payment locally. So as soon as we receive it, we check, like, oh, we have an invoice, cool, settle it back. And over here, we're actually getting a response from a remote peer and then forwarding it back through. The key components here are the circuit map and the forwarding packages. Those are the primary things that were added here. The forwarding packages are mainly reliable with our main. The forwarding packages are mainly reliable, are mainly responsible for ensuring reliable forwarding of all the packets internally within the switch. So if we basically write everything immediately to disk, and then we come back up, we can always know how to sort of resume our state. And it'll sort of aggressively make sure that these are pushed through and get to the outward links. The circuit map's job is a little different. It sort of sits in the middle. And what its job is is to make sure that we never send a packet through the switch more than once within a particular sort of leak cycle. Within a particular boot cycle. And so it has to basically handle this job of managing broadcasting messages between M of M different peers. So this is a huge communication bottleneck problem. And so getting a lot of efficiency there was pretty critical. So we'll start here with the circuit map. So whenever we get a circuit, or sorry, whenever we get HTLC, it's assigned a circuit key, which is a tuple of the channel and the HTLC ID. The HTLC ID is sort of auto-increment for each channel. HTLC ID sort of auto-increment for each channel, so they'll just like ratchet it up and we'll get them in order. And there's sort of like when you're forwarding a payment, there's sort of an incoming key, so the person who forwarded it to me will assign like some HTLC ID tied to this channel, and then I will sort of go through the circuit map and then assign it an outgoing key, and I will assign it sort of an HTLC on my outgoing channel that the remote peer will then handle. So the job of the circuit map primarily is to line up those two incoming and outgoing keys. Line up those two incoming and outgoing keys. The primary reason is that when the payment comes back across some remote peer, then I have to look up by the outgoing key where do I actually send this, or which channel do I actually send this back along? Like who was the one who originally sent it to me? And that whole process needs to be persistent, because if it doesn't, then we might receive a payment and be like, oh, well, I don't know, draw. And that's like the worst case that could possibly happen here because blackholing a payment is basically the worst case. If I send a payment and it just gets lost by the network or your node goes down and reaches. If I send a payment and it just gets lost by the network, or your node goes down and restarts and doesn't know how to handle anything, or doesn't realize that it already has received this payment and just drops it, then that's just gonna sit there and time out until the CL TV expires. So that's not too great. Now, some of the big challenges here is that some of the links may not be online at the time I'm actually trying to make this payment, right? So the semantics of an ad, which is when you're going out, and blowing the diagram this way, are different than when you're going backwards. So when you're going out, it's kind of like a best case. Going backwards. So when you're going out, it's kind of like a best case or best effort forwarding, right? If I'm sending a payment and the rope appears on a line, I'll just be like, oh, they're not online. I'll send the bail back. And so, like, that's a little bit easier. But with the response, right, I need to make sure that always gets back. If I committed to forwarding this HLC and in fact did, then when I get a response, I need to make sure it gets back. Otherwise, that person is going to be sitting there waiting for forever, really. So that's one of the big challenges. It's also in between. That's one of the big challenges. It's also in between this whole process, the links can flap. So they might come online, go down, and repeat this process. And we've seen this online. I don't know if you guys have tested LD, you see this a node will come up, send an error, go down, come up, send an air, go down. I'm sure a lot of you have seen that. So making sure that all of this state stays consistent when that process is happening is a big challenge. And really, the big things here, like I said, are reliable delivery and at most one semantics. And we have to do all this while at the same time avoiding a write. And we have to do all this while at the same time avoiding a write for every single HTLC. Because I can assure you, and it was tested, if you do it once per HTLC, you'll get a throughput of about 10 transactions per second. And that's actually probably the best case. There's about three subsystems that I was working on where they all had to be kind of joined in concert to do this. And just putting one of them with a singular write per, putting a singular write per subsystem or per HTLC dropped the performance dramatically. System or per HTLC dropped its performance dramatically. So doing all this in a batched manner was hugely critical. Like I said, and then forwarding packages, so going back to our diagram, so the forwarding packages are internal to every link. As soon as we receive packets from the outgoing world, we sort of write them to disk. And like I said, that serves the sort of primary state in which we'll read again on startup to make sure that we reforward these internally. Like I said, they're all batched because Like I said, they're all batched because, and they're batched at the level of the channel updates. So when I do like a commit, sig, receive, revoke, and act, all HTLCs that are done or handled in those batches are all sort of processed atomically. And the reason is it kind of like simplifies actually the recovery logic when you think about it. If you actually receive a batch of HTLCs but then actually process them individually, let's say I get a batch of 10 and I start processing the first three and then I go down. When I come back up, First three and then I go down. When I come back up, how do I necessarily know where I stopped? It could have been, like, for example, especially with something that needs to be like a replay protection, right? If I process three and I say, oh, those were good the first time, then I've written that to disk that they were good. Then I come back up and then I check again and it's like I've replayed myself. So actually by doing everything in a batch, making sure that our persistence logic is back to the same level as the actual logical atomicity of the channels is actually a huge safety feature in my opinion. A huge safety feature, in my opinion, and it was also kind of necessary from a performance perspective. So, basically, when these forwarding packages, we only ever write to disk with these forwarding packages in the best case. So, during the normal operation, all we do is write to disk, everything's buffered in memory throughout the entire switch, and then only if we crash, do we actually have to reconstruct our in-memory state? There are kind of a few edge cases here, because like I said, this is just kind of the flow of a payment. Cases here, because, like I said, this is just kind of the flow of a payment in memory. This doesn't include all the actual persistent restarts and stuff, so each sort of like subsystem has a bunch of internal recovery procedures and stuff like that. And so, one of the cool things about this design is that because everything's only rigid to disk when we come up, or in the process of when we are done actually with these 40 packages, we can garbage collect them totally asynchronously. We can garbage collect them totally asynchronously just by reading the disk. We read the disk, be like, oh, hey, this one's done, remove it. And we kind of do that once a minute, and that doesn't interfere at all with the channels. It can be done kind of on a global passive level. So basically, the win here is that we're able to batch things heavily, and that is a big win for performance. So, moving on to multi-chain stuff, in this latest version of 0.4 beta, we restructured the data directory entirely. We now segregate Entirely. We now segregate, oh, segregated directories. I don't know. We now separate graph and chain data. So LD right now can support Bitcoin and Litecoin, is what it's configured for. We just added Litecoin D support, which builds, I mean, it's almost entirely the same components as the Bitcoin D backend. But each chain sort of has chain data that it needs. It might be headers, or if you're using Neutrino, it might be compact filters. And then additionally, each one has a wall. Compact filters, and then additionally, each one has a wallet. So, in the chain data, as you see, we sort of store them by like Bitcoin, like whatever testnet, mainnet, subnet you're on, and then the actual data entirely. And then, the difference between that and graph data is that graph data is sort of shared across all possible chains you'd be listening on. So, like, if, for example, if you guys saw the swaps demo, all the graph data for both Bitcoin and Litecoin was all in the graph directory, whereas like they would. Litecoin was all in the graph directory, whereas they would actually have separate mainnet, or not mainnet, testnet, VDC and LTC directories and wallets. So this is a nice separation of just directories and concerns. And hopefully that gives you sort of an interview of where, or introductory look ahead as to where this will go when we actually incorporate the full multi-chain daemon support. So another cool thing is in getting prep in preparing for that, we implemented determination. In getting preparing for that, we implemented deterministic per-chain key derivation. So, if you guys are familiar with the BIP32 key derivation path, it goes hard to purpose, hard in coin type, hard in account, change, like external intel or child index. What's up? That's 44. Oh, my bad, yeah, yeah. And then, yeah, so we, so a couple changes that we made to support like the configuration schemes that Lolly was talking about. like the configuration schemes that Lollu was talking about. We deployed purpose 1017. Shout out to Brick Squad, Waka Waka. We use slip 44 for key derivation, so you know Bitcoin 0, testnet 1, Litecoin test 2. And then for all the key derivation, because we have different types of accounts, like you know, the standard wallet might just use like a default account of zero, but like Lalu added all this stuff to do like you know the multi-sig keys, revocation base points, payment base points, all these things, so that we actually swap out the account for this key family sort of notion. Actually, swap out the account for this key family sort of notion. And so, this is nice because in the multi-chain setting, we can set the different point types and we initialize one wallet per chain. So, each one of those gets placed in its own chain directory if you're using LND in that setting. Those wallets can independently be rescanned and recovered. So, I'm currently working on all the rescan logic so that you'll be able to just say, oh, restart LND, pop in and look ahead of 1,000 or 5,000 or 20, whatever it is, and it'll just sort of drive keys, scan forward, looking for them and update. It is, and it'll just sort of drive keys, scan forward, looking for them, and update your balance as it goes. And finally, the biggest benefit to all this is that you can use one AEZ cypher seed and be able to manage funds on all different chains. Finally, getting to some more onion routing packet construction. So in this last version, we sort of optimized the actual construction process. So before we had sort of a quadratic algorithm that when you're processing a path link to 20 hops, we basically do this O of n squared algorithm to actually direct. Hops, we basically do this O of n squared algorithm to actually derive all the ephemeral private keys and blinded pub keys. Here are the like you can see the equations right here. Basically, you can see that like you know, you go from zero to I, and then actually the blinding factor is also exempt from zero to i. But because they're like shared across all of them, we can cache them all. And the effects of this are pretty immense. We saw an 8x speedup dropping the time from roughly down to 4.5 milliseconds on my machine, like 75, 65 less memory. Seconds on my machine, like 75, 65% less memory, 77% fewer allocations. And shout out to Jimbo, is he here? No. Well, he read this up and then it was accepted to the Bolt L in spec. So now it's like the reference implementation for how you derive the honey packets, or at least like these keys. And then we have some benchmarks down here at the bottom, and then finally, like just a comparison of the number of total scalar and base multiplications that you use to compute them. So pretty big. And base multiplications that you use to compute them. So, pretty big savings. And finally, now a part of this whole HTLC packet forwarding is the ability to detect replayed Sphinx onion packets. So, if I just take, so the packets over the network, they come over in this 13 1366 byte onion blob, and I receive that off the network, then I'm able to process it and get things like the which hop I'm forwarding to, the amount, the timeout. Like the which hop I'm forwarding to, the amount, the timeout, CLTV, stuff like that. And if someone were to actually just intercept those packets and start replaying them to me, it's sort of like a privacy leak because people could determine, like, oh, you can see my actions based off, or the actions I take based off of processing that packet, as well as like, you know, someone might just process it again or whatever. So we want to prevent that as much as possible. And this operation has to survive restarts. So if I send you a bunch of stuff, make you crash or DDoS you, come back up, try to send you the same things. make you crash or DDoS you, come back up, try to send you the same things, you know, you shouldn't be able to accept that, or you should at least detect it. So the way we implement this is we implement a decaying log, which when I receive this onion blob and parse it, I am actually able to generate a shared secret, one of the ones that's, well the equation isn't here, but yeah. And then we hash that and take about the first 20 bytes and store sort of like a on disk table of all those. Then when when these packets come in I actually just compare against all the ones that I know and if any are like found to be duplicates we reject them. That I know, and if any are like found to be duplicates, we reject them. And then so, so and then in that process, we actually record in that batch which ones were actually marked as replays. Because going back to the sort of example of when we're processing a batch of 10 HTLCs and we get down to the first three, if we restart and then come up again, we might actually be replaying ourselves and not actually know the difference. So, to prevent against that, we use this batch. So to prevent against that, we use this a batch identifier, this ID, which is the short chain ID commit height, and we use that as an identifier. We pass that in. If the batch has already been sort of tested for replays, we just return the actual replayed indexes of those packets. And we know off the bat that we don't have to do any more processing. That was just the decision before, and we're going to deterministically replay that. And yeah, like I said, this is primarily our protection against rejecting packets against ourselves after restart. ourselves after restart. We can answer any questions people have. If there are any, I'm sure there'll be a lot we'll see. Just a quick question about the wallet architecture. The wallet that, like the address that you send funks to when you start or when you sweep the channel, are those in the regular BIP44 path? Yes. For those we use BIP44 path Um yes, for those we use BIP49, which is like BIP44-based, and that's BIP49 is basically um an SP2SH, but we modify it to use uh witness, uh, you know, like a native witness loop for the uh um for the change address, and then we use I think it's called BIP84, which is basically just pure uh NSPS cache. So you basically like rescam for those what you have to see if anything else. In your key derivation purposes you listed a multi-sig purpose. Is that just for the channel anchor? Uh yeah, yeah, that's Anchor or? Uh, yeah, yeah, that's just for the funny job put it on. Do you have another use for something? Well, you know, well, two, sign and all the yeah, we can have like seven, right? You know, yeah. But these are like track distinctly, right? So, like, the wallet itself tracks are the ones that are good. So, you're using the same recovery words. You're using the same recovery words as is traditional at this point, correct? Currently, yes, but with the way the scheme is, the actual enciphering is distinct from the encoding. So, we can swap out recovery words in the future. So, I have a library where we went through 100,000 words to find the most memorable, most international words, and the only thing that's National words, and the only thing that's left in the thing is I need to do hamming distance, so the touches. So if somebody is interested, we can come. I've got like 80% of what's necessary for a much better word list. Cool, cool. Yeah, yeah, so it's version, so we can get in the future if we want to modify word lists or any other parameters. But yeah, definitely. Thank you. Oh, and to add on to that, we can also do different languages as well. So you can translate the same sort of raw encoded cycle text into Japanese or whatever you need. More like French. So we're in the early days of Lightning. Can you talk about how you guys see the network growing and it being adopted by members of business users? Sure, sure, yeah. So I mean I think like we're in the very, very early days right now we basically have a bunch of like enthusiasts who like very excited, maybe aren't really used to like running kind of like interfacing services. Maybe aren't really used to when it kind of like interfacing services. It's like, oh, someone's doing a TCP have open attack. What do I do? I think once we get back to the initial phase, we're going to work ourselves with Magnet Labs to kind of giving people the educational resources to have operating nodes, right? So, what should you be setting as far as your kernel settings? How do you want to actually connect to other peers? How do you want to actually manage your connections? As far as accountability. So, we're going to do a lot of work to actually get the education for the node operators to basically know what they're doing, kind of like be a little more aware of the network. We also have planned this UI for neural operators. About the network. We also have planned this UI for neural operators to kind of like be able to look at their node and acknowledge what's going on as far as payments coming in, kind of optimizing my channel, things like that. And then, as far as merchants, annotation, things like that, I think we're in a phase where they can start experimenting with it now. Because I think one big thing was for them to see actually live on the network beyond a test, right? So now to explain the main money. We're actually seeing some people already experimenting. I think right now, kind of like BitVille is the major version on the network, and people kind of realize, oh, I connect to them, and I can. On the network, and people kind of like overrealize, like, oh, I can connect to them, and I can wrap everywhere else. So, I think we'll also see different merchants come up and kind of like, you know, take advantage of that. One thing we've seen in the past as well, people offering discounts for channel creation. It's like, oh, you know, open a channel to me, and I'll pay the opening key or something like that, right? Maybe people will be incentivized to have different lower fees from the get-go. But so I think it's kind of in a phase where people can jump around early for your early enthusiasm, but no, it's kind of like the early days, people will kind of still be setting up the infrastructure. Then beyond that, once we actually see things mature a little bit more, I think more exchange will come on because there'll be pretty cool things you can do with the exchange. I think more exchanges will come on because it'll be pretty cool things you can do with exchanges as far as making them less custodial. Have kind of like a hybrid type of channel and I can push my money over there and actually execute a trade. And also maybe do cross-excharge arbitrage. I have an account on two exchanges, I can send my Litecoin over here, sell it, send a Bitcoin, buy more Bitcoin that's what you want to do, and then do whatever else I want with it. But yeah, that answered your question if you want more specific stuff. I'm wondering about what the major attack vectors are on the Lightning Network. Yeah. are on the Lightning Network? Yeah, tech factors include taking down nodes, I guess, which is why you want to have multiple nodes for a particular service, to ensure that you actually have availability of your users. Other things you can do which aren't fully buttons yet are that you don't necessarily need to be advertised by the network. You only need to do so if you're actually running a routing node, right? So me as a merchant service, I could basically just be on the edge and not even advertise that they're on there. The only people that actually want to route towards me to actually do different payments can do that. Like Rock Tours need to actually do different payments. You can do that. Other things include maybe trying to spam a node with very small channels or something like that. You can basically have policies that I need a channel above half Bitcoin or whatever, right? That kind of adds a cost barrier to actually spamming all this date. Other things include, I guess, doing things on the chain. If you can kind of make the chain super fool and something like that, make a bunch of tax. Then we have other defenses in that kind of do like a scorched earth approach where if you're trying to shoot me, well, I'll just send all your money to my mercies, right? And that's it. Well, I'll just send all your money to my mercies, right? And that's it. So there's like that's like the first step I do. I'm really cure. I just want to basically have it to be like a strong determinant for any cheers to actually go against me. But also, yeah, any other one? I think about that pretty much covers most of it. But yeah, I'd say availability is probably one of the biggest things. So doing things like putting yourself in like a Tor node so you have your IP isn't available, stuff like that. I forgot to add Tor. Oh, yeah. Okay, but also we have outbound Tor support. Now it's basically, you know, I can be connected. I can connect using Tor. Now it's basically, you know, I can be connected, I can connect using Tor onto the network later on in the version. We're going to add hidden service support as well. So that can basically be around it, but not give away my IP address. So maybe that's even better now because they don't know that I'm using Comcast on Sambaroon or something like that. So they can just know that I'm in the world somewhere instead. Yeah. It also kind of depends on what your definition of attack is, whether it's inconvenience or whether it's actually full-on exploits. So I mean there's varying levels of maybe all those, but in general I'd say the ones that are most practical will probably just be like inconvenience as well. In general, I'd say the ones that are most practical would probably just be inconveniences rather than just full of exploits. Yeah, and I guess there's things that haven't really merged yet. Maybe some active privacy attacks, people kind of like doing things actively on the network more than willing to try to animalize users. And then it becomes a whole bowl of insurance by privacy in that service. So we'll see when that comes. Hey, I'm curious how watchtowers are going to work for more intermediate nodes, like people who have nodes on their cell phones. Nodes, like people who have nodes on their cell phones that aren't specifically routing 24/7? Gotcha. Well, you know, they wouldn't be accepting states from individuals, but you know, they would be exporting those states themselves. But also, you know, I guess there's a question of kind of like the compensation structure. There's a few ones you can say everyone isn't for free because they're altruistic, they want to exceed, they can accept all the states. We can do something else where maybe you may, you know, pay like some small social amount per state itself. But then also, there could be a thing with like a bonus where if you actually act and serve justice, then you get like you know 10% or something like that. So, but you know. Then you get 10% or something like that. But ideally, once we're all on mobile platforms, it's gonna be all in the background. So maybe like so ideally just does it for nodes automatically. But maybe if I'm a power user, I can basically scan a QR code and make sure it's back to my Bitcoin D node as well, just for say keeping constant or whatever else. Yeah. Ideally you might have one or two or three just so you can also cross-reference them and when you come up, make sure that they're all they're they're all consistent with like your state when you went down or if you lost something and you try to recover then like they're all telling you the same thing. Something you try to recover, then they're all telling you the same thing. And that's just helping, I think, a lot of the instances there. Yeah, it wouldn't be redundant and distributed, basically, because then it's kind of like a one-event thing at that point. But then, like, you know, one thing we do within the cartoon is we actually scale the CSV value according to the channel size. So maybe it's like a $10 channel, it's like one day, but it's like $20K. It's like, give me a few weeks, just to make sure things don't happen there. Yeah, but I think what you're getting at also is that once we have more effective watch towers and stuff like that, and node availability isn't as much of an attack vector for. That and node availability isn't as much of an attack vector for stealing your funds, then we can actually reduce those timeouts. And then all the other inconveniences, like, oh, my funds locked up for two weeks, aren't as so much of a problem anymore. Yeah, it's like as some of the timeouts are kind of on the higher side, that's because it's new. So we want people to be a little more cautious, even though it can be used to them that much more. So trying to be grateful, basically, right? Hi. I was curious, it's a small detail: the macaroons. Could you speak a bit more about them and kind of what some use cases? Could you speak a bit more about them and kind of what some use cases are and lay a feature for node operators or users? Sure, yeah. I mean, so like they're very, very simple. Basically, it's just an HMAC, right? It's an HMAC of a root key and then maybe some authenticating data along with that, right? So because of this, you can't actually forage Mac whenever, and you can't go the other direction, right? So you're able to attenuate it, maybe add on a channel for one BTC on Monday by kind of like hashing that down, and then I can still verify the root chain itself, right? So it basically has this final digest, but then also kind of like information on how to reconstruct. Digest, but then also, you know, kind of like information on how to reconstruct that digest from the root macro root key. And these are actually pretty used, you know, like I think Google uses them pretty extensively now. Anytime you actually, if you can tech your headers, maybe have like back roots that are being used for them. And in terms of like node operators, you can, you know, you can have a setup where maybe your macro roots aren't even on your node at all, right? Like instead, you can basically double them remotely and you have like a special macro root for only particular purpose, right? But then, you know, beyond that, you can have kind of like monitoring service, maybe some like, you know, there's some kind of like metric gathering server that wants to see, you know, what's going on as far as node operators. You can give them. If they want to see what's going on in Spark Nerd operators, you can give them that macaroon to only collect that data itself. But then I think the other cooler aspect here where you basically do kind of like microservice type infrastructure, where I can have different distinct instances that only do what they need to do and nothing else. So it kind of isolates that. So if they break into this box, they can only list my channels and maybe not actually make payments and things like that. So right now, like I was saying, we have an admin, which is basically all purposes. We have read-only, we can only do read-only. And then we have invoice to basically make addresses and list invoices. But later on, we're going to add tooling to basically let you do very custom macaroons. Later on, we're going to add tooling to basically let you do very custom macaroons. And this could be cool in the future because you could say maybe we're on a kind of mobile operating system, and via intent, I can pass a Macaroon to maybe send a payment every day to some video game app, right? And then only using the Macarono via Intent, I pass that to L and D, L D connects executed on there. So there's pretty cool architectures in terms of having multiple different services with the same L D, but having them all have very strict privileges of what they can do. I was wondering what would happen if you sort of Satchi spam channel anchors? Sochi spam? You know during the Sochi Olympics somebody was sending all these one Satoshi outputs. Yeah, I remember that. Oh you mean if they just like made a bunch of channels? Or if people were spamming channel anchors with additional UTXOs would they just get integrated into like a commitment? Would they just get integrated into later commitment transactions? Would they as you mean be lost? Do you mean like making a bunch of very small channels? Or third-party spam CTXOs to channel anchors. Oh, I see what you're saying. I mean, they wouldn't be a part of the original funding output, right? They'd be just different outputs on the same address. So, like, you, like, LD may could spend from them, but it may not be worth it if they're only one Satoshi channel. You can later add like kind of like constraints on what channels you accept incoming. Like, just write a degree income channel. Constraints on what channels you accept incoming. Just a degree in income channels, free. It's like, oh, you're going to put up like $100 to me, that's fine. Maybe you don't want a 10 cents output because maybe that's going to be desk in the future, right? So you can kind of like, it's not implemented yet, but there's an issue of it on the GitHub. There's much beginner issues by that people want to actually do your development LD. That you're going to implement that basically, like kind of like have more finite policy on what channel do I accept type of thing. Maybe if someone donated a lot of money, it might be worth it. But the ease required to get your one Satoshi back into your wallet balance is going to be more than tier one Satoshi. Is going to need more than tier 17. So another question about the watchtowers. Two little questions. Are they necessarily trusted? Is there any way to outsource it trustlessly? And then, secondly, is there anything about it that's deterministic? Like you have the deterministic wallet structure. Can you harden the keys that are used for relocation and then give an XPUB or an XPrive, I guess, the watchtower, that kind of thing? Or do you need to give them a key every time your channel updates? Gotcha. Yeah, good questions. What was the first word again? Are they trustless? The first word again? Are they trustless? Yes, you can say they're trustless because you basically have them do a particular action, and if they don't, when you have one other one, they can do that action. Could the watchtower, instead of returning the funds to you, sweep the channel to the side? No, because I give them a signature, and if they create another transaction, it's invalid at that point, right? So you can say, you know, I tell them what the transaction should look like, and we use X69 to basically ensure the inputs and outputs are ordered deterministically. So I give them a signature basically invalid information, and they can only do that, right? signature basically balance information and they can only do that, right? So if we say okay you can take 50% on you know like justice enforcement, then they can only do that as well. And you need to set that up in advance, the watchtower's fee. Exactly. So it's kind of like we said if we do negotiation, okay, you know, here's 20%, you know, I'll you know, we want to society a single state, and at that point they can only do that action. So what they can do is just like not act, but then in that case you know you wanted to have these like be redundant other places and like you know it takes like one out choice to secure a bunch of things and people like you know Bitcoin so maybe you know they'll do that for a free frequency. will like Bitcoin so maybe they'll do that for free. And then the second question was about you'd have to update every with every channel update you'd have to tell the watchtower what the new yeah but certain aspects are deterministic like you know there's like a revocation scheme which uh you know can let them compress the space into like log n rather than like having one for every individual one and there's a few different versions some involve kind of like you know using these blinded keys to ensure they don't know what channel they're watching the other ones which may go with which a little more generic is kind of like you have like an encrypted blob and I encrypt the blob itself with half a TXID right which you don't know The blob itself with half the TXID, right? Which you don't know. But if you see this part of the half-txt ID on the chain, you have the full thing you can decrypt with it and act. And otherwise, you have to brute force it, you know, AES, 286, whatever. So good luck with that. Thanks. Great presentation. So, one problem I've had when spinning up Lightning nodes is figuring out who to connect to and who to open channels. Is figuring out who to connect to and who to open channels with. And especially if you're trying to receive payments, getting somebody who's willing to open a channel with funds on their side. So, do you have any potential solutions or do you envision how this problem will be solved in the future and allowing people to figure out who to connect to? Sure, yeah. Yeah, sure. I think one of the things that will be kind of under development a lot this year and in the future will be like further work on autopilot. You can imagine, I think, there's like a couple different. If you can imagine, I think there's a couple different use cases that users might have. You might be specifically a routing node, you might be a person that's trying to pay, you might be a merchant, you might maybe an exchange or whatever. There might be different sort of use cases. And so you can sort of think of optimizing the attachment profile of Autopilot based on whatever your use cases, right, or what you're trying to optimize for. And there might be different fitness models for which you are trying to. Yeah, Autopilot is something Lollibuil. It's basically like an automated channel maker. So it'll look WalletBuild, it's basically like an automated channel maker. So it'll look at, it can take certain inputs, like, oh, did your wallet balance change? Did you connect to somebody? Stuff like that. And then it'll try to look and see, oh, what's a good channel to make. So you can use different heuristics to guide that attachment. And so, and the feature, I think, or at the same time, there could be more matchmaking services. People say, hey, I'm trying to meet, maybe we can link up, and there's an exchange for making a channel or something. And that's another way of going about it. Or something, and like that's another way of going about it. But in general, I think if you have a more informed and more optimized autopilot, then a lot of these problems might go away because hopefully you just make channels to a better portion of the network that you're trying to target. Yeah, so the end goal is that you kind of like put money in a box and it just does it, it just works right. We're not quite there yet, but we're making tries towards that. And the current one doesn't testnet, and maybe some people are doing that mainnet kind of like tends to minimize the average hop distance, kind of like you know, tend towards a scale-free type network. That's what we're environmentally on testnet. No, we're gonna be doing a lot. Network, that's the work that you want to call a testnet. We're going to be doing a lot more experimentation on that front, but even within the code base itself, everything is kind of abstract like an interface level. So you can basically add what's called like an attachmenturistic, right? Which has things like, you know, do I have more channels? And then who should I connect to? And right now, basically, it only uses data in the graph itself, like I was seeing in the future. We could also start to actually put in signals from each individual channel, maybe create a channel fitness, and then from there kind of like colour them down in terms of what we should be doing. And the other thing about establishing kind of like inbound liquidity, so I think there will be kind of like liquidity. So, I think you know, there will be kind of like you know liquidity mass making services, where it's kind of like you know, you can basically buy a channel for incoming, right? And you can say that someone wants to do that because if you're very popular, then maybe you know, they'll be earning revenue routing towards you. But you can also say, maybe if I'm buying a channel, I can give you, you know, you know, like a credit for like you know, free 20 payments, something like that, right? So, intended by also like you know, having like outsource transmits me. So, if you're a merchant, maybe, you know, you buy some family because you compute kind of like what your total inflows would be on that day and what you need as well as there. So, you can kind of like have a set of basically radical banks. So, you can kind of have a set of a basically viable bandwidth itself. So, it's like period, kind of, but like, you know, the costs are very, very low because you're kind of like opening a channel. You should use some layers to basically signal your preferences and pricing and things like that, and that's your people up. And then, I think to finish up, I think the last thing that really helped out as well is when we enable dual blender channels. Because, like, right now, yeah, you might be right. It might be hard to get someone to be like, hey, just put up much money in this channel to me and nothing else, right? Like, but if you're going to both put up some collateral and basically be like, oh, we're both being able to route keys initially off the bat without having to wait for the channel to normalize everything. Like, route keys initially off the bat without having to wait for the channel to normalize or anything, then you're gonna have a lot better time selling that, I think. So, what about like reputation of that? Like, is there a good third-party services that exist? Yeah, possibly, possibly. It probably won't be limited in the protocol level just because for like identity and privacy reasons, but like, yeah, it's kind of possible. People want like uptime. You could even compute real-time reputation amongst your other peers. Like, okay, you know, every single time I send an HLC, it doesn't get canceled back from like that, right? So, there'll be some sort of kind of like metrics of robustness, because you don't want to connect to like a whole secure peer that's only. Like metrics of robustness. You don't want to connect a whole CPA to only add one kind of liability or things like that. If you want something to open a channel, just call Justin. Which is very funny. There's a question. It seems to me that from your presentation, the replay protection is built into the router of the switch, which will inevitably slow down the performance of the router or switch. My stupid question: why we designed that way? Um my stupid question was why we designed that way. And the second question is can you actually move the r replay protection to the node, to the edge, to the end node, instead of putting the switch advanced, that's what will increase the performance of your switch advisors. Yeah, so actually that's actually how it works. So I didn't really get to where the actual switch replay protection was actually uh put in that diagram, but it actually happens at the link level at the outer edges. Um almost uh almost all the logic in the switch was actually pushed out of the interior as much as possible, sort of enable like pushed out of the interior as much as possible to sort of enable like the performance as you were saying. Um the only e even like when we're actually adding to the circuit map, because of the nature of the way uh like incoming channels work, like when I'm receiving HTLC and like I submit a batch to like the the switch, um like the only person who will ever submit those that range of channel IDs is that link. So as long as it itself isn't replaying them, it like there's gonna be no contention there. So those are actually all done in parallel and then the only thing that goes through the actual center of the switch is just all held in memory. thing that goes through the actual center of the switch is just all held in memory. I have two practical questions sort of. I was running Lightning D and then I was running LTCD and I was trying to just grab the graph and compute the number of nodes and it didn't match. I run them against the same Bitcoin node and consistently node and consistently Lightning D of a C implementation was giving me about like recently it was about 800 nodes and LTCD was giving me about 480 or something like that. Is this on which network? Mainnet? On mainnet, both. So there might be a couple reasons for that. I mean one is just that you know there's no guarantee that you will end up seeing the same actual graph. Guarantee that you will end up seeing the same actual graph. It kind of depends on who those nodes are connected to and what actual gossip information you're getting from them. It's sort of like an eventually consistent, hopefully, it's like a hopefully eventually consistent system. But like I said, there's no guarantee that you will actually get all routing updates and they just sort of come in as a best effort thing. The other reason that there might be differences is depending on the level of validation that the different implementations are applying, you might actually be storing invalid, valid, like announcements or whatever on one. Invalid, valid, like, announcements or whatever on one, while one is actually doing more heavy filtering, which I would guess is the primary reason, but I don't know what they're saying. So, yeah, like LD only accepts nodes that have channels open. But if another implementation is a little more relaxed in validation, then you can see that the values will be different. But also, different implementations have different policies on kind of like when they garbage the channel. So it could be the case that the channel has been there for a year and nothing has happened with it. We'll forget that, right? The implementation may not forget that. They should be around the same size, but also depending on your knowledge of private channels. size, but also depending on your knowledge of private channels, you could have a bigger graph and tell them elsewhere because they don't really know of the extent of what private channel backup network you can have. There could be some other alternative private network channel that no one knows about, but it's used super heavily right now, and it's kind of like it's bridged via the public network as well. And that should actually, the ability to garbage collect those old nodes is actually really beneficial to your routing performance and usability because if you're spending time trying to route through dead nodes, that's just going to increase the time and the number of trials it takes to actually make a payment go through. gonna like increase the time and like the number of trials it takes to actually make a payment go through. So so we'll prove that. I see. And the second question that could be just my ignorance, is it possible so when you open a no uh when you when you open a channel, you commit some funds to the channel and is it possible to observe those funds being depleted gradually uh at the level of network or not? Or you just see the event of closing the channel? Uh network isn't like RPC or like peer-to-peer network. Uh let's say RPC. Uh let's say RPC. Uh yeah, you can like for RPC you can like just pull the um it's like you know we have that forwarding events thing now which basically will show you every single forwarding event in the channel. So you know you can use that to see kind of like uh what's happening. There's not yet a streaming or a particular movie in the future and there's also a list channel which you can just pull it and see one balancer or a V modified item. Last question. Last question. Okay, so say, I think you mentioned this a little bit earlier when you had the dust in the transaction, so a transaction that's super small, and then you're going through intermediary node, and say what if he goes uncooperative or he goes silent, and then the channel is like HTLC locked, right? So you would have to have output for that lock. But if it's super small, you can't have an output, so would it be stuck there, or like, how would that be? So, would it be stuck there or how would that? So, you're saying like a desk HLC never gets fully completed? Yeah. So, we won't record that in the log. We only record things that actually get extended and then we come back across. Okay. But then, also, no type of dynamic desk limit, so I can say, okay, I only accept HLC above two cases doches. So, I would not do anything else, right? And then we can also do things around ensuring that our push transaction is always valid by consensus or by popular policy. Valid by consensus or by popular policy, to avoid having the desktop layered. So it goes to my fees right now, basically. So you kind of mentioned that you would have a minimum transaction that you would have to have. You couldn't have a micro payment or anything like that. Well, it depends. Certain nodes will say maybe I'm a high-value node, so I don't like to have a micro node. Other ones will say, I want that because I want the fees, I don't want even the frequency of the payments to work on. Okay. Because I wasn't sure if Lightning Network was supposed to handle very, very large transactions versus maybe smaller nodes. Very, very large transactions versus maybe smaller, faster, daily transactions out within the page. Yeah, which means they can do both. And we also have this thing called AMP, which kind of lets you split a larger payment into a bunch of smaller payments. So maybe if people have smaller channels, you can still do $100 payment through a bunch of $10 channels, possibly. Okay, that's cool. I guess that's it. Thank you, I suppose.
